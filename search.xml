<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>1.14.无所吊胃，加油蓝桥杯，加油考研！</title>
    <url>/posts/a3ba5a6b.html</url>
    <content><![CDATA[今天是2025年1月14日
我打算写一写缓解焦虑。终于终于终于不用再上课了！！！
[progressbar progress=“12.3”]2025[/progressbar]
放寒假已经三四天了，期末考试成绩出来了，还是那样不太理想；
在上学的时间里，我都在上课和自习的挣扎中，既然在课堂中学不到东西，那为什么要上课，还不如自己学；在加上本来对自己的学业有点失望，总有人我就是考不过，那就这样吧，哼哼哼。
下学期我要免听！再不考上研我这辈子也就这样了，本科还不如上一个让自己高兴的大学，不管是二本还是一本。
]]></content>
      <categories>
        <category>uncategorized</category>
      </categories>
  </entry>
  <entry>
    <title>NumPy科学计算库</title>
    <url>/posts/e1ca7594.html</url>
    <content><![CDATA[[toc]
NumPy科学计算库
课程介绍
NumPy（Numerical Python）是Python的一种开源的数值计算扩展。提供多维数组对象，各种派生对象（如掩码数组和矩阵），这种工具可用来存储和处理大型矩阵，比Python自身的嵌套列表（nested list structure)结构要高效的多（该结构也可以用来表示矩阵（matrix）），支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库，包括数学、逻辑、形状操作、排序、选择、输入输出、离散傅立叶变换、基本线性代数，基本统计运算和随机模拟等等。


几乎所有从事Python工作的数据分析师都利用NumPy的强大功能。

强大的N维数组
成熟的广播功能
用于整合C/C++和Fortran代码的工具包
NumPy提供了全面的数学功能、随机数生成器和线性代数功能



安装Python库


第一种方式：

pip install jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple



第二种方式：


直接安装anaconda下载


注意：Add Path！！！ 添加一下环境变量~


百度网盘链接: https://pan.baidu.com/s/1sQ8LMH6q8ezVUzNjSCtgyQ 提取码: sm7m




启动终端


Windows----&gt; 快捷键：win + R -----&gt;输入：cmd回车------&gt;命令行出来


Mac ----&gt;启动终端




启动jupyter

进入终端输入指令:jupyter notebook
在哪里启动jupyter启动，浏览器上的目录，对应哪里，windows默认路径是：C:\Users\lufengkun
C:\Users\xxx



第一部分 基本操作
第一节 数组创建
创建数组的最简单的方法就是使用array函数，将Python下的list转换为ndarray。
import numpy as npl = [1,3,5,7,9] # 列表arr = np.array(l) # 将列表转换为NumPy数组arr # 数据一样，NumPy数组的方法，功能更加强大# 输出为# array([1, 3, 5, 7, 9])
我们可以利用np中的一些内置函数来创建数组，比如我们创建全0的数组，也可以创建全1数组，全是其他数字的数组，或者等差数列数组，正态分布数组，随机数。
import numpy as nparr1 = np.ones(10) # 输出为：array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])arr2 = np.zeros(10) # 输出为： array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])arr3 = np.full(shape = [2,3],fill_value=2.718) # 输出为：# array([[2.718, 2.718, 2.718],#       [2.718, 2.718, 2.718]])arr4 = np.arange(start = 0,stop = 20,step = 2) # 等差数列 输出为：array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])arr5 = np.linspace(start =0,stop = 9,num = 10) # 等差数列 输出为：array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])arr6 = np.random.randint(0,100,size = 10) # int随机数 输出为：array([ 4,  8, 79, 62, 34, 35,  2, 65, 47, 18])arr7 = np.random.randn(5) # 正态分布 输出为：array([ 0.57807872,  0.37922855,  2.37936837, -0.28688769,  0.2882854 ])arr8 = np.random.random(size = 5) # float 随机数 输出为：array([0.59646412, 0.37960586, 0.38077327, 0.76983539, 0.22689201])
第二节 查看操作

jupyter扩展插件（不安装，已经不兼容了）

pip install jupyter_contrib_nbextensions -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install jupyter_nbextensions_configurator -i https://pypi.tuna.tsinghua.edu.cn/simple
jupyter contrib nbextension install --user
jupyter nbextensions_configurator enable --user
退出，重新进入jupyter notebook就可以了



NumPy的数组类称为ndarray，也被称为别名 array。请注意，numpy.array这与标准Python库类不同array.array，后者仅处理一维数组且功能较少。ndarray对象的重要属性是
1.2.1 数组的轴数、维度
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.ndim # 输出 3
1.2.2 数组尺寸形状
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.shape # 输出 (3,4,5)
1.2.3 数组元素的总数
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.size # 输出 3*4*5 = 60
1.2.4 数据类型
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.dtype # 输出 dtype(&#x27;int64&#x27;)
1.2.5 数组中每个元素的大小（以字节为单位）
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.itemsize #输出是 8 ，因为数据类型是int64，64位，一个字节是8位，所以64/8 = 8
第三节 文件IO操作
1.3.1 保存数组
save方法保存ndarray到一个npy文件，也可以使用savez将多个array保存到一个.npz文件中
x = np.random.randn(5)y = np.arange(0,10,1)#save方法可以存一个ndarraynp.save(&quot;x_arr&quot;,x)#如果要存多个数组，要是用savez方法，保存时以key-value形式保存，key任意（xarr、yarr）np.savez(&quot;some_array.npz&quot;,xarr = x,yarr=y)
1.3.2 读取
load方法来读取存储的数组，如果是.npz文件的话，读取之后相当于形成了一个key-value类型的变量，通过保存时定义的key来获取相应的array
np.load(&#x27;x_arr.npy&#x27;) # 直接加载# 通过key获取保存的数组数据np.load(&#x27;some_array.npz&#x27;)[&#x27;yarr&#x27;]
1.3.3 读写csv、txt文件
arr = np.random.randint(0,10,size = (3,4))#储存数组到txt文件np.savetxt(&quot;arr.csv&quot;,arr,delimiter=&#x27;,&#x27;) # 文件后缀是txt也是一样的#读取txt文件，delimiter为分隔符，dtype为数据类型np.loadtxt(&quot;arr.csv&quot;,delimiter=&#x27;,&#x27;,dtype=np.int32)
第二部分 数据类型
ndarray的数据类型：

int: int8、uint8、int16、int32、int64
float: float16、float32、float64
str

array创建时，指定
import numpy as npnp.array([1,2,5,8,2],dtype = &#x27;float32&#x27;) # 输出 ：array([1., 2., 5., 8., 2.], dtype=float32)
asarray转换时指定
import numpy as nparr = [1,3,5,7,2,9,0]# asarray 将列表进行变换np.asarray(arr,dtype = &#x27;float32&#x27;) # 输出：array([1., 3., 5., 7., 2., 9., 0.], dtype=float32)
数据类型转换astype
import numpy as nparr = np.random.randint(0,10,size = 5,dtype = &#x27;int16&#x27;) # 输出：array([6, 6, 6, 6, 3], dtype=int16)# 使用astype进行转换arr.astype(&#x27;float32&#x27;) # 输出：array([1., 4., 0., 6., 6.], dtype=float32)
第三部分 数组运算
加减乘除幂运算
import numpy as nparr1 = np.array([1,2,3,4,5])arr2 = np.array([2,3,1,5,9])arr1 - arr2 # 减法arr1 * arr2 # 乘法arr1 / arr2 # 除法arr1**arr2 # 两个星号表示幂运算
逻辑运算
import numpy as nparr1 = np.array([1,2,3,4,5])arr2 = np.array([1,0,2,3,5])arr1 &lt; 5arr1 &gt;= 5arr1 == 5arr1 == arr2arr1 &gt; arr2
数组与标量计算
数组与标量的算术运算也会将标量值传播到各个元素
import numpy as nparr = np.arange(1,10)1/arrarr+5arr*5
*=、+=、-=操作
某些操作（例如+=和*=）只会修改现有数组，而不是创建一个新数组。
import numpy as nparr1 = np.arange(5)arr1 +=5arr1 -=5arr1 *=5# arr1 /=5 不支持运算
第四部分 复制和视图
在操作数组时，有时会将其数据复制到新数组中，有时不复制。
对于初学者来说，这通常会引起混乱。有以下三种情况
完全没有复制
import numpy as npa = np.random.randint(0,100,size = (4,5))b = aa is b # 返回True a和b是两个不同名字对应同一个内存对象b[0,0] = 1024 # 命运共同体display(a,b)
查看或浅拷贝
不同的数组对象可以共享相同的数据。该view方法创建一个查看相同数据的新数组对象
import numpy as npa = np.random.randint(0,100,size = (4,5))b = a.view() # 使用a中的数据创建一个新数组对象a is b # 返回False a和b是两个不同名字对应同一个内存对象b.base is a # 返回True，b视图的根数据和a一样b.flags.owndata # 返回False b中的数据不是其自己的a.flags.owndata # 返回True a中的数据是其自己的b[0,0] = 1024 # a和b的数据都发生改变display(a,b)
深拷贝
import numpy as npa = np.random.randint(0,100,size = (4,5))b = a.copy()b is a # 返回Falseb.base is a # 返回Falseb.flags.owndata # 返回Truea.flags.owndata # 返回Trueb[0,0] = 1024 # b改变，a不变，分道扬镳display(a,b)


copy应该在不再需要原来的数组情况下，切片后调用。例如，假设a是一个巨大的中间结果，而最终结果b仅包含的一小部分a，则在b使用切片进行构造时应制作一个深拷贝：
import numpy as npa = np.arange(1e8)b = a[::1000000].copy() # 每100万个数据中取一个数据del a # 不在需要a，删除占大内存的ab.shape # shape(100,)


第五部分 索引、切片和迭代
第一节 基本索引和切片
numpy中数组切片是原始数组的视图，这意味着数据不会被复制，视图上任何数据的修改都会反映到原数组上
arr = np.array([0,1,2,3,4,5,6,7,8,9])arr[5] #索引 输出 5arr[5:8] #切片输出：array([5, 6, 7])arr[2::2] # 从索引2开始每两个中取一个 输出 array([2, 4, 6, 8])arr[::3] # 不写索引默认从0开始，每3个中取一个 输出为 array([0, 3, 6, 9])arr[1:7:2] # 从索引1开始到索引7结束，左闭右开，每2个数中取一个 输出 array([1, 3, 5])arr[::-1] # 倒序 输出 array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])arr[::-2] # 倒序 每两个取一个 输出  array([9, 7, 5, 3, 1])arr[5:8]=12 # 切片赋值会赋值到每个元素上，与列表操作不同temp = arr[5:8]temp[1] = 1024arr # 输出：array([   0,    1,    2,    3,    4,   12, 1024,   12,    8,    9])
对于二维数组或者高维数组，我们可以按照之前的知识来索引，当然也可以传入一个以逗号隔开的索引列表来选区单个或多个元素
arr2d = np.array([[1,3,5],[2,4,6],[-2,-7,-9],[6,6,6]]) # 二维数组 shape(3,4)arr2d[0,-1] #索引 等于arr2d[0][-1] 输出 5arr2d[0,2]  #索引 等于arr2d[0][2] ==  arr2d[0][-1] 输出 5arr2d[:2,-2:] #切片 第一维和第二维都进行切片 等于arr2d[:2][:,1:] arr2d[:2,1:] #切片 1 == -2 一个是正序，另个一是倒序，对应相同的位置# 输出：#array([[3, 5],#       [4, 6]])
第二节 花式索引和索引技巧

整数数组进行索引即花式索引,其和切片不一样，它总是将数据复制到新数组中

import numpy as np#一维arr1 = np.array([1,2,3,4,5,6,7,8,9,10])arr2 = arr1[[1,3,3,5,7,7,7]] # 输出 array([2, 4, 4, 6, 8, 8, 8])arr2[-1] = 1024 # 修改值，不影响arr1#二维arr2d = np.array([[1,3,5,7,9],[2,4,6,8,10],[12,18,22,23,37],[123,55,17,88,103]]) #shape(4,5)arr2d[[1,3]] # 获取第二行和第四行，索引从0开始的所以1对应第二行 # 输出 array([[  2,   4,   6,   8,  10],#            [123,  55,  17,  88, 103]])arr2d[([1,3],[2,4])] # 相当于arr2d[1,2]获取一个元素,arr2d[3,4]获取另一个元素# 输出为 array([  6, 103])# 选择一个区域arr2d[np.ix_([1,3,3,3],[2,4,4])] # 相当于 arr2d[[1,3,3,3]][:,[2,4,4]]arr2d[[1,3,3,3]][:,[2,4,4]]# ix_()函数可用于组合不同的向量# 第一个列表存的是待提取元素的行标，第二个列表存的是待提取元素的列标# 输出为# array([[  6,  10,  10],#        [ 17, 103, 103],#        [ 17, 103, 103],#        [ 17, 103, 103]])

boolean值索引

names = np.array([&#x27;softpo&#x27;,&#x27;Brandon&#x27;,&#x27;Will&#x27;,&#x27;Michael&#x27;,&#x27;Will&#x27;,&#x27;Ella&#x27;,&#x27;Daniel&#x27;,&#x27;softpo&#x27;,&#x27;Will&#x27;,&#x27;Brandon&#x27;])cond1 = names == &#x27;Will&#x27;cond1 # 输出array([False, False,  True, False,  True, False, False, False,  True, False])names[cond1] # array([&#x27;Will&#x27;, &#x27;Will&#x27;, &#x27;Will&#x27;], dtype=&#x27;&lt;U7&#x27;)arr = np.random.randint(0,100,size = (10,8)) # 0~100随机数cond2 = arr &gt; 90 # 找到所有大于90的索引，返回boolean类型的数组 shape(10,8)，大于返回True，否则Falsearr[cond2] # 返回数据全部是大于90的
第六部分 形状操作
数组变形
import numpy as nparr1 = np.random.randint(0,10,size = (3,4,5))arr2 = arr1.reshape(12,5) # 形状改变，返回新数组arr3 = arr1.reshape(-1,5) # 自动“整形”，自动计算
数组转置
import numpy as nparr1 = np.random.randint(0,10,size = (3,5)) # shape(3,5)arr1.T # shape(5,3) 转置arr2 = np.random.randint(0,10,size = (3,6,4)) # shape(3,6,4)np.transpose(arr2,axes=(2,0,1)) # transpose改变数组维度 shape(4,3,6)
数组堆叠
import numpy as nparr1 = np.array([[1,2,3]])arr2 = np.array([[4,5,6]])np.concatenate([arr1,arr2],axis = 0) # 串联合并shape(2,3) axis = 0表示第一维串联 输出为# array([[1, 2, 3],#        [4, 5, 6]])np.concatenate([arr1,arr2],axis = 1) # shape(1,6) axis = 1表示第二维串联 输出为：array([[1, 2, 3, 4, 5, 6]])np.hstack((arr1,arr2)) # 水平方向堆叠 输出为：array([[1, 2, 3, 4, 5, 6]])np.vstack((arr1,arr2)) # 竖直方向堆叠，输出为：# array([[1, 2, 3],#        [4, 5, 6]])
split数组拆分
import numpy as nparr = np.random.randint(0,10,size = (6,5)) # shape(6,5)np.split(arr,indices_or_sections=2,axis = 0) # 在第一维（6）平均分成两份 np.split(arr,indices_or_sections=[2,3],axis = 1) # 在第二维（5）以索引2，3为断点分割成3份np.vsplit(arr,indices_or_sections=3) # 在竖直方向平均分割成3份np.hsplit(arr,indices_or_sections=[1,4]) # 在水平方向，以索引1，4为断点分割成3份
第七部分 广播机制
当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播（broadcasting）
一维数组广播
import numpy as nparr1 = np.sort(np.array([0,1,2,3]*3)).reshape(4,3) #shape(4,3)arr2 = np.array([1,2,3]) # shape(3,)arr3 = arr1 + arr2 # arr2进行广播复制4份 shape(4,3)arr3
二维数组的广播
import numpy as nparr1 = np.sort(np.array([0,1,2,3]*3)).reshape(4,3) # shape(4,3)arr2 = np.array([[1],[2],[3],[4]]) # shape(4,1)arr3 = arr1 + arr2 # arr2 进行广播复制3份 shape(4,3)arr3
三维数组广播
import numpy as nparr1 = np.array([0,1,2,3,4,5,6,7]*3).reshape(3,4,2) #shape(3,4,2)arr2 = np.array([0,1,2,3,4,5,6,7]).reshape(4,2) #shape(4,2)arr3 = arr1 + arr2 # arr2数组在0维上复制3份 shape(3,4,2)arr3
第八部分 通用函数
第一节 通用函数：元素级数字函数
abs、sqrt、square、exp、log、sin、cos、tan，maxinmum、minimum、all、any、inner、clip、round、trace、ceil、floor
import numpy as nparr1 = np.array([1,4,8,9,16,25])np.sqrt(arr1) # 开平方np.square(arr1) # 平方np.clip(arr1,2,16) # 输出 array([ 2,  4,  8,  9, 16, 16])x = np.array([1,5,2,9,3,6,8])y = np.array([2,4,3,7,1,9,0])np.maximum(x,y) # 返回两个数组中的比较大的值arr2 = np.random.randint(0,10,size = (5,5))np.inner(arr2[0],arr2) #返回一维数组向量内积
第二节 where函数
where 函数，三个参数，条件为真时选择值的数组，条件为假时选择值的数组
import numpy as nparr1 = np.array([1,3,5,7,9])arr2 = np.array([2,4,6,8,10])cond = np.array([True,False,True,True,False])np.where(cond,arr1,arr2) # True选择arr1，False选择arr2的值# 输出 array([ 1,  4,  5,  7, 10])arr3 = np.random.randint(0,30,size = 20)np.where(arr3 &lt; 15,arr3,-15) # 小于15还是自身的值，大于15设置成-15
第三节 排序方法
np中还提供了排序方法，排序方法是就地排序，即直接改变原数组
arr.sort()、np.sort()、arr.argsort()
import numpy as nparr = np.array([9,3,11,6,17,5,4,15,1])arr.sort() # 直接改变原数组np.sort(arr) # 返回深拷贝排序结果arr = np.array([9,3,11,6,17,5,4,15,1])arr.argsort() # 返回从小到大排序索引 array([8, 1, 6, 5, 3, 0, 2, 7, 4])
第四节 集合运算函数
A = np.array([2,4,6,8])B = np.array([3,4,5,6])np.intersect1d(A,B) # 交集 array([4, 6])np.union1d(A,B) # 并集 array([2, 3, 4, 5, 6, 8])np.setdiff1d(A,B) #差集，A中有，B中没有 array([2, 8])
第五节 数学和统计函数
min、max、mean、median、sum、std、var、cumsum、cumprod、argmin、argmax、argwhere、cov、corrcoef
import numpy as nparr1 = np.array([1,7,2,19,23,0,88,11,6,11])arr1.min() # 计算最小值 0arr1.argmax() # 计算最大值的索引 返回 6np.argwhere(arr1 &gt; 20) # 返回大于20的元素的索引np.cumsum(arr1) # 计算累加和arr2 = np.random.randint(0,10,size = (4,5))arr2.mean(axis = 0) # 计算列的平均值arr2.mean(axis = 1) # 计算行的平均值np.cov(arr2,rowvar=True) # 协方差矩阵np.corrcoef(arr2,rowvar=True) # 相关性系数
第九部分 线性代数
矩阵乘积
#矩阵的乘积A = np.array([[4,2,3],              [1,3,1]]) # shape(2,3)B = np.array([[2,7],              [-5,-7],              [9,3]]) # shape(3,2)np.dot(A,B) # 矩阵运算 A的最后一维和B的第一维必须一致A @ B # 符号 @ 表示矩阵乘积运算
矩阵其他计算
下面可以计算矩阵的逆、行列式、特征值和特征向量、qr分解值，svd分解值
#计算矩阵的逆from numpy.linalg import inv,det,eig,qr,svdA = np.array([[1,2,3],              [2,3,4],              [4,5,8]]) # shape(3,3)inv(t) # 逆矩阵det(t)#计算矩阵行列式
第十部分 实战-用NumPy分析鸢尾花花萼属性各项指标
案列：读取iris数据集中的花萼长度数据（已保存为csv格式） 并对其进行排序、去重，并求出和、累积和、均值、标准差、方差、最小值、最大值。
import numpy as np  # 导入类库 numpydata = np.loadtxt(&#x27;./iris.csv&#x27;,delimiter = &#x27;,&#x27;)  # 读取数据文件，data是二维的数组data.sort(axis = -1)  # 简单排序print(&#x27;简单排序后：&#x27;, data)print(&#x27;数据去重后：&#x27;, np.unique(data)) # 去除重复数据print(&#x27;数据求和：&#x27;, np.sum(data))  # 数组求和print(&#x27;元素求累加和&#x27;, np.cumsum(data))  # 元素求累加和print(&#x27;数据的均值：&#x27;, np.mean(data))  # 均值print(&#x27;数据的标准差：&#x27;, np.std(data))  # 标准差print(&#x27;数据的方差：&#x27;, np.var(data))  # 方差print(&#x27;数据的最小值：&#x27;, np.min(data))  # 最小值print(&#x27;数据的最大值：&#x27;, np.max(data))  # 最大值]]></content>
      <categories>
        <category>program</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>NumPy科学计算库</title>
    <url>/posts/e1ca7594.html</url>
    <content><![CDATA[[toc]
NumPy科学计算库
课程介绍
NumPy（Numerical Python）是Python的一种开源的数值计算扩展。提供多维数组对象，各种派生对象（如掩码数组和矩阵），这种工具可用来存储和处理大型矩阵，比Python自身的嵌套列表（nested list structure)结构要高效的多（该结构也可以用来表示矩阵（matrix）），支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库，包括数学、逻辑、形状操作、排序、选择、输入输出、离散傅立叶变换、基本线性代数，基本统计运算和随机模拟等等。


几乎所有从事Python工作的数据分析师都利用NumPy的强大功能。

强大的N维数组
成熟的广播功能
用于整合C/C++和Fortran代码的工具包
NumPy提供了全面的数学功能、随机数生成器和线性代数功能



安装Python库


第一种方式：

pip install jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple



第二种方式：


直接安装anaconda下载


注意：Add Path！！！ 添加一下环境变量~




百度网盘链接: https://pan.baidu.com/s/1sQ8LMH6q8ezVUzNjSCtgyQ 提取码: sm7m



启动终端

Windows----&gt; 快捷键：win + R -----&gt;输入：cmd回车------&gt;命令行出来


Mac ----&gt;启动终端



启动jupyter

进入终端输入指令:jupyter notebook
在哪里启动jupyter启动，浏览器上的目录，对应哪里，windows默认路径是：C:\Users\lufengkun
C:\Users\xxx



第一部分 基本操作
第一节  数组创建
创建数组的最简单的方法就是使用array函数，将Python下的list转换为ndarray。
import numpy as npl = [1,3,5,7,9] # 列表arr = np.array(l) # 将列表转换为NumPy数组arr # 数据一样，NumPy数组的方法，功能更加强大# 输出为# array([1, 3, 5, 7, 9])
我们可以利用np中的一些内置函数来创建数组，比如我们创建全0的数组，也可以创建全1数组，全是其他数字的数组，或者等差数列数组，正态分布数组，随机数。
import numpy as nparr1 = np.ones(10) # 输出为：array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])arr2 = np.zeros(10) # 输出为： array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])arr3 = np.full(shape = [2,3],fill_value=2.718) # 输出为：# array([[2.718, 2.718, 2.718],#       [2.718, 2.718, 2.718]])arr4 = np.arange(start = 0,stop = 20,step = 2) # 等差数列 输出为：array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])arr5 = np.linspace(start =0,stop = 9,num = 10) # 等差数列 输出为：array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])arr6 = np.random.randint(0,100,size = 10) # int随机数 输出为：array([ 4,  8, 79, 62, 34, 35,  2, 65, 47, 18])arr7 = np.random.randn(5) # 正态分布 输出为：array([ 0.57807872,  0.37922855,  2.37936837, -0.28688769,  0.2882854 ])arr8 = np.random.random(size = 5) # float 随机数 输出为：array([0.59646412, 0.37960586, 0.38077327, 0.76983539, 0.22689201])
第二节  查看操作

jupyter扩展插件（不安装）

pip install jupyter_contrib_nbextensions -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install jupyter_nbextensions_configurator -i https://pypi.tuna.tsinghua.edu.cn/simple
jupyter contrib nbextension install --user
jupyter nbextensions_configurator enable --user
退出，重新进入jupyter notebook就可以了



NumPy的数组类称为ndarray，也被称为别名 array。请注意，numpy.array这与标准Python库类不同array.array，后者仅处理一维数组且功能较少。ndarray对象的重要属性是
1.2.1 数组的轴数、维度
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.ndim # 输出 3
1.2.2 数组尺寸形状
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.shape # 输出 (3,4,5)
1.2.3 数组元素的总数
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.size # 输出 3*4*5 = 60
1.2.4 数据类型
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.dtype # 输出 dtype(&#x27;int64&#x27;)
1.2.5 数组中每个元素的大小（以字节为单位）
import numpy as np arr = np.random.randint(0,100,size = (3,4,5))arr.itemsize #输出是 8 ，因为数据类型是int64，64位，一个字节是8位，所以64/8 = 8
第三节 文件IO操作
1.3.1 保存数组
save方法保存ndarray到一个npy文件，也可以使用savez将多个array保存到一个.npz文件中
x = np.random.randn(5)y = np.arange(0,10,1)#save方法可以存一个ndarraynp.save(&quot;x_arr&quot;,x)#如果要存多个数组，要是用savez方法，保存时以key-value形式保存，key任意（xarr、yarr）np.savez(&quot;some_array.npz&quot;,xarr = x,yarr=y)
1.3.2 读取
load方法来读取存储的数组，如果是.npz文件的话，读取之后相当于形成了一个key-value类型的变量，通过保存时定义的key来获取相应的array
np.load(&#x27;x_arr.npy&#x27;) # 直接加载# 通过key获取保存的数组数据np.load(&#x27;some_array.npz&#x27;)[&#x27;yarr&#x27;]
1.3.3 读写csv、txt文件
arr = np.random.randint(0,10,size = (3,4))#储存数组到txt文件np.savetxt(&quot;arr.csv&quot;,arr,delimiter=&#x27;,&#x27;) # 文件后缀是txt也是一样的#读取txt文件，delimiter为分隔符，dtype为数据类型np.loadtxt(&quot;arr.csv&quot;,delimiter=&#x27;,&#x27;,dtype=np.int32)
第二部分 数据类型
ndarray的数据类型：

int: int8、uint8、int16、int32、int64
float: float16、float32、float64
str

array创建时，指定
import numpy as npnp.array([1,2,5,8,2],dtype = &#x27;float32&#x27;) # 输出 ：array([1., 2., 5., 8., 2.], dtype=float32)
asarray转换时指定
import numpy as nparr = [1,3,5,7,2,9,0]# asarray 将列表进行变换np.asarray(arr,dtype = &#x27;float32&#x27;) # 输出：array([1., 3., 5., 7., 2., 9., 0.], dtype=float32)
数据类型转换astype
import numpy as nparr = np.random.randint(0,10,size = 5,dtype = &#x27;int16&#x27;) # 输出：array([6, 6, 6, 6, 3], dtype=int16)# 使用astype进行转换arr.astype(&#x27;float32&#x27;) # 输出：array([1., 4., 0., 6., 6.], dtype=float32)
第三部分 数组运算
加减乘除幂运算
import numpy as nparr1 = np.array([1,2,3,4,5])arr2 = np.array([2,3,1,5,9])arr1 - arr2 # 减法arr1 * arr2 # 乘法arr1 / arr2 # 除法arr1**arr2 # 两个星号表示幂运算
逻辑运算
import numpy as nparr1 = np.array([1,2,3,4,5])arr2 = np.array([1,0,2,3,5])arr1 &lt; 5arr1 &gt;= 5arr1 == 5arr1 == arr2arr1 &gt; arr2
数组与标量计算
数组与标量的算术运算也会将标量值传播到各个元素
import numpy as nparr = np.arange(1,10)1/arrarr+5arr*5
*=、+=、-=操作
某些操作（例如+=和*=）只会修改现有数组，而不是创建一个新数组。
import numpy as nparr1 = np.arange(5)arr1 +=5arr1 -=5arr1 *=5# arr1 /=5 不支持运算
第四部分 复制和视图
在操作数组时，有时会将其数据复制到新数组中，有时不复制。
对于初学者来说，这通常会引起混乱。有以下三种情况
完全没有复制
import numpy as npa = np.random.randint(0,100,size = (4,5))b = aa is b # 返回True a和b是两个不同名字对应同一个内存对象b[0,0] = 1024 # 命运共同体display(a,b)
查看或浅拷贝
不同的数组对象可以共享相同的数据。该view方法创建一个查看相同数据的新数组对象
import numpy as npa = np.random.randint(0,100,size = (4,5))b = a.view() # 使用a中的数据创建一个新数组对象a is b # 返回False a和b是两个不同名字对应同一个内存对象b.base is a # 返回True，b视图的根数据和a一样b.flags.owndata # 返回False b中的数据不是其自己的a.flags.owndata # 返回True a中的数据是其自己的b[0,0] = 1024 # a和b的数据都发生改变display(a,b)
深拷贝
import numpy as npa = np.random.randint(0,100,size = (4,5))b = a.copy()b is a # 返回Falseb.base is a # 返回Falseb.flags.owndata # 返回Truea.flags.owndata # 返回Trueb[0,0] = 1024 # b改变，a不变，分道扬镳display(a,b)


copy应该在不再需要原来的数组情况下，切片后调用。例如，假设a是一个巨大的中间结果，而最终结果b仅包含的一小部分a，则在b使用切片进行构造时应制作一个深拷贝：
import numpy as npa = np.arange(1e8)b = a[::1000000].copy() # 每100万个数据中取一个数据del a # 不在需要a，删除占大内存的ab.shape # shape(100,)


第五部分 索引、切片和迭代
第一节 基本索引和切片
numpy中数组切片是原始数组的视图，这意味着数据不会被复制，视图上任何数据的修改都会反映到原数组上
arr = np.array([0,1,2,3,4,5,6,7,8,9])arr[5] #索引 输出 5arr[5:8] #切片输出：array([5, 6, 7])arr[2::2] # 从索引2开始每两个中取一个 输出 array([2, 4, 6, 8])arr[::3] # 不写索引默认从0开始，每3个中取一个 输出为 array([0, 3, 6, 9])arr[1:7:2] # 从索引1开始到索引7结束，左闭右开，每2个数中取一个 输出 array([1, 3, 5])arr[::-1] # 倒序 输出 array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])arr[::-2] # 倒序 每两个取一个 输出  array([9, 7, 5, 3, 1])arr[5:8]=12 # 切片赋值会赋值到每个元素上，与列表操作不同temp = arr[5:8]temp[1] = 1024arr # 输出：array([   0,    1,    2,    3,    4,   12, 1024,   12,    8,    9])
对于二维数组或者高维数组，我们可以按照之前的知识来索引，当然也可以传入一个以逗号隔开的索引列表来选区单个或多个元素
arr2d = np.array([[1,3,5],[2,4,6],[-2,-7,-9],[6,6,6]]) # 二维数组 shape(3,4)arr2d[0,-1] #索引 等于arr2d[0][-1] 输出 5arr2d[0,2]  #索引 等于arr2d[0][2] ==  arr2d[0][-1] 输出 5arr2d[:2,-2:] #切片 第一维和第二维都进行切片 等于arr2d[:2][:,1:] arr2d[:2,1:] #切片 1 == -2 一个是正序，另个一是倒序，对应相同的位置# 输出：#array([[3, 5],#       [4, 6]])
第二节 花式索引和索引技巧

整数数组进行索引即花式索引,其和切片不一样，它总是将数据复制到新数组中

import numpy as np#一维arr1 = np.array([1,2,3,4,5,6,7,8,9,10])arr2 = arr1[[1,3,3,5,7,7,7]] # 输出 array([2, 4, 4, 6, 8, 8, 8])arr2[-1] = 1024 # 修改值，不影响arr1#二维arr2d = np.array([[1,3,5,7,9],[2,4,6,8,10],[12,18,22,23,37],[123,55,17,88,103]]) #shape(4,5)arr2d[[1,3]] # 获取第二行和第四行，索引从0开始的所以1对应第二行 # 输出 array([[  2,   4,   6,   8,  10],#            [123,  55,  17,  88, 103]])arr2d[([1,3],[2,4])] # 相当于arr2d[1,2]获取一个元素,arr2d[3,4]获取另一个元素# 输出为 array([  6, 103])# 选择一个区域arr2d[np.ix_([1,3,3,3],[2,4,4])] # 相当于 arr2d[[1,3,3,3]][:,[2,4,4]]arr2d[[1,3,3,3]][:,[2,4,4]]# ix_()函数可用于组合不同的向量# 第一个列表存的是待提取元素的行标，第二个列表存的是待提取元素的列标# 输出为# array([[  6,  10,  10],#        [ 17, 103, 103],#        [ 17, 103, 103],#        [ 17, 103, 103]])

boolean值索引

names = np.array([&#x27;softpo&#x27;,&#x27;Brandon&#x27;,&#x27;Will&#x27;,&#x27;Michael&#x27;,&#x27;Will&#x27;,&#x27;Ella&#x27;,&#x27;Daniel&#x27;,&#x27;softpo&#x27;,&#x27;Will&#x27;,&#x27;Brandon&#x27;])cond1 = names == &#x27;Will&#x27;cond1 # 输出array([False, False,  True, False,  True, False, False, False,  True, False])names[cond1] # array([&#x27;Will&#x27;, &#x27;Will&#x27;, &#x27;Will&#x27;], dtype=&#x27;&lt;U7&#x27;)arr = np.random.randint(0,100,size = (10,8)) # 0~100随机数cond2 = arr &gt; 90 # 找到所有大于90的索引，返回boolean类型的数组 shape(10,8)，大于返回True，否则Falsearr[cond2] # 返回数据全部是大于90的
第六部分 形状操作
数组变形
import numpy as nparr1 = np.random.randint(0,10,size = (3,4,5))arr2 = arr1.reshape(12,5) # 形状改变，返回新数组arr3 = arr1.reshape(-1,5) # 自动“整形”，自动计算
数组转置
import numpy as nparr1 = np.random.randint(0,10,size = (3,5)) # shape(3,5)arr1.T # shape(5,3) 转置arr2 = np.random.randint(0,10,size = (3,6,4)) # shape(3,6,4)np.transpose(arr2,axes=(2,0,1)) # transpose改变数组维度 shape(4,3,6)
数组堆叠
import numpy as nparr1 = np.array([[1,2,3]])arr2 = np.array([[4,5,6]])np.concatenate([arr1,arr2],axis = 0) # 串联合并shape(2,3) axis = 0表示第一维串联 输出为# array([[1, 2, 3],#        [4, 5, 6]])np.concatenate([arr1,arr2],axis = 1) # shape(1,6) axis = 1表示第二维串联 输出为：array([[1, 2, 3, 4, 5, 6]])np.hstack((arr1,arr2)) # 水平方向堆叠 输出为：array([[1, 2, 3, 4, 5, 6]])np.vstack((arr1,arr2)) # 竖直方向堆叠，输出为：# array([[1, 2, 3],#        [4, 5, 6]])
split数组拆分
import numpy as nparr = np.random.randint(0,10,size = (6,5)) # shape(6,5)np.split(arr,indices_or_sections=2,axis = 0) # 在第一维（6）平均分成两份 np.split(arr,indices_or_sections=[2,3],axis = 1) # 在第二维（5）以索引2，3为断点分割成3份np.vsplit(arr,indices_or_sections=3) # 在竖直方向平均分割成3份np.hsplit(arr,indices_or_sections=[1,4]) # 在水平方向，以索引1，4为断点分割成3份
第七部分 广播机制
当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播（broadcasting）
一维数组广播

import numpy as nparr1 = np.sort(np.array([0,1,2,3]*3)).reshape(4,3) #shape(4,3)arr2 = np.array([1,2,3]) # shape(3,)arr3 = arr1 + arr2 # arr2进行广播复制4份 shape(4,3)arr3
二维数组的广播

import numpy as nparr1 = np.sort(np.array([0,1,2,3]*3)).reshape(4,3) # shape(4,3)arr2 = np.array([[1],[2],[3],[4]]) # shape(4,1)arr3 = arr1 + arr2 # arr2 进行广播复制3份 shape(4,3)arr3
三维数组广播

import numpy as nparr1 = np.array([0,1,2,3,4,5,6,7]*3).reshape(3,4,2) #shape(3,4,2)arr2 = np.array([0,1,2,3,4,5,6,7]).reshape(4,2) #shape(4,2)arr3 = arr1 + arr2 # arr2数组在0维上复制3份 shape(3,4,2)arr3

第八部分 通用函数
第一节 通用函数：元素级数字函数
abs、sqrt、square、exp、log、sin、cos、tan，maxinmum、minimum、all、any、inner、clip、round、trace、ceil、floor
import numpy as nparr1 = np.array([1,4,8,9,16,25])np.sqrt(arr1) # 开平方np.square(arr1) # 平方np.clip(arr1,2,16) # 输出 array([ 2,  4,  8,  9, 16, 16])x = np.array([1,5,2,9,3,6,8])y = np.array([2,4,3,7,1,9,0])np.maximum(x,y) # 返回两个数组中的比较大的值arr2 = np.random.randint(0,10,size = (5,5))np.inner(arr2[0],arr2) #返回一维数组向量内积
第二节 where函数
where 函数，三个参数，条件为真时选择值的数组，条件为假时选择值的数组
import numpy as nparr1 = np.array([1,3,5,7,9])arr2 = np.array([2,4,6,8,10])cond = np.array([True,False,True,True,False])np.where(cond,arr1,arr2) # True选择arr1，False选择arr2的值# 输出 array([ 1,  4,  5,  7, 10])arr3 = np.random.randint(0,30,size = 20)np.where(arr3 &lt; 15,arr3,-15) # 小于15还是自身的值，大于15设置成-15
第三节 排序方法
np中还提供了排序方法，排序方法是就地排序，即直接改变原数组
arr.sort()、np.sort()、arr.argsort()
import numpy as nparr = np.array([9,3,11,6,17,5,4,15,1])arr.sort() # 直接改变原数组np.sort(arr) # 返回深拷贝排序结果arr = np.array([9,3,11,6,17,5,4,15,1])arr.argsort() # 返回从小到大排序索引 array([8, 1, 6, 5, 3, 0, 2, 7, 4])
第四节 集合运算函数
A = np.array([2,4,6,8])B = np.array([3,4,5,6])np.intersect1d(A,B) # 交集 array([4, 6])np.union1d(A,B) # 并集 array([2, 3, 4, 5, 6, 8])np.setdiff1d(A,B) #差集，A中有，B中没有 array([2, 8])
第五节 数学和统计函数
min、max、mean、median、sum、std、var、cumsum、cumprod、argmin、argmax、argwhere、cov、corrcoef
import numpy as nparr1 = np.array([1,7,2,19,23,0,88,11,6,11])arr1.min() # 计算最小值 0arr1.argmax() # 计算最大值的索引 返回 6np.argwhere(arr1 &gt; 20) # 返回大于20的元素的索引np.cumsum(arr1) # 计算累加和arr2 = np.random.randint(0,10,size = (4,5))arr2.mean(axis = 0) # 计算列的平均值arr2.mean(axis = 1) # 计算行的平均值np.cov(arr2,rowvar=True) # 协方差矩阵np.corrcoef(arr2,rowvar=True) # 相关性系数
第九部分 线性代数
矩阵乘积
#矩阵的乘积A = np.array([[4,2,3],              [1,3,1]]) # shape(2,3)B = np.array([[2,7],              [-5,-7],              [9,3]]) # shape(3,2)np.dot(A,B) # 矩阵运算 A的最后一维和B的第一维必须一致A @ B # 符号 @ 表示矩阵乘积运算
矩阵其他计算
下面可以计算矩阵的逆、行列式、特征值和特征向量、qr分解值，svd分解值
#计算矩阵的逆from numpy.linalg import inv,det,eig,qr,svdA = np.array([[1,2,3],              [2,3,4],              [4,5,8]]) # shape(3,3)inv(t) # 逆矩阵det(t)#计算矩阵行列式
第十部分 实战-用NumPy分析鸢尾花花萼属性各项指标
案列：读取iris数据集中的花萼长度数据（已保存为csv格式）
并对其进行排序、去重，并求出和、累积和、均值、标准差、方差、最小值、最大值。
import numpy as np  # 导入类库 numpydata = np.loadtxt(&#x27;./iris.csv&#x27;,delimiter = &#x27;,&#x27;)  # 读取数据文件，data是二维的数组data.sort(axis = -1)  # 简单排序print(&#x27;简单排序后：&#x27;, data)print(&#x27;数据去重后：&#x27;, np.unique(data)) # 去除重复数据print(&#x27;数据求和：&#x27;, np.sum(data))  # 数组求和print(&#x27;元素求累加和&#x27;, np.cumsum(data))  # 元素求累加和print(&#x27;数据的均值：&#x27;, np.mean(data))  # 均值print(&#x27;数据的标准差：&#x27;, np.std(data))  # 标准差print(&#x27;数据的方差：&#x27;, np.var(data))  # 方差print(&#x27;数据的最小值：&#x27;, np.min(data))  # 最小值print(&#x27;数据的最大值：&#x27;, np.max(data))  # 最大值
]]></content>
      <categories>
        <category>program</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>SVC支持向量机综合案例</title>
    <url>/posts/f219e09d.html</url>
    <content><![CDATA[1、支持向量机原理可视化
1.1、导包
import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom sklearn.svm import SVC
1.2、创建数据
rs = np.random.RandomState(256)X = rs.randn(300,2)y = X[:,0]*X[:,1] &gt;=0plt.figure(figsize=(6,6))plt.scatter(X[:,0],X[:,1])

1.3、创建测试数据
x1 = np.linspace(-3,3,20)x2 = np.linspace(-3,3,18)X1,X2 = np.meshgrid(x1,x2)plt.figure(figsize=(6,6))plt.scatter(X1,X2)X_test = np.concatenate([X1.reshape(-1,1),X2.reshape(-1,1)],axis = 1)X_test.shape

1.4、模型训练
svc = SVC(kernel = &#x27;rbf&#x27;)svc.fit(X,y)y_ = svc.predict(X_test)plt.figure(figsize=(6,6))plt.scatter(X_test[:,0],X_test[:,1],c = y_)

1.5、原理2D可视化
# 二维 --------&gt; 三维# 分离平面，上面一类点，下面也是一类点# 这些点有的距离近，有的距离远d = svc.decision_function(X_test)# 等高面plt.figure(figsize=(5,5))plt.contourf(X1,X2,d.reshape(180,200))

1.6、原理3D可视化
plt.figure(figsize=(12,9))ax = plt.subplot(111,projection = &#x27;3d&#x27;)ax.contourf(X1,X2,d.reshape(180,200))ax.view_init(40,-60)

2、SVC建模人脸识别
2.1、导包
import numpy as npfrom sklearn.svm import SVCfrom sklearn.model_selection import train_test_splitimport matplotlib.pyplot as pltfrom sklearn.decomposition import PCAfrom sklearn.model_selection import GridSearchCVfrom sklearn import datasets
2.2、数据加载
# 第一次加载，需要联网下载# 下载路径：C:\Users\likai\scikit_learn_data\lfw_homefaces = datasets.fetch_lfw_people(resize= 1,min_faces_per_person=70)# 形状是：(125,94)X  = faces[&#x27;data&#x27;]y = faces[&#x27;target&#x27;]display(X.shape,y.shape)
2.3、数据降维与拆分
pca = PCA(n_components=0.9)X_pca = pca.fit_transform(X)display(X_pca.shape)X_train,X_test,X_train_pca,X_test_pca,y_train,y_test = train_test_split(X,X_pca,y,test_size = 0.1)display(X_train.shape,X_test.shape)display(X_train_pca.shape,X_test_pca.shape)
2.4、直接使用SVC建模预测
svc = SVC()svc.fit(X_train_pca,y_train)svc.score(X_test_pca,y_test)# 输出：0.7984496124031008
2.5、网格搜索确定最佳参数
%%timesvc = SVC()params = &#123;&#x27;C&#x27;:np.logspace(-3,1,20),&#x27;kernel&#x27;:[&#x27;rbf&#x27;,&#x27;poly&#x27;,&#x27;sigmoid&#x27;,&#x27;linear&#x27;]&#125;gc = GridSearchCV(estimator = svc,param_grid = params)gc.fit(X_train_pca,y_train)print(&#x27;网格搜索确定最佳参数：&#x27;,gc.best_params_)print(&#x27;模型得分是：&#x27;,gc.score(X_test_pca,y_test))y_pred = gc.predict(X_test_pca)# 输出&#x27;&#x27;&#x27;网格搜索确定最佳参数： &#123;&#x27;C&#x27;: 3.792690190732246, &#x27;kernel&#x27;: &#x27;rbf&#x27;&#125;模型得分是： 0.8837209302325582Wall time: 36.2 s&#x27;&#x27;&#x27;
2.6、数据可视化
target_names = faces.target_namesprint(&#x27;目标任务名字如下：&#x27;,target_names)plt.figure(figsize=(5*2,10*3))for i in range(50):    plt.subplot(10,5,i + 1)    plt.imshow(X_test[i].reshape(125,-1),cmap = &#x27;gray&#x27;)    true_name = target_names[y_test[i]].split(&#x27; &#x27;)[-1]    pred_name = target_names[y_pred[i]].split(&#x27; &#x27;)[-1]    plt.title(&#x27;True:%s\nPred:%s&#x27; % (true_name,pred_name))    plt.axis(&#x27;off&#x27;)

]]></content>
      <categories>
        <category>program</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas数据分析库</title>
    <url>/posts/a67c1f02.html</url>
    <content><![CDATA[

pandas数据分析库
第一部分 课程介绍

Python在数据处理和准备方面一直做得很好，但在数据分析和建模方面就差一些。pandas帮助填补了这一空白，使您能够在Python中执行整个数据分析工作流程，而不必切换到更特定于领域的语言，如R。
与出色的 jupyter工具包和其他库相结合，Python中用于进行数据分析的环境在性能、生产率和协作能力方面都是卓越的。
pandas是 Python 的核心数据分析支持库，提供了快速、灵活、明确的数据结构，旨在简单、直观地处理关系型、标记型数据。pandas是Python进行数据分析的必备高级工具。
pandas的主要数据结构是 **Series(**一维数据)与 DataFrame (二维数据)，这两种数据结构足以处理金融、统计、社会科学、工程等领域里的大多数案例
处理数据一般分为几个阶段：数据整理与清洗、数据分析与建模、数据可视化与制表，Pandas 是处理数据的理想工具。
pip install pandas -i https://pypi.tuna.tsinghua.edu.cn/simple

第二部分 数据结构
第一节 Series
用列表生成 Series时，Pandas 默认自动生成整数索引，也可以指定索引
l = [0,1,7,9,np.NAN,None,1024,512]# 无论是numpy中的NAN还是Python中的None在pandas中都以缺失数据NaN对待s1 = pd.Series(data = l)  # pandas自动添加索引s2 = pd.Series(data = l,index = list(&#x27;abcdefhi&#x27;),dtype=&#x27;float32&#x27;) # 指定行索引# 传入字典创建，key行索引s3 = pd.Series(data = &#123;&#x27;a&#x27;:99,&#x27;b&#x27;:137,&#x27;c&#x27;:149&#125;,name = &#x27;Python_score&#x27;) display(s1,s2,s3)
第二节 DataFrame
DataFrame是由多种类型的列构成的二维标签数据结构，类似于 Excel 、SQL 表，或 Series 对象构成的字典。
import numpy as npimport pandas as pd# index 作为行索引，字典中的key作为列索引，创建了3*3的DataFrame表格二维数组df1 = pd.DataFrame(data = &#123;&#x27;Python&#x27;:[99,107,122],&#x27;Math&#x27;:[111,137,88],&#x27;En&#x27;:[68,108,43]&#125;,# key作为列索引                   index = [&#x27;张三&#x27;,&#x27;李四&#x27;,&#x27;Michael&#x27;]) # 行索引df2 = pd.DataFrame(data = np.random.randint(0,151,size = (5,3)),                   index = [&#x27;Danial&#x27;,&#x27;Brandon&#x27;,&#x27;softpo&#x27;,&#x27;Ella&#x27;,&#x27;Cindy&#x27;],# 行索引                   columns=[&#x27;Python&#x27;,&#x27;Math&#x27;,&#x27;En&#x27;])# 列索引
第三部分 数据查看

查看DataFrame的常用属性和DataFrame的概览和统计信息

import numpy as npimport pandas as pd# 创建 shape(150,3)的二维标签数组结构DataFramedf = pd.DataFrame(data = np.random.randint(0,151,size = (150,3)),                   index = None,# 行索引默认                   columns=[&#x27;Python&#x27;,&#x27;Math&#x27;,&#x27;En&#x27;])# 列索引# 查看其属性、概览和统计信息df.head(10) # 显示头部10行，默认5个df.tail(10) # 显示末尾10行，默认5个df.shape # 查看形状，行数和列数df.dtypes # 查看数据类型df.index # 行索引df.columns # 列索引df.values # 对象值，二维ndarray数组df.describe() # 查看数值型列的汇总统计,计数、平均值、标准差、最小值、四分位数、最大值df.info() # 查看列索引、数据类型、非空计数和内存信息
第四部分 数据输入与输出
第一节 csv
import numpy as npimport pandas as pddf = DataFrame(data = np.random.randint(0,50,size = [50,5]), # 薪资情况               columns=[&#x27;IT&#x27;,&#x27;化工&#x27;,&#x27;生物&#x27;,&#x27;教师&#x27;,&#x27;士兵&#x27;])# 保存到当前路径下，文件命名是：salary.csv。csv逗号分割值文件格式df.to_csv(&#x27;./salary.csv&#x27;,          sep = &#x27;;&#x27;, # 文本分隔符，默认是逗号          header = True,# 是否保存列索引          index = True) # 是否保存行索引，保存行索引，文件被加载时，默认行索引会作为一列# 加载pd.read_csv(&#x27;./salary.csv&#x27;,            sep = &#x27;;&#x27;,# 默认是逗号            header = [0],#指定列索引            index_col=0) # 指定行索引pd.read_table(&#x27;./salary.csv&#x27;, # 和read_csv类似，读取限定分隔符的文本文件            sep = &#x27;;&#x27;,            header = [0],#指定列索引            index_col=1) # 指定行索引,IT作为行索引
第二节 Excel
pip install xlrd -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install xlwt -i https://pypi.tuna.tsinghua.edu.cn/simple
import numpy as npimport pandas as pddf1 = pd.DataFrame(data = np.random.randint(0,50,size = [50,5]), # 薪资情况               columns=[&#x27;IT&#x27;,&#x27;化工&#x27;,&#x27;生物&#x27;,&#x27;教师&#x27;,&#x27;士兵&#x27;])df2 = pd.DataFrame(data = np.random.randint(0,50,size = [150,3]),# 计算机科目的考试成绩                   columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])# 保存到当前路径下，文件命名是：salary.xlsdf1.to_excel(&#x27;./salary.xls&#x27;,            sheet_name = &#x27;salary&#x27;,# Excel中工作表的名字            header = True,# 是否保存列索引            index = False) # 是否保存行索引，保存行索引pd.read_excel(&#x27;./salary.xls&#x27;,              sheet_name=0,# 读取哪一个Excel中工作表，默认第一个              header = 0,# 使用第一行数据作为列索引              names = list(&#x27;ABCDE&#x27;),# 替换行索引              index_col=1)# 指定行索引，B作为行索引# 一个Excel文件中保存多个工作表with pd.ExcelWriter(&#x27;./data.xlsx&#x27;) as writer:    df1.to_excel(writer,sheet_name=&#x27;salary&#x27;,index = False)    df2.to_excel(writer,sheet_name=&#x27;score&#x27;,index = False)pd.read_excel(&#x27;./data.xlsx&#x27;,              sheet_name=&#x27;salary&#x27;) # 读取Excel中指定名字的工作表 
第三节 SQL
pip install sqlalchemy -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install pymysql -i https://pypi.tuna.tsinghua.edu.cn/simple
数据库引擎配置
import pandas as pd# SQLAlchemy是Python编程语言下的一款开源软件。提供了SQL工具包及对象关系映射（ORM）工具from sqlalchemy import create_enginedf = pd.DataFrame(data = np.random.randint(0,50,size = [150,3]),# 计算机科目的考试成绩                   columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])# 数据库连接conn = create_engine(&#x27;mysql+pymysql://root:12345678@localhost/pandas?charset=UTF8MB4&#x27;)# 保存到数据库df.to_sql(&#x27;score&#x27;,#数据库中表名          conn,# 数据库连接          if_exists=&#x27;append&#x27;)#如果表名存在，追加数据# 从数据库中加载pd.read_sql(&#x27;select * from score limit 10&#x27;, # sql查询语句            conn, # 数据库连接            index_col=&#x27;Python&#x27;) # 指定行索引名
第四节 HDF5
pip install tables -i https://pypi.tuna.tsinghua.edu.cn/simple
HDF5是一个独特的技术套件，可以管理非常大和复杂的数据收集。
HDF5，可以存储不同类型数据的文件格式，后缀通常是.h5，它的结构是层次性的。
一个HDF5文件可以被看作是一个组包含了各类不同的数据集。
对于HDF5文件中的数据存储，有两个核心概念：group 和 dataset
dataset 代表数据集，一个文件当中可以存放不同种类的数据集，这些数据集如何管理，就用到了group
最直观的理解，可以参考我们的文件管理系统，不同的文件位于不同的目录下。
目录就是HDF5中的group, 描述了数据集dataset的分类信息，通过group 有效的将多种dataset 进行管理和区分；文件就是HDF5中的dataset, 表示的是具体的数据。
import numpy as npimport pandas as pddf1 = pd.DataFrame(data = np.random.randint(0,50,size = [50,5]), # 薪资情况               columns=[&#x27;IT&#x27;,&#x27;化工&#x27;,&#x27;生物&#x27;,&#x27;教师&#x27;,&#x27;士兵&#x27;])df2 = pd.DataFrame(data = np.random.randint(0,50,size = [150,3]),# 计算机科目的考试成绩                   columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])# 保存到当前路径下，文件命名是：data.h5df1.to_hdf(&#x27;./data.h5&#x27;,key=&#x27;salary&#x27;) # 保存数据的key，标记df2.to_hdf(&#x27;./data.h5&#x27;,key = &#x27;score&#x27;)pd.read_hdf(&#x27;./data.h5&#x27;,            key = &#x27;salary&#x27;)#获取指定的标记、key的数据
第五部分 数据选取
第一节 字段数据
import pandas as pdimport numpy as npdf = pd.DataFrame(data = np.random.randint(0,150,size = [150,3]),# 计算机科目的考试成绩                   columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])df[&#x27;Python&#x27;] # 获取单列，Seriesdf.Python # 获取单列，Seriesdf[[&#x27;Python&#x27;,&#x27;Keras&#x27;]] # 获取多列，DataFramedf[3:15] # 行切片
第二节 标签选择
import pandas as pdimport numpy as npdf = pd.DataFrame(data = np.random.randint(0,150,size = [10,3]),# 计算机科目的考试成绩                  index = list(&#x27;ABCDEFGHIJ&#x27;),# 行标签                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])df.loc[[&#x27;A&#x27;,&#x27;C&#x27;,&#x27;D&#x27;,&#x27;F&#x27;]] # 选取指定行标签数据。df.loc[&#x27;A&#x27;:&#x27;E&#x27;,[&#x27;Python&#x27;,&#x27;Keras&#x27;]] # 根据行标签切片，选取指定列标签的数据df.loc[:,[&#x27;Keras&#x27;,&#x27;Tensorflow&#x27;]] # :默认保留所有行df.loc[&#x27;E&#x27;::2,&#x27;Python&#x27;:&#x27;Tensorflow&#x27;] # 行切片从标签E开始每2个中取一个，列标签进行切片df.loc[&#x27;A&#x27;,&#x27;Python&#x27;] # 选取标量值
第三节 位置选择
import pandas as pdimport numpy as npdf = pd.DataFrame(data = np.random.randint(0,150,size = [10,3]),# 计算机科目的考试成绩                  index = list(&#x27;ABCDEFGHIJ&#x27;),# 行标签                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])df.iloc[4] # 用整数位置选择。df.iloc[2:8,0:2] # 用整数切片，类似NumPydf.iloc[[1,3,5],[0,2,1]] # 整数列表按位置切片df.iloc[1:3,:] # 行切片df.iloc[:,:2] # 列切片df.iloc[0,2] # 选取标量值
第四节 boolean索引
import pandas as pdimport numpy as npdf = pd.DataFrame(data = np.random.randint(0,150,size = [10,3]),# 计算机科目的考试成绩                  index = list(&#x27;ABCDEFGHIJ&#x27;),# 行标签，用户                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;]) # 考试科目cond1 = df.Python &gt; 100 #  判断Python分数是否大于100，返回值是boolean类型的Seriesdf[cond1] # 返回Python分数大于100分的用户所有考试科目数据cond2 = (df.Python &gt; 50) &amp; (df[&#x27;Keras&#x27;] &gt; 50) # &amp;与运算df[cond2] # 返回Python和Keras同时大于50分的用户的所有考试科目数据df[df &gt; 50]# 选择DataFrame中满足条件的值，如果满足返回值，不然返回空数据NaNdf[df.index.isin([&#x27;A&#x27;,&#x27;C&#x27;,&#x27;F&#x27;])] # isin判断是否在数组中，返回也是boolean类型值
第五节 赋值操作
import pandas as pdimport numpy as npdf = pd.DataFrame(data = np.random.randint(0,150,size = [10,3]),# 计算机科目的考试成绩                  index = list(&#x27;ABCDEFGHIJ&#x27;),# 行标签，用户                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;]) # 考试科目s = pd.Series(data = np.random.randint(0,150,size = 9),index=list(&#x27;BCDEFGHIJ&#x27;),name = &#x27;PyTorch&#x27;)df[&#x27;PyTorch&#x27;] = s # 增加一列，DataFrame行索引自动对齐df.loc[&#x27;A&#x27;,&#x27;Python&#x27;] = 256 # 按标签赋值df.iloc[3,2] = 512 # 按位置赋值df.loc[:,&#x27;Python&#x27;] = np.array([128]*10) # 按NumPy数组进行赋值df[df &gt;= 128] = -df # 按照where条件进行赋值，大于等于128变成原来的负数，否则不变df
第六部分 数据集成
pandas 提供了多种将 Series、DataFrame 对象组合在一起的功能
第一节 concat数据串联
import pandas as pdimport numpy as npdf1 = pd.DataFrame(data = np.random.randint(0,150,size = [10,3]),# 计算机科目的考试成绩                  index = list(&#x27;ABCDEFGHIJ&#x27;),# 行标签，用户                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;]) # 考试科目df2 = pd.DataFrame(data = np.random.randint(0,150,size = [10,3]),# 计算机科目的考试成绩                  index = list(&#x27;KLMNOPQRST&#x27;),# 行标签，用户                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;]) # 考试科目df3 = pd.DataFrame(data = np.random.randint(0,150,size = (10,2)),                  index = list(&#x27;ABCDEFGHIJ&#x27;),                  columns=[&#x27;PyTorch&#x27;,&#x27;Paddle&#x27;])pd.concat([df1,df2],axis = 0) # df1和df2行串联，df2的行追加df2行后面df1.append(df2) # 在df1后面追加df2pd.concat([df1,df3],axis = 1) # df1和df2列串联，df2的列追加到df1列后面
第二节 插入
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,151,size = (10,3)),                  index = list(&#x27;ABCDEFGHIJ&#x27;),                  columns = [&#x27;Python&#x27;,&#x27;Keras&#x27;,&#x27;Tensorflow&#x27;])df.insert(loc = 1,column=&#x27;Pytorch&#x27;,value=1024) # 插入列df# 对行的操作，使用追加append，默认在最后面，无法指定位置# 如果想要在指定位置插入行：切割-添加-合并
第三节 Join SQL风格合并
数据集的合并（merge）或连接（join）运算是通过一个或者多个键将数据链接起来的。这些运算是关系型数据库的核心操作。pandas的merge函数是数据集进行join运算的主要切入点。
import pandas as pdimport numpy as np# 表一中记录的是name和体重信息df1 = pd.DataFrame(data = &#123;&#x27;name&#x27;:[&#x27;softpo&#x27;,&#x27;Daniel&#x27;,&#x27;Brandon&#x27;,&#x27;Ella&#x27;],&#x27;weight&#x27;:[70,55,75,65]&#125;)# 表二中记录的是name和身高信息df2 = pd.DataFrame(data = &#123;&#x27;name&#x27;:[&#x27;softpo&#x27;,&#x27;Daniel&#x27;,&#x27;Brandon&#x27;,&#x27;Cindy&#x27;],&#x27;height&#x27;:[172,170,170,166]&#125;)df3 = pd.DataFrame(data = &#123;&#x27;名字&#x27;:[&#x27;softpo&#x27;,&#x27;Daniel&#x27;,&#x27;Brandon&#x27;,&#x27;Cindy&#x27;],&#x27;height&#x27;:[172,170,170,166]&#125;)# 根据共同的name将俩表的数据，进行合并pd.merge(df1,df2,         how = &#x27;inner&#x27;,# 内合并代表两对象交集         on = &#x27;name&#x27;)pd.merge(df1,df3,         how = &#x27;outer&#x27;,# 全外连接，两对象并集         left_on = &#x27;name&#x27;,# 左边DataFrame使用列标签 name进行合并         right_on = &#x27;名字&#x27;)# 右边DataFrame使用列标签 名字进行合并# 创建10名学生的考试成绩df4 = pd.DataFrame(data = np.random.randint(0,151,size = (10,3)),                   index = list(&#x27;ABCDEFHIJK&#x27;),                   columns=[&#x27;Python&#x27;,&#x27;Keras&#x27;,&#x27;Tensorflow&#x27;])# 计算每位学生各科平均分，转换成DataFramescore_mean = pd.DataFrame(df4.mean(axis = 1).round(1),columns=[&#x27;平均分&#x27;])# 将平均分和df3使用merge进行合并，它俩有共同的行索引pd.merge(left = df4,right = score_mean,         left_index=True,# 左边DataFrame使用行索引进行合并         right_index=True)# 右边的DataFrame使用行索引进行合并
第七部分 数据清洗
import numpy as npimport pandas as pddf = pd.DataFrame(data = &#123;&#x27;color&#x27;:[&#x27;red&#x27;,&#x27;blue&#x27;,&#x27;red&#x27;,&#x27;green&#x27;,&#x27;blue&#x27;,None,&#x27;red&#x27;],                          &#x27;price&#x27;:[10,20,10,15,20,0,np.NaN]&#125;)# 1、重复数据过滤df.duplicated() # 判断是否存在重复数据df.drop_duplicates() # 删除重复数据# 2、空数据过滤df.isnull() # 判断是否存在空数据，存在返回True，否则返回Falsedf.dropna(how = &#x27;any&#x27;) # 删除空数据df.fillna(value=1111) # 填充空数据# 3、指定行或者列过滤del df[&#x27;color&#x27;] # 直接删除某列df.drop(labels = [&#x27;price&#x27;],axis = 1)# 删除指定列df.drop(labels = [0,1,5],axis = 0) # 删除指定行# 4、函数filter使用df = pd.DataFrame(np.array(([3,7,1], [2, 8, 256])),                  index=[&#x27;dog&#x27;, &#x27;cat&#x27;],                  columns=[&#x27;China&#x27;, &#x27;America&#x27;, &#x27;France&#x27;])df.filter(items=[&#x27;China&#x27;, &#x27;France&#x27;])# 根据正则表达式删选列标签df.filter(regex=&#x27;a$&#x27;, axis=1)# 选择行中包含ogdf.filter(like=&#x27;og&#x27;, axis=0)# 5、异常值过滤df2 = pd.DataFrame(data = np.random.randn(10000,3)) # 正态分布数据# 3σ过滤异常值，σ即是标准差cond = (df2 &gt; 3*df2.std()).any(axis = 1)index = df2[cond].index # 不满足条件的行索引df2.drop(labels=index,axis = 0) # 根据行索引，进行数据删除
第八部分 数据转换
第一节 轴和元素替换
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,10,size = (10,3)),                  index = list(&#x27;ABCDEFHIJK&#x27;),                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])df.iloc[4,2] = None # 空数据#1、重命名轴索引df.rename(index = &#123;&#x27;A&#x27;:&#x27;AA&#x27;,&#x27;B&#x27;:&#x27;BB&#x27;&#125;,columns = &#123;&#x27;Python&#x27;:&#x27;人工智能&#x27;&#125;) # 2、替换值df.replace(3,1024) #将3替换为1024df.replace([0,7],2048) # 将0和7替换为2048df.replace(&#123;0:512,np.nan:998&#125;) # 根据字典键值对进行替换df.replace(&#123;&#x27;Python&#x27;:2&#125;,-1024) # 将Python这一列中等于2的，替换为-1024
第二节 map Series元素改变
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,10,size = (10,3)),                  index = list(&#x27;ABCDEFHIJK&#x27;),                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])df.iloc[4,2] = None # 空数据# 1、map批量元素改变，Series专有df[&#x27;Keras&#x27;].map(&#123;1:&#x27;Hello&#x27;,5:&#x27;World&#x27;,7:&#x27;AI&#x27;&#125;) # 字典映射df[&#x27;Python&#x27;].map(lambda x:True if x &gt;=5 else False) # 隐式函数映射def convert(x): # 显示函数映射    if x%3 == 0:        return True    elif x%3 == 1:        return Falsedf[&#x27;Tensorflow&#x27;].map(convert)
第三节 apply元素改变。既支持 Series，也支持 DataFrame
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,10,size = (10,3)),                  index = list(&#x27;ABCDEFHIJK&#x27;),                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])df.iloc[4,2] = None # 空数据# 1、apply 应用方法数据转换，通用# Series，其中x是Series中元素df[&#x27;Keras&#x27;].apply(lambda x:True if x &gt;5 else False) # DataFrame，其中的x是DataFrame中列或者行，是Seriesdf.apply(lambda x : x.median(),axis = 0) # 列的中位数def convert(x): # 自定义方法    return (x.mean().round(1),x.count())df.apply(convert,axis = 1) # 行平均值，计数# 2、applymap DataFrame专有df.applymap(lambda x : x + 100) # 计算DataFrame中每个元素
第四节 transform变形金刚
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,10,size = (10,3)),                  index = list(&#x27;ABCDEFHIJK&#x27;),                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])df.iloc[4,2] = None # 空数据# 1、一列执行多项计算df[&#x27;Python&#x27;].transform([np.sqrt,np.exp]) # Series处理def convert(x):    if x.mean() &gt; 5:        x *= 10    else:        x *= -10    return x# 2、多列执行不同计算df.transform(&#123;&#x27;Python&#x27;:convert,&#x27;Tensorflow&#x27;:np.max,&#x27;Keras&#x27;:np.min&#125;) # DataFrame处理
第五节 重排随机抽样哑变量
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,10,size = (10,3)),                  index = list(&#x27;ABCDEFHIJK&#x27;),                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])ran = np.random.permutation(10) # 随机重排df.take(ran) # 重排DataFramedf.take(np.random.randint(0,10,size = 15)) # 随机抽样# 哑变量，独热编码，1表示有，0表示没有df = pd.DataFrame(&#123;&#x27;key&#x27;:[&#x27;b&#x27;,&#x27;b&#x27;,&#x27;a&#x27;,&#x27;c&#x27;,&#x27;a&#x27;,&#x27;b&#x27;]&#125;)pd.get_dummies(df,prefix=&#x27;&#x27;,prefix_sep=&#x27;&#x27;)
第九部分 数据重塑
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,100,size = (10,3)),                  index = list(&#x27;ABCDEFHIJK&#x27;),                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])df.T # 转置df2 = pd.DataFrame(data = np.random.randint(0,100,size = (20,3)),                   index = pd.MultiIndex.from_product([list(&#x27;ABCDEFHIJK&#x27;),[&#x27;期中&#x27;,&#x27;期末&#x27;]]),#多层索引                   columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])df2.unstack(level = -1) # 行旋转成列，level指定哪一层，进行变换df2.stack() # 列旋转成行df2.stack().unstack(level = 1) # 行列互换# 多层索引DataFrame数学计算df2.mean() # 各学科平均分df2.mean(level=0) # 各学科，每个人期中期末平均分df2.mean(level = 1) # 各学科，期中期末所有人平均分
第十部分 数学和统计方法
pandas对象拥有一组常用的数学和统计方法。它们属于汇总统计，对Series汇总计算获取mean、max值或者对DataFrame行、列汇总计算返回一个Series。
第一节 简单统计指标
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,100,size = (20,3)),                  index = list(&#x27;ABCDEFHIJKLMNOPQRSTU&#x27;),                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])# 1、简单统计指标df.count() # 非NA值的数量df.max(axis = 0) #轴0最大值，即每一列最大值df.min() #默认计算轴0最小值df.median() # 中位数df.sum() # 求和df.mean(axis = 1) #轴1平均值，即每一行的平均值df.quantile(q = [0.2,0.4,0.8]) # 分位数df.describe() # 查看数值型列的汇总统计,计数、平均值、标准差、最小值、四分位数、最大值
第二节 索引标签、位置获取
# 2、索引位置df[&#x27;Python&#x27;].argmin() # 计算最小值位置df[&#x27;Keras&#x27;].argmax() # 最大值位置df.idxmax() # 最大值索引标签df.idxmin() # 最小值索引标签
第三节 更多统计指标
# 3、更多统计指标df[&#x27;Python&#x27;].value_counts() # 统计元素出现次数df[&#x27;Keras&#x27;].unique() # 去重df.cumsum() # 累加df.cumprod() # 累乘df.std() # 标准差df.var() # 方差df.cummin() # 累计最小值df.cummax() # 累计最大值df.diff() # 计算差分df.pct_change() # 计算百分比变化
第四节 高级统计指标
# 4、高级统计指标df.cov() # 属性的协方差df[&#x27;Python&#x27;].cov(df[&#x27;Keras&#x27;]) # Python和Keras的协方差df.corr() # 所有属性相关性系数df.corrwith(df[&#x27;Tensorflow&#x27;]) # 单一属性相关性系数
协方差：Cov(X,Y)=fracsumlimits_1n(X_i−overlineX)(Y_i−overlineY)n−1Cov(X,Y) = \\frac{\\sum\\limits\_1^n(X\_i - \\overline{X})(Y\_i - \\overline{Y})}{n-1}Cov(X,Y)=fracsumlimits_1n(X_i−overlineX)(Y_i−overlineY)n−1
相关性系数：r(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{Var\[X\]Var\[Y\]}}
第十一部分 数据排序
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,30,size = (30,3)),                  index = list(&#x27;qwertyuioijhgfcasdcvbnerfghjcf&#x27;),                  columns = [&#x27;Python&#x27;,&#x27;Keras&#x27;,&#x27;Pytorch&#x27;])# 1、索引列名排序df.sort_index(axis = 0,ascending=True) # 按索引排序，降序df.sort_index(axis = 1,ascending=False) #按列名排序，升序# 2、属性值排序df.sort_values(by = [&#x27;Python&#x27;]) #按Python属性值排序df.sort_values(by = [&#x27;Python&#x27;,&#x27;Keras&#x27;])#先按Python，再按Keras排序# 3、返回属性n大或者n小的值df.nlargest(10,columns=&#x27;Keras&#x27;) # 根据属性Keras排序,返回最大10个数据df.nsmallest(5,columns=&#x27;Python&#x27;) # 根据属性Python排序，返回最小5个数据
第十二部分 分箱操作
分箱操作就是将连续数据转换为分类对应物的过程。比如将连续的身高数据划分为：矮中高。
分箱操作分为等距分箱和等频分箱。
分箱操作也叫面元划分或者离散化。
import numpy as npimport pandas as pddf = pd.DataFrame(data = np.random.randint(0,150,size = (100,3)),                  columns=[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;])# 1、等宽分箱pd.cut(df.Python,bins = 3)# 指定宽度分箱pd.cut(df.Keras,#分箱数据       bins = [0,60,90,120,150],#分箱断点       right = False,# 左闭右开       labels=[&#x27;不及格&#x27;,&#x27;中等&#x27;,&#x27;良好&#x27;,&#x27;优秀&#x27;])# 分箱后分类# 2、等频分箱pd.qcut(df.Python,q = 4,# 4等分        labels=[&#x27;差&#x27;,&#x27;中&#x27;,&#x27;良&#x27;,&#x27;优&#x27;]) # 分箱后分类
第十三部分 分组聚合
第一节 分组
import numpy as npimport pandas as pd# 准备数据df = pd.DataFrame(data = &#123;&#x27;sex&#x27;:np.random.randint(0,2,size = 300), # 0男，1女                          &#x27;class&#x27;:np.random.randint(1,9,size = 300),#1~8八个班                          &#x27;Python&#x27;:np.random.randint(0,151,size = 300),#Python成绩                          &#x27;Keras&#x27;:np.random.randint(0,151,size =300),#Keras成绩                          &#x27;Tensorflow&#x27;:np.random.randint(0,151,size=300),                          &#x27;Java&#x27;:np.random.randint(0,151,size = 300),                          &#x27;C++&#x27;:np.random.randint(0,151,size = 300)&#125;)df[&#x27;sex&#x27;] = df[&#x27;sex&#x27;].map(&#123;0:&#x27;男&#x27;,1:&#x27;女&#x27;&#125;) # 将0，1映射成男女# 1、分组-&gt;可迭代对象# 1.1 先分组再获取数据g = df.groupby(by = &#x27;sex&#x27;)[[&#x27;Python&#x27;,&#x27;Java&#x27;]] # 单分组for name,data in g:    print(&#x27;组名：&#x27;,name)    print(&#x27;数据：&#x27;,data)df.groupby(by = [&#x27;class&#x27;,&#x27;sex&#x27;])[[&#x27;Python&#x27;]] # 多分组# 1.2 对一列值进行分组df[&#x27;Python&#x27;].groupby(df[&#x27;class&#x27;]) # 单分组df[&#x27;Keras&#x27;].groupby([df[&#x27;class&#x27;],df[&#x27;sex&#x27;]]) # 多分组# 1.3 按数据类型分组df.groupby(df.dtypes,axis = 1)# 1.4 通过字典进行分组m = &#123;&#x27;sex&#x27;:&#x27;category&#x27;,&#x27;class&#x27;:&#x27;category&#x27;,&#x27;Python&#x27;:&#x27;IT&#x27;,&#x27;Keras&#x27;:&#x27;IT&#x27;,&#x27;Tensorflow&#x27;:&#x27;IT&#x27;,&#x27;Java&#x27;:&#x27;IT&#x27;,&#x27;C++&#x27;:&#x27;IT&#x27;&#125;for name,data in df.groupby(m,axis = 1):    print(&#x27;组名&#x27;,name)    print(&#x27;数据&#x27;,data)
第二节 分组聚合
# 2、分组直接调用函数进行聚合# 按照性别分组，其他列均值聚合df.groupby(by = &#x27;sex&#x27;).mean().round(1) # 保留1位小数# 按照班级和性别进行分组，Python、Keras的最大值聚合df.groupby(by = [&#x27;class&#x27;,&#x27;sex&#x27;])[[&#x27;Python&#x27;,&#x27;Keras&#x27;]].max()# 按照班级和性别进行分组，计数聚合。统计每个班，男女人数df.groupby(by = [&#x27;class&#x27;,&#x27;sex&#x27;]).size()# 基本描述性统计聚合df.groupby(by = [&#x27;class&#x27;,&#x27;sex&#x27;]).describe()
第三节 分组聚合apply、transform
# 3、分组后调用apply，transform封装单一函数计算# 返回分组结果df.groupby(by = [&#x27;class&#x27;,&#x27;sex&#x27;])[[&#x27;Python&#x27;,&#x27;Keras&#x27;]].apply(np.mean).round(1)def normalization(x):    return (x - x.min())/(x.max() - x.min()) # 最大值最小值归一化# 返回全数据，返回DataFrame.shape和原DataFrame.shape一样。df.groupby(by = [&#x27;class&#x27;,&#x27;sex&#x27;])[[&#x27;Python&#x27;,&#x27;Tensorflow&#x27;]].transform(normalization).round(3)
第四节 分组聚合agg
# 4、agg 多中统计汇总操作# 分组后调用agg应用多种统计汇总df.groupby(by = [&#x27;class&#x27;,&#x27;sex&#x27;])[[&#x27;Tensorflow&#x27;,&#x27;Keras&#x27;]].agg([np.max,np.min,pd.Series.count])# 分组后不同属性应用多种不同统计汇总df.groupby(by = [&#x27;class&#x27;,&#x27;sex&#x27;])[[&#x27;Python&#x27;,&#x27;Keras&#x27;]].agg(&#123;&#x27;Python&#x27;:[(&#x27;最大值&#x27;,np.max),(&#x27;最小值&#x27;,np.min)],                                                          &#x27;Keras&#x27;:[(&#x27;计数&#x27;,pd.Series.count),(&#x27;中位数&#x27;,np.median)]&#125;)
第五节 透视表pivot_table
# 5、透视表# 透视表也是一种分组聚合运算def count(x):    return len(x)df.pivot_table(values=[&#x27;Python&#x27;,&#x27;Keras&#x27;,&#x27;Tensorflow&#x27;],# 要透视分组的值               index=[&#x27;class&#x27;,&#x27;sex&#x27;], # 分组透视指标               aggfunc=&#123;&#x27;Python&#x27;:[(&#x27;最大值&#x27;,np.max)], # 聚合运算                        &#x27;Keras&#x27;:[(&#x27;最小值&#x27;,np.min),(&#x27;中位数&#x27;,np.median)],                        &#x27;Tensorflow&#x27;:[(&#x27;最小值&#x27;,np.min),(&#x27;平均值&#x27;,np.mean),(&#x27;计数&#x27;,count)]&#125;)
第十四部分 时间序列
第一节 时间戳操作
# 1、创建方法pd.Timestamp(&#x27;2020-8-24 12&#x27;)# 时刻数据pd.Period(&#x27;2020-8-24&#x27;,freq = &#x27;M&#x27;) # 时期数据index = pd.date_range(&#x27;2020.08.24&#x27;,periods=5,freq = &#x27;M&#x27;) # 批量时刻数据pd.period_range(&#x27;2020.08.24&#x27;,periods=5,freq=&#x27;M&#x27;) # 批量时期数据ts = pd.Series(np.random.randint(0,10,size = 5),index = index) # 时间戳索引Series# 2、转换方法pd.to_datetime([&#x27;2020.08.24&#x27;,&#x27;2020-08-24&#x27;,&#x27;24/08/2020&#x27;,&#x27;2020/8/24&#x27;])pd.to_datetime([1598582232],unit=&#x27;s&#x27;)dt = pd.to_datetime([1598582420401],unit = &#x27;ms&#x27;) # 世界标准时间dt + pd.DateOffset(hours = 8) # 东八区时间dt + pd.DateOffset(days = 100) # 100天后日期
第二节 时间戳索引
index = pd.date_range(&quot;2020-8-24&quot;, periods=200, freq=&quot;D&quot;)ts = pd.Series(range(len(index)), index=index)# str类型索引ts[&#x27;2020-08-30&#x27;] # 日期访问数据ts[&#x27;2020-08-24&#x27;:&#x27;2020-09-3&#x27;] # 日期切片ts[&#x27;2020-08&#x27;] # 传入年月ts[&#x27;2020&#x27;] # 传入年# 时间戳索引ts[pd.Timestamp(&#x27;2020-08-30&#x27;)]ts[pd.Timestamp(&#x27;2020-08-24&#x27;):pd.Timestamp(&#x27;2020-08-30&#x27;)] # 切片ts[pd.date_range(&#x27;2020-08-24&#x27;,periods=10,freq=&#x27;D&#x27;)]# 时间戳索引属性ts.index.year # 获取年ts.index.dayofweek # 获取星期几ts.index.weekofyear # 一年中第几个星期几
第三节 时间序列常用方法
在做时间序列相关的工作时，经常要对时间做一些移动/滞后、频率转换、采样等相关操作，我们来看下这些操作如何使用
index = pd.date_range(&#x27;8/1/2020&#x27;, periods=365, freq=&#x27;D&#x27;)ts = pd.Series(np.random.randint(0, 500, len(index)), index=index)# 1、移动ts.shift(periods = 2) #  数据后移 ts.shift(periods = -2) # 数据前移# 日期移动ts.shift(periods = 2,freq = pd.tseries.offsets.Day()) # 天移动ts.tshift(periods = 1,freq = pd.tseries.offsets.MonthOffset()) #月移动# 2、频率转换ts.asfreq(pd.tseries.offsets.Week()) # 天变周ts.asfreq(pd.tseries.offsets.MonthEnd()) # 天变月ts.asfreq(pd.tseries.offsets.Hour(),fill_value = 0) #天变小时，又少变多，fill_value为填充值# 3、重采样# resample 表示根据日期维度进行数据聚合，可以按照分钟、小时、工作日、周、月、年等来作为日期维度ts.resample(&#x27;2W&#x27;).sum() # 以2周为单位进行汇总ts.resample(&#x27;3M&#x27;).sum().cumsum() # 以季度为单位进行汇总# 4、DataFrame重采样d = dict(&#123;&#x27;price&#x27;: [10, 11, 9, 13, 14, 18, 17, 19],          &#x27;volume&#x27;: [50, 60, 40, 100, 50, 100, 40, 50],          &#x27;week_starting&#x27;:pd.date_range(&#x27;24/08/2020&#x27;,periods=8,freq=&#x27;W&#x27;)&#125;)df1 = pd.DataFrame(d)df1.resample(&#x27;M&#x27;,on = &#x27;week_starting&#x27;).apply(np.sum)df1.resample(&#x27;M&#x27;,on = &#x27;week_starting&#x27;).agg(&#123;&#x27;price&#x27;:np.mean,&#x27;volume&#x27;:np.sum&#125;)days = pd.date_range(&#x27;1/8/2020&#x27;, periods=4, freq=&#x27;D&#x27;)data2 = dict(&#123;&#x27;price&#x27;: [10, 11, 9, 13, 14, 18, 17, 19],           &#x27;volume&#x27;: [50, 60, 40, 100, 50, 100, 40, 50]&#125;)df2 = pd.DataFrame(data2,                   index=pd.MultiIndex.from_product([days,[&#x27;morning&#x27;,&#x27;afternoon&#x27;]]))df2.resample(&#x27;D&#x27;, level=0).sum()
第四节 时区表示
index = pd.date_range(&#x27;8/1/2012 00:00&#x27;, periods=5, freq=&#x27;D&#x27;)ts = pd.Series(np.random.randn(len(index)), index)import pytzpytz.common_timezones # 常用时区# 时区表示ts = ts.tz_localize(tz=&#x27;UTC&#x27;)# 转换成其它时区ts.tz_convert(tz = &#x27;Asia/Shanghai&#x27;)
第十五部分 数据可视化
pip install matplotlib -i https://pypi.tuna.tsinghua.edu.cn/simple
import numpy as npimport pandas as pd# 1、线形图df1 = pd.DataFrame(data = np.random.randn(1000,4),                  index = pd.date_range(start = &#x27;27/6/2012&#x27;,periods=1000),                  columns=list(&#x27;ABCD&#x27;))df1.cumsum().plot()# 2、条形图df2 = pd.DataFrame(data = np.random.rand(10,4),                   columns = list(&#x27;ABCD&#x27;))df2.plot.bar(stacked = True) # stacked 是否堆叠# 3、饼图df3 = pd.DataFrame(data = np.random.rand(4,2),                   index = list(&#x27;ABCD&#x27;),                   columns=[&#x27;One&#x27;,&#x27;Two&#x27;])df3.plot.pie(subplots = True,figsize = (8,8))# 4、散点图df4 = pd.DataFrame(np.random.rand(50, 4), columns=list(&#x27;ABCD&#x27;))df4.plot.scatter(x=&#x27;A&#x27;, y=&#x27;B&#x27;) # A和B关系绘制# 在一张图中绘制AC散点图，同时绘制BD散点图ax = df4.plot.scatter(x=&#x27;A&#x27;, y=&#x27;C&#x27;, color=&#x27;DarkBlue&#x27;, label=&#x27;Group 1&#x27;);df4.plot.scatter(x=&#x27;B&#x27;, y=&#x27;D&#x27;, color=&#x27;DarkGreen&#x27;, label=&#x27;Group 2&#x27;, ax=ax)# 气泡图，散点有大小之分df4.plot.scatter(x=&#x27;A&#x27;,y=&#x27;B&#x27;,s = df4[&#x27;C&#x27;]*200)# 5、面积图df5 = pd.DataFrame(data = np.random.rand(10, 4),                    columns=list(&#x27;ABCD&#x27;))df5.plot.area(stacked = True);# stacked 是否堆叠# 6、箱式图df6 = pd.DataFrame(data = np.random.rand(10, 5),                    columns=list(&#x27;ABCDE&#x27;))df6.plot.box()# 7、直方图df7 = pd.DataFrame(&#123;&#x27;A&#x27;: np.random.randn(1000) + 1, &#x27;B&#x27;: np.random.randn(1000),                    &#x27;C&#x27;: np.random.randn(1000) - 1&#125;)df7.plot.hist(alpha=0.5) #带透明度直方图df7.plot.hist(stacked = True)# 堆叠图df7.hist(figsize = (8,8)) # 子视图绘制
第十六部分 实战-数据分析师招聘数据分析
第一节 分析目标

各城市对数据分析岗位的需求情况
不同细分领域对数据分析岗的需求情况
数据分析岗位的薪资状况
工作经验与薪水的关系
公司都要求什么掌握什么技能
岗位的学历要求高吗
不同规模的企业对工资经验的要求以及提供的薪资水平

第二节 数据加载
import pandas as pdimport numpy as npjob = pd.read_csv(&#x27;./job.csv&#x27;)job.drop_duplicates(inplace = True) # 删除重复数据
第三节 数据清洗


过滤非数据分析的岗位
# 数据分析相应的岗位数量cond = job[&quot;positionName&quot;].str.contains(&quot;数据分析&quot;)  # 职位名中含有数据分析字眼的# 筛选出我们想要的字段，并剔除positionNamejob = job[cond]job.reset_index(inplace=True) # 行索引 重置job


数据中的薪水是一个区间，这里用薪水区间的均值作为相应职位的薪水
# 处理过程#1、将salary中的字符串均小写化（因为存在8k-16k和8K-16K）#2、运用正则表达式提取出薪资区间#3、将提取出来的数字转化为int型#4、取区间的平均值job[&quot;salary&quot;] = job[&quot;salary&quot;].str.lower()\               .str.extract(r&#x27;(\d+)[k]-(\d+)k&#x27;)\               .applymap(lambda x:int(x))\               .mean(axis=1)


从job_detail中提取出技能要求 将技能分为以下几类 Python SQL Tableau Excel SPSS/SAS 处理方式： 如果job_detail中含有上述五类，则赋值为1，不含有则为0


job[&quot;job_detail&quot;] = job[&quot;job_detail&quot;].str.lower().fillna(&quot;&quot;)  #将字符串小写化，并将缺失值赋值为空字符串
job[&quot;Python&quot;] = job[&quot;job_detail&quot;].map(lambda x:1 if ('python' in x) else 0)
job[&quot;SQL&quot;] = job[&quot;job_detail&quot;].map(lambda x:1 if ('sql' in x) or ('hive' in x)  else 0)
job[&quot;Tableau&quot;] = job[&quot;job_detail&quot;].map(lambda x:1 if 'tableau' in x  else 0)
job[&quot;Excel&quot;] = job[&quot;job_detail&quot;].map(lambda x:1 if 'excel' in x  else 0)
job['SPSS/SAS'] = job['job_detail'].map(lambda x:1 if ('spss' in x) or ('sas' in x) else 0)
    *   处理行业信息        在行业信息中有多个标签，对其进行处理，筛选最显著的行业标签。        ```python    def clean_industry(industry):        industry = industry.split(&quot;,&quot;)        if industry[0]==&quot;移动互联网&quot; and len(industry)&gt;1:            return industry[1]        else:            return industry[0]    job[&quot;industryField&quot;] = job.industryField.map(clean_industry)




数据分析师职位的数据预处理基本完成，后续使用matplotlib进行数据可视化分析。


pandas库的亮点

一个快速、高效的DataFrame对象，用于数据操作和综合索引；
用于在内存数据结构和不同格式之间读写数据的工具：CSV和文本文件、Microsoft Excel、SQL数据库和快速HDF 5格式；
智能数据对齐和丢失数据的综合处理：在计算中获得基于标签的自动对齐，并轻松地将凌乱的数据操作为有序的形式；
数据集的灵活调整和旋转；
基于智能标签的切片、花式索引和大型数据集的子集；
可以从数据结构中插入和删除列，以实现大小可变；
通过在强大的引擎中聚合或转换数据，允许对数据集进行拆分应用组合操作;
数据集的高性能合并和连接；
层次轴索引提供了在低维数据结构中处理高维数据的直观方法；
时间序列-功能：日期范围生成和频率转换、移动窗口统计、移动窗口线性回归、日期转换和滞后。甚至在不丢失数据的情况下创建特定领域的时间偏移和加入时间序列；
对性能进行了高度优化，用Cython或C编写了关键代码路径。
Python与pandas在广泛的学术和商业领域中使用，包括金融，神经科学，经济学，统计学，广告，网络分析，等等
学到这里，体会一会pandas库的亮点，如果对哪些还不熟悉，请对之前知识点再次进行复习。

]]></content>
      <categories>
        <category>program</category>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Dijkstra</title>
    <url>/posts/9c98f56c.html</url>
    <content><![CDATA[Dijkstra 最小花费
题目描述
在 n 个人中，某些人的银行账号之间可以互相转账。这些人之间转账的手续费各不相同。给定这些人之间转账时需要从转账金额里扣除百分之几的手续费，请问 A 最少需要多少钱使得转账后 B 收到 100 元。
输入格式
第一行输入两个正整数 n,m，分别表示总人数和可以互相转账的人的对数。以下 m 行每行输入三个正整数 x,y,z，表示标号为 x 的人和标号为 y 的人之间互相转账需要扣除 z% 的手续费 ( z&lt;100 )。最后一行输入两个正整数 A,B。数据保证 A 与 B 之间可以直接或间接地转账。**
输出格式
输出 A 使得 B 到账 100 元最少需要的总费用。 精确到小数点后 8 位。
数据范围
1≤n≤2000, m≤105
import java.util.*;public class Main&#123;    static int N=2010;    static int M=100010;    static int INF=0x3f3f3f3f;    static int n,m,idx;    static int A,B;    static int []h=new int[N],e=new int[M*2];    static int []ne=new int[M*2];    static double []w=new double[M*2];    static double []dist=new double[N];    static boolean[]st=new boolean[N];    public static void add(int a,int b,double c)&#123;        e[idx]=b;        w[idx]=c;        ne[idx]=h[a];        h[a]=idx++;    &#125;    public static double dj()&#123;        Arrays.fill(dist,INF);        dist[A]=100;        PriorityQueue&lt;PII&gt;q=new PriorityQueue&lt;&gt;();        q.offer(new PII(A,100));        while(q.size()!=0)&#123;            PII t=q.poll();            int num=t.num;            double distence=t.distence;            if(st[num])continue;            st[num]=true;            for(int i=h[num];i!=-1;i=ne[i])&#123;                int j=e[i];                if(dist[j]&gt;distence/w[i])&#123;                    dist[j]=distence/w[i];                    q.offer(new PII(j,dist[j]));                &#125;            &#125;        &#125;        return dist[B];    &#125;    public static void main(String[]args)&#123;        Scanner scan=new Scanner(System.in);        n=scan.nextInt();        m=scan.nextInt();        Arrays.fill(h,-1);        while(m--&gt;0)&#123;            int a=scan.nextInt();            int b=scan.nextInt();            int c=scan.nextInt();            double w=(1-(double)c/100);            add(a,b,w);            add(b,a,w);        &#125;        A=scan.nextInt();        B=scan.nextInt();        double t=dj();        System.out.printf(&quot;%.8f&quot;,t);    &#125;&#125;class PII implements Comparable&lt;PII&gt;&#123;    int num;    double distence;    public PII(int num,double distence)&#123;        this.num=num;        this.distence=distence;    &#125;    public int compareTo(PII o)&#123;        return Double.compare(distence,o.distence);    &#125;&#125;]]></content>
      <categories>
        <category>program</category>
        <category>算法模板</category>
      </categories>
      <tags>
        <tag>最短路</tag>
      </tags>
  </entry>
  <entry>
    <title>多元线性回归</title>
    <url>/posts/7d0d429b.html</url>
    <content><![CDATA[[toc]
多元线性回归
测试：$$ evidence_{i}=\sum_{j}W_{ij}x_{j}+b_{i} $$
1、基本概念
线性回归是机器学习中有监督机器学习下的一种算法。 回归问题主要关注的是因变量(需要预测的值，可以是一个也可以是多个)和一个或多个数值型的自变量(预测变量)之间的关系。
需要预测的值:即目标变量，target，y，连续值预测变量。
影响目标变量的因素：X_1X\_1X_1…X_nX\_nX_n，可以是连续值也可以是离散值。
因变量和自变量之间的关系:即模型，model，是我们要求解的。
1.1、连续值

1.2、离散值

1.3、简单线性回归
前面提到过，算法说白了就是公式，简单线性回归属于一个算法，它所对应的公式。
y=wx+by = wx + by=wx+b
这个公式中，y 是目标变量即未来要预测的值，x 是影响 y 的因素，w,b 是公式上的参数即要求的模型。其实 b 就是咱们的截距，w 就是斜率嘛！ 所以很明显如果模型求出来了，未来影响 y 值的未知数就是一个 x 值，也可以说影响 y 值 的因素只有一个，所以这是就叫简单线性回归的原因。
同时可以发现从 x 到 y 的计算，x 只是一次方，所以这是算法叫线性回归的原因。 其实，大家上小学时就已经会解这种一元一次方程了。为什么那个时候不叫人工智能算法呢？因为人工智能算法要求的是最优解！
1.4、最优解
Actual value:真实值，一般使用 y 表示。
Predicted value:预测值，是把已知的 x 带入到公式里面和猜出来的参数 w,b 计算得到的，一般使用 haty\\hat{y}haty 表示。
Error:误差，预测值和真实值的差距，一般使用 varepsilon\\varepsilonvarepsilon 表示。
最优解:尽可能的找到一个模型使得整体的误差最小，整体的误差通常叫做损失 Loss。
Loss:整体的误差，Loss 通过损失函数 Loss function 计算得到。

1.5、多元线性回归
现实生活中，往往影响结果 y 的因素不止一个，这时 x 就从一个变成了 n 个，X_1X\_1X_1…X_nX\_nX_n 同时简单线性回归的公式也就不在适用了。多元线性回归公式如下：
$ \hat{y} = w_1X_1 + w_2X_2 + …… + w_nX_n + b $
b是截距，也可以使用w_0w\_0w_0来表示
haty=w_1X_1+w_2X_2+……+w_nX_n+w_0\\hat{y} = w\_1X\_1 + w\_2X\_2 + …… + w\_nX\_n + w\_0haty=w_1X_1+w_2X_2+……+w_nX_n+w_0
\\hat{y} = w\_1X\_1 + w\_2X\_2 + …… + w\_nX\_n + w\_0 \* 1
使用向量来表示，X表示所有的变量，是一维向量；W表示所有的系数（包含w_0w\_0w_0），是一维向量，根据向量乘法规律，可以这么写：
haty=WTX\\hat{y} = W^TXhaty=WTX

2、正规方程
2.1、最小二乘法矩阵表示
最小二乘法可以将误差方程转化为有确定解的代数方程组（其方程式数目正好等于未知数的个数），从而可求解出这些未知参数。这个有确定解的代数方程组称为最小二乘法估计的正规方程。公式如下：
theta=(XTX)−1XTy\\theta = (X^TX)^{-1}X^Tytheta=(XTX)−1XTy 或者 W=(XTX)−1XTyW = (X^TX)^{-1}X^TyW=(XTX)−1XTy ，其中的W、thetaW、\\thetaW、theta 即使方程的解！

公式是如何推导的？
最小二乘法公式如下：
J(theta)=frac12sumlimitsi=0n(htheta(x_i)−y_i)2J(\\theta) = \\frac{1}{2}\\sum\\limits_{i = 0}^n(h_{\\theta}(x\_i) - y\_i)^2J(theta)=frac12sumlimitsi=0n​(htheta​(x_i)−y_i)2
使用矩阵表示：
J(theta)=frac12sumlimitsi=0n(htheta(xi)−y)(htheta(x_i)−y)J(\\theta) = \\frac{1}{2}\\sum\\limits_{i = 0}^n(h_{\\theta(x_i)} - y)(h_{\\theta(x\_i)} - y)J(theta)=frac12sumlimitsi=0n​(htheta(xi​)​−y)(htheta(x_i)​−y)
J(theta)=frac12(Xtheta−y)T(Xtheta−y)J(\\theta) = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y)J(theta)=frac12(Xtheta−y)T(Xtheta−y)
之所以要使用转置T，是因为，矩阵运算规律是：矩阵A的一行乘以矩阵B的一列！
2.2、多元一次方程举例
1、二元一次方程
begincasesx+y=14 2x−y=10endcases\\begin{cases} x + y=14\\  2x - y = 10\\ \\end{cases}begincasesx+y=14 2x−y=10endcases
2、三元一次方程
begincasesx−y+z=1002x+y−z=803x−2y+6z=256endcases\\begin{cases} x - y + z = 100\\ 2x + y -z = 80\\ 3x - 2y + 6z = 256\\ \\end{cases}begincasesx−y+z=1002x+y−z=803x−2y+6z=256endcases
3、八元一次方程
\\left{\\begin{align}&14x\_2 + 8x\_3 + 5x\_5 + -2x\_6 + 9x\_7 + -3x\_8 = 339\\&-4x\_1 + 10x\_2 + 6x\_3 + 4x\_4 + -14x\_5 + -2x\_6 + -14x\_7 + 8x\_8 = -114\\&-1x\_1 + -6x\_2 + 5x\_3 + -12x\_4 + 3x\_5 + -3x\_6 + 2x\_7 + -2x\_8 = 30\\&5x\_1 + -2x\_2 + 3x\_3 + 10x\_4 + 5x\_5 + 11x\_6 + 4x\_7 + -8x\_8 = 126\\&-15x\_1 + -15x\_2 + -8x\_3 + -15x\_4 + 7x\_5 + -4x\_6 + -12x\_7 + 2x\_8 = -395\\&11x\_1 + -10x\_2 + -2x\_3 + 4x\_4 + 3x\_5 + -9x\_6 + -6x\_7 + 7x\_8 = -87\\&-14x\_1 + 4x\_3 + -3x\_4 + 5x\_5 + 10x\_6 + 13x\_7 + 7x\_8 = 422\\&-3x\_1 + -7x\_2 + -2x\_3 + -8x\_4 + -6x\_6 + -5x\_7 + -9x\_8 = -309\\end{align}\\right.
# 上面八元一次方程对应的X数据[[  0  14   8   0   5  -2   9  -3] [ -4  10   6   4 -14  -2 -14   8] [ -1  -6   5 -12   3  -3   2  -2] [  5  -2   3  10   5  11   4  -8] [-15 -15  -8 -15   7  -4 -12   2] [ 11 -10  -2   4   3  -9  -6   7] [-14   0   4  -3   5  10  13   7] [ -3  -7  -2  -8   0  -6  -5  -9]]# 对应的y[ 339 -114   30  126 -395  -87  422 -309]
2.3、矩阵转置公式与求导公式：
转置公式如下：


(mA)T=mAT(mA)^T = mA^T(mA)T=mAT，其中m是常数


(A+B)T=AT+BT(A + B)^T = A^T + B^T(A+B)T=AT+BT


(AB)T=BTAT(AB)^T = B^TA^T(AB)T=BTAT


(AT)T=A(A^T)^T = A(AT)T=A


求导公式如下：


\\frac{\\partial X^T}{\\partial X} = I$$ 求解出来是单位矩阵


fracpartialAXTpartialX=A\\frac{\\partial AX^T}{\\partial X} = AfracpartialAXTpartialX=A

fracpartialAXpartialX=AT\\frac{\\partial AX}{\\partial X} = A^T
fracpartialAXpartialX=AT


fracpartialXApartialX=AT\\frac{\\partial XA}{\\partial X} = A^T
fracpartialXApartialX=AT

fracpartialXTAXpartialX=(A+AT)X;\\frac{\\partial X^TAX}{\\partial X} = (A + A^T)X;fracpartialXTAXpartialX=(A+AT)X; A不是对称矩阵
fracpartialXTAXpartialX=2AX;\\frac{\\partial X^TAX}{\\partial X} = 2AX;fracpartialXTAXpartialX=2AX; A是对称矩阵

2.4、推导正规方程 theta\\thetatheta 的解：

矩阵乘法公式展开



J(theta)=frac12(Xtheta−y)T(Xtheta−y)J(\\theta) = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y)J(theta)=frac12(Xtheta−y)T(Xtheta−y)


J(theta)=frac12(thetaTXT−yT)(Xtheta−y)J(\\theta) = \\frac{1}{2}(\\theta^TX^T - y^T)(X\\theta - y)J(theta)=frac12(thetaTXT−yT)(Xtheta−y)


J(theta)=frac12(thetaTXTXtheta−thetaTXTy−yTXtheta+yTy)J(\\theta) = \\frac{1}{2}(\\theta^TX^TX\\theta - \\theta^TX^Ty -y^TX\\theta + y^Ty)J(theta)=frac12(thetaTXTXtheta−thetaTXTy−yTXtheta+yTy)



进行求导（注意X、y是已知量，theta\\thetatheta 是未知数）：


J′(theta)=frac12(thetaTXTXtheta−thetaTXTy−yTXtheta+yTy)′J&#x27;(\\theta) = \\frac{1}{2}(\\theta^TX^TX\\theta - \\theta^TX^Ty -y^TX\\theta + y^Ty)&#x27;J′(theta)=frac12(thetaTXTXtheta−thetaTXTy−yTXtheta+yTy)′


根据上面求导公式进行运算：


J′(theta)=frac12(XTXtheta+(thetaTXTX)T−XTy−(yTX)T)J&#x27;(\\theta) = \\frac{1}{2}(X^TX\\theta + (\\theta^TX^TX)^T-X^Ty - (y^TX)^T)J′(theta)=frac12(XTXtheta+(thetaTXTX)T−XTy−(yTX)T)
J′(theta)=frac12(XTXtheta+XTXtheta−XTy−XTy)J&#x27;(\\theta) = \\frac{1}{2}(X^TX\\theta + X^TX\\theta -X^Ty - X^Ty)J′(theta)=frac12(XTXtheta+XTXtheta−XTy−XTy)
J′(theta)=frac12(2XTXtheta−2XTy)J&#x27;(\\theta) = \\frac{1}{2}(2X^TX\\theta -2X^Ty)J′(theta)=frac12(2XTXtheta−2XTy)
J′(theta)=XTXtheta−XTyJ&#x27;(\\theta) =X^TX\\theta -X^TyJ′(theta)=XTXtheta−XTy
J′(theta)=XT(Xtheta−y)J&#x27;(\\theta) =X^T(X\\theta -y)J′(theta)=XT(Xtheta−y) 矩阵运算分配律


令导数J′(theta)=0：J&#x27;(\\theta) = 0：J′(theta)=0：



0=XTXtheta−XTy0 =X^TX\\theta -X^Ty0=XTXtheta−XTy


XTXtheta=XTyX^TX\\theta = X^TyXTXtheta=XTy



矩阵没有除法，使用逆矩阵进行转化：


(XTX)−1XTXtheta=(XTX)−1XTy(X^TX)^{-1}X^TX\\theta = (X^TX)^{-1}X^Ty(XTX)−1XTXtheta=(XTX)−1XTy
Itheta=(XTX)−1XTyI\\theta = (X^TX)^{-1}X^TyItheta=(XTX)−1XTy
theta=(XTX)−1XTy\\theta = (X^TX)^{-1}X^Tytheta=(XTX)−1XTy

到此为止，公式推导出来了~

2.5、凸函数判定
判定损失函数是凸函数的好处在于我们可能很肯定的知道我们求得的极值即最优解，一定是全局最优解。

如果是非凸函数，那就不一定可以获取全局最优解~

来一个更加立体的效果图：

判定凸函数的方式: 判定凸函数的方式非常多，其中一个方法是看黑塞矩阵是否是半正定的。
黑塞矩阵(hessian matrix)是由目标函数在点 X 处的二阶偏导数组成的对称矩阵。
对于我们的式子来说就是在导函数的基础上再次对θ来求偏导，结果就是 XTXX^TXXTX。所谓正定就是 XTXX^TXXTX 的特征值全为正数，半正定就是 XTXX^TXXTX 的特征值大于等于 0， 就是半正定。
J′(theta)=XTXtheta−XTyJ&#x27;(\\theta) =X^TX\\theta -X^TyJ′(theta)=XTXtheta−XTy
J′′(theta)=XTXJ&#x27;&#x27;(\\theta) =X^TXJ′′(theta)=XTX
这里我们对 J(theta)J(\\theta)J(theta) 损失函数求二阶导数的黑塞矩阵是 XTXX^TXXTX ，得到的一定是半正定的，自己和自己做点乘嘛！
这里不用数学推导证明这一点。在机器学习中往往损失函数都是凸函数，到深度学习中损失函数往往是非凸函数，即找到的解未必是全局最优，只要模型堪用就好！机器学习特点是：不强调模型 100% 正确，只要是有价值的，堪用的，就Okay！

3、线性回归算法推导
3.1、深入理解回归
回归简单来说就是“回归平均值”(regression to the mean)。但是这里的 mean 并不是把 历史数据直接当成未来的预测值，而是会把期望值当作预测值。 追根溯源回归这个词是一个叫高尔顿的人发明的，他通过大量观察数据发现:父亲比较高，儿子也比较高；父亲比较矮，那么儿子也比较矮！正所谓“龙生龙凤生凤老鼠的儿子会打洞”！但是会存在一定偏差~
父亲是 1.98，儿子肯定很高，但有可能不会达到1.98   父亲是 1.69，儿子肯定不高，但是有可能比 1.69 高
大自然让我们回归到一定的区间之内，这就是大自然神奇的力量。
高尔顿是谁?达尔文的表弟，这下可以相信他说的十有八九是对的了吧！
人类社会很多事情都被大自然这种神奇的力量只配置：身高、体重、智商、相貌……
这种神秘的力量就叫正态分布。大数学家高斯，深入研究了正态分布，最终推导出了线性回归的原理：最小二乘法！

接下来，我们跟着高斯的足迹继续向下走~
3.2、误差分析
误差 varepsilon_i\\varepsilon\_ivarepsilon_i 等于第 i 个样本实际的值 y_iy\_iy_i 减去预测的值 haty\\hat{y}haty ，公式可以表达为如下：
varepsilon_i=y_i−haty\\varepsilon\_i = y\_i - \\hat{y}varepsilon_i=y_i−haty
varepsilon_i=y_i−WTx_i\\varepsilon\_i = y\_i - W^Tx\_ivarepsilon_i=y_i−WTx_i
假定所有的样本的误差都是独立的，有上下的震荡，震荡认为是随机变量，足够多的随机变量叠加之后形成的分布，它服从的就是正态分布，因为它是正常状态下的分布，也就是高斯分布！均值是某一个值，方差是某一个值。 方差我们先不管，均值我们总有办法让它去等于零 0 的，因为我们这里是有截距b， 所有误差我们就可以认为是独立分布的，1&lt;=i&lt;=n，服从均值为 0，方差为某定值的高斯分布。机器学习中我们假设误差符合均值为0，方差为定值的正态分布！！！

3.3、最大似然估计
最大似然估计(maximum likelihood estimation, MLE)一种重要而普遍的求估计量的方法。最大似然估计明确地使用概率模型，其目标是寻找能够以较高概率产生观察数据的系统发生树。最大似然估计是一类完全基于统计的系统发生树重建方法的代表。
是不是，有点看不懂，太学术了，我们举例说明~
假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？

请告诉我答案！

很多小伙伴，甚至不用算，凭感觉，就能给出答案：70%！
下面是详细推导过程：


最大似然估计，计算


白球概率是p，黑球是1-p（罐子中非黑即白）


罐子中取一个请问是白球的概率是多少？


pp
p




罐子中取两个球，两个球都是白色，概率是多少？


p2p^2
p2




罐子中取5个球都是白色，概率是多少？


p5p^5
p5




罐子中取10个球，9个是白色，一个是黑色，概率是多少呢？


C101=C101C_{10}^1 = C_{10}^1C101​=C101​ 这个两个排列组合公式是相等的~

C109p9(1−p)=C101p9(1−p)C_{10}^9p^9(1-p) = C_{10}^1p^9(1-p)
C109​p9(1−p)=C101​p9(1−p)




罐子取100个球，70次是白球，30次是黑球，概率是多少？


P=C_10030p70(1−p)30P = C\_{100}^{30}p^{70}(1-p)^{30}
P=C_10030p70(1−p)30




最大似然估计，什么时候P最大呢？
C_10030C\_{100}^{30}C_10030是常量，可以去掉！
p &gt; 0，1- p &gt; 0，所以上面概率想要求最大值，那么求导数即可！


P' = 70_p^{69}_(1-p)^{30} + p^{70}_30_(1-p)^{29}\*(-1)

令导数为0：


0 = 70_p^{69}_(1-p)^{30} +p^{70}_30_(1-p)^{29}\*(-1)

公式化简：


0=70(1−p)−p300 = 70_(1-p) - p_30
0=70(​1−p)−p3​0


0 = 70 - 100\*p



p = 70%


3.4、高斯分布-概率密度函数
最常见的连续概率分布是正态分布，也叫高斯分布，而这正是我们所需要的，其概率密度函数如下:

公式如下：
f(xmu,sigma2)=frac1sqrt2pisigmae−frac(x−mu)22sigma2f(x\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}f(xmu,sigma2)=frac1sqrt2pisigmae−frac(x−mu)22sigma2
随着参数μ和σ变化，概率分布也产生变化。 下面重要的步骤来了，我们要把一组数据误差出现的总似然，也就是一组数据之所以对应误差出现的整体可能性表达出来了，因为数据的误差我们假设服从一个高斯分布，并且通过截距项来平移整体分布的位置从而使得μ=0，所以样本的误差我们可以表达其概率密度函数的值如下:
f(varepsilonmu=0,sigma2)=frac1sqrt2pisigmae−frac(varepsilon−0)22sigma2f(\\varepsilon\\mu = 0,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(\\varepsilon - 0)^2}{2\\sigma^2}}f(varepsilonmu=0,sigma2)=frac1sqrt2pisigmae−frac(varepsilon−0)22sigma2
简化如下：
f(varepsilon0,sigma2)=frac1sqrt2pisigmae−fracvarepsilon22sigma2f(\\varepsilon 0,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{\\varepsilon ^2}{2\\sigma^2}}f(varepsilon0,sigma2)=frac1sqrt2pisigmae−fracvarepsilon22sigma2
3.5、误差总似然
和前面黑球白球问题类似，也是一个累乘问题~
P=prodlimits_i=0nf(varepsiloni0,sigma2)=prodlimitsi=0nfrac1sqrt2pisigmae−fracvarepsilon_i22sigma2P = \\prod\\limits\_{i = 0}^{n}f(\\varepsilon_i0,\\sigma^2) = \\prod\\limits_{i = 0}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{\\varepsilon\_i ^2}{2\\sigma^2}}P=prodlimits_i=0nf(varepsiloni​0,sigma2)=prodlimitsi=0n​frac1sqrt2pisigmae−fracvarepsilon_i22sigma2
根据前面公式varepsilon_i=y_i−WTx_i\\varepsilon\_i = y\_i - W^Tx\_ivarepsilon_i=y_i−WTx_i可以推导出来如下公式：
P=prodlimits_i=0nf(varepsiloni0,sigma2)=prodlimitsi=0nfrac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2P = \\prod\\limits\_{i = 0}^{n}f(\\varepsilon_i0,\\sigma^2) = \\prod\\limits_{i = 0}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y\_i - W^Tx\_i)^2}{2\\sigma^2}}P=prodlimits_i=0nf(varepsiloni​0,sigma2)=prodlimitsi=0n​frac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2
公式中的未知变量就是WTW^TWT，即方程的系数，系数包含截距~如果，把上面当成一个方程，就是概率P关于W的方程！其余符号，都是常量！
PW=prodlimitsi=0nfrac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2P_W= \\prod\\limits_{i = 0}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y\_i - W^Tx\_i)^2}{2\\sigma^2}}PW​=prodlimitsi=0n​frac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2
现在问题，就变换成了，求最大似然问题了！不过，等等~
累乘的最大似然，求解是非常麻烦的！
接下来，我们通过，求对数把累乘问题，转变为累加问题（加法问题，无论多复杂，都难不倒我了！）

3.6、最小二乘法MSE
PW=prodlimitsi=0nfrac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2P_W = \\prod\\limits_{i = 0}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y\_i - W^Tx\_i)^2}{2\\sigma^2}}PW​=prodlimitsi=0n​frac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2
根据对数，单调性，对上面公式求自然底数e的对数，效果不变~
log_e(P_W)=loge(prodlimitsi=0nfrac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2)log\_e(P\_W) = log_e(\\prod\\limits_{i = 0}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y\_i - W^Tx\_i)^2}{2\\sigma^2}})log_e(P_W)=loge​(prodlimitsi=0n​frac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2)
接下来 log 函数继续为你带来惊喜，数学上连乘是个大麻烦，即使交给计算机去求解它也得哭出声来。惊喜是:

log_a(XY)=log_aX+log_aYlog\_a(XY) = log\_aX + log\_aYlog_a(XY)=log_aX+log_aY
log_afracXY=log_aX−log_aYlog\_a\\frac{X}{Y} = log\_aX - log\_aYlog_afracXY=log_aX−log_aY
log\_aX^n = n\*log\_aX
log_a(X_1X_2……X_n)=log_aX_1+log_aX_2+……+log_aX_nlog\_a(X\_1X\_2……X\_n) = log\_aX\_1 + log\_aX\_2 + …… + log\_aX\_nlog_a(X_1X_2……X_n)=log_aX_1+log_aX_2+……+log_aX_n
log_xxn=n(ninR)log\_xx^n = n(n\\in R)log_xxn=n(ninR)
log_afrac1X=−log_aXlog\_a\\frac{1}{X} = -log\_aXlog_afrac1X=−log_aX
log\_a\\sqrt\[x\]{N^y} = \\frac{y}{x}log\_aN

log_e(P_W)=loge(prodlimitsi=0nfrac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2)log\_e(P\_W) = log_e(\\prod\\limits_{i = 0}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y\_i - W^Tx\_i)^2}{2\\sigma^2}})log_e(P_W)=loge​(prodlimitsi=0n​frac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2)
=sumlimits_i=0nlog_e(frac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2)=\\sum\\limits\_{i = 0}^{n}log\_e(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y\_i - W^Tx\_i)^2}{2\\sigma^2}})=sumlimits_i=0nlog_e(frac1sqrt2pisigmae−frac(y_i−WTx_i)22sigma2)累乘问题变成累加问题~
乘风破浪，继续推导—&gt;
=sumlimits_i=0n(log_efrac1sqrt2pisigma−frac(y_i−WTx_i)22sigma2)=\\sum\\limits\_{i = 0}^{n}(log\_e\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{(y\_i - W^Tx\_i)^2}{2\\sigma^2})=sumlimits_i=0n(log_efrac1sqrt2pisigma−frac(y_i−WTx_i)22sigma2)
=sumlimits_i=0n(log_efrac1sqrt2pisigma−frac1sigma2cdotfrac12(y_i−WTx_i)2)=\\sum\\limits\_{i = 0}^{n}(log\_e\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{\\sigma^2}\\cdot\\frac{1}{2}(y\_i - W^Tx\_i)^2)=sumlimits_i=0n(log_efrac1sqrt2pisigma−frac1sigma2cdotfrac12(y_i−WTx_i)2)
上面公式是最大似然求对数后的变形，其中pi、sigma\\pi、\\sigmapi、sigma都是常量，而(y_i−WTx_i)2(y\_i - W^Tx\_i)^2(y_i−WTx_i)2肯定大于零！上面求最大值问题，即可转变为如下求最小值问题：
L(W)=frac12sumlimits_i=0n(y(i)−WTx(i))2L(W) = \\frac{1}{2}\\sum\\limits\_{i = 0}^n(y^{(i)} - W^Tx^{(i)})^2L(W)=frac12sumlimits_i=0n(y(i)−WTx(i))2 L代表Loss，表示损失函数，损失函数越小，那么上面最大似然就越大~
有的书本上公式，也可以这样写，用J(theta)J(\\theta)J(theta)表示一个意思，theta\\thetatheta 的角色就是W：
J(theta)=frac12sumlimitsi=1n(y(i)−thetaTx(i))2=frac12sumlimitsi=1n(thetaTx(i)−y(i))2J(\\theta) = \\frac{1}{2}\\sum\\limits_{i = 1}^n(y^{(i)} - \\theta^Tx^{(i)})^2 = \\frac{1}{2}\\sum\\limits_{i = 1}^n(\\theta^Tx^{(i)} - y^{(i)})^2J(theta)=frac12sumlimitsi=1n​(y(i)−thetaTx(i))2=frac12sumlimitsi=1n​(thetaTx(i)−y(i))2
进一步提取：
J(theta)=frac12sumlimitsi=1n(htheta(x(i))−y(i))2J(\\theta) = \\frac{1}{2}\\sum\\limits_{i = 1}^n(h_{\\theta}(x^{(i)}) - y^{(i)})^2J(theta)=frac12sumlimitsi=1n​(htheta​(x(i))−y(i))2
其中：
haty=h_theta(X)=Xtheta\\hat{y} = h\_{\\theta}(X) =X \\thetahaty=h_theta(X)=Xtheta 表示全部数据，是矩阵，X表示多个数据，进行矩阵乘法时，放在前面
hatyi=htheta(x(i))=thetaTx(i)\\hat{y}_i = h_{\\theta}(x^{(i)}) = \\theta^Tx^{(i)}hatyi​=htheta​(x(i))=thetaTx(i) 表示第i个数据，是向量，所以进行乘法时，其中一方需要转置
因为最大似然公式中有个负号，所以最大总似然变成了最小化负号后面的部分。 到这里，我们就已经推导出来了 MSE 损失函数J(theta)J(\\theta)J(theta)，从公式我们也可以看出来 MSE 名字的来 历，mean squared error，上式也叫做最小二乘法！
3.7、归纳总结升华
这种最小二乘法估计，其实我们就可以认为，假定了误差服从正太分布，认为样本误差的出现是随机的，独立的，使用最大似然估计思想，利用损失函数最小化 MSE 就能求出最优解！所以反过来说，如果我们的数据误差不是互相独立的，或者不是随机出现的，那么就不适合去假设为正太分布，就不能去用正太分布的概率密度函数带入到总似然的函数中，故而就不能用 MSE 作为损失函数去求解最优解了！所以，最小二乘法不是万能的~
还有譬如假设误差服从泊松分布，或其他分布那就得用其他分布的概率密度函数去推导出损失函数了。
所以有时我们也可以把线性回归看成是广义线性回归。比如，逻辑回归，泊松回归都属于广义线性回归的一种，这里我们线性回归可以说是最小二乘线性回归。
4、线性回归实战
4.1、使用正规方程进行求解
4.1.1、简单线性回归
y=wx+by = wx + by=wx+b
一元一次方程，在机器学习中一元表示一个特征，b表示截距，y表示目标值。
import numpy as npimport matplotlib.pyplot as plt# 转化成矩阵X = np.linspace(0,10,num = 30).reshape(-1,1)# 斜率和截距，随机生成w = np.random.randint(1,5,size = 1)b = np.random.randint(1,10,size = 1)# 根据一元一次方程计算目标值y，并加上“噪声”，数据有上下波动~y = X * w + b + np.random.randn(30,1)plt.scatter(X,y)# 重新构造X，b截距，相当于系数w0，前面统一乘以1X = np.concatenate([X,np.full(shape = (30,1),fill_value= 1)],axis = 1)# 正规方程求解θ = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y).round(2)print(&#x27;一元一次方程真实的斜率和截距是：&#x27;,w, b)print(&#x27;通过正规方程求解的斜率和截距是：&#x27;,θ)# 根据求解的斜率和截距绘制线性回归线型图plt.plot(X[:,0],X.dot(θ),color = &#x27;green&#x27;)
效果如下（random.randn是随机生成正太分布数据，所以每次执行图形会有所不同）：

4.1.2、多元线性回归
y=w_1x_1+w_2x_2+by = w\_1x\_1 + w\_2x\_2 + by=w_1x_1+w_2x_2+b
二元一次方程，x_1、x_2x\_1、x\_2x_1、x_2 相当于两个特征，b是方程截距
import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d.axes3d import Axes3D # 绘制三维图像# 转化成矩阵x1 = np.random.randint(-150,150,size = (300,1))x2 = np.random.randint(0,300,size = (300,1))# 斜率和截距，随机生成w = np.random.randint(1,5,size = 2)b = np.random.randint(1,10,size = 1)# 根据二元一次方程计算目标值y，并加上“噪声”，数据有上下波动~y = x1 * w[0] + x2 * w[1] + b + np.random.randn(300,1)fig = plt.figure(figsize=(9,6))ax = Axes3D(fig)ax.scatter(x1,x2,y) # 三维散点图ax.view_init(elev=10, azim=-20) # 调整视角# 重新构造X，将x1、x2以及截距b，相当于系数w0，前面统一乘以1进行数据合并X = np.concatenate([x1,x2,np.full(shape = (300,1),fill_value=1)],axis = 1)w = np.concatenate([w,b])# 正规方程求解θ = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y).round(2)print(&#x27;二元一次方程真实的斜率和截距是：&#x27;,w)print(&#x27;通过正规方程求解的斜率和截距是：&#x27;,θ.reshape(-1))# # 根据求解的斜率和截距绘制线性回归线型图x = np.linspace(-150,150,100)y = np.linspace(0,300,100)z = x * θ[0] + y * θ[1] + θ[2]ax.plot(x,y,z ,color = &#x27;red&#x27;)
效果如下：

4.2、机器学习库scikit-learn
4.2.1、scikit-learn简介

4.2.2、scikit-learn实现简单线性回归
from sklearn.linear_model import LinearRegressionimport numpy as npimport matplotlib.pyplot as plt# 转化成矩阵X = np.linspace(0,10,num = 30).reshape(-1,1)# 斜率和截距，随机生成w = np.random.randint(1,5,size = 1)b = np.random.randint(1,10,size = 1)# 根据一元一次方程计算目标值y，并加上“噪声”，数据有上下波动~y = X * w + b + np.random.randn(30,1)plt.scatter(X,y)# 使用scikit-learn中的线性回归求解model = LinearRegression()model.fit(X,y)w_ = model.coef_b_ = model.intercept_print(&#x27;一元一次方程真实的斜率和截距是：&#x27;,w, b)print(&#x27;通过scikit-learn求解的斜率和截距是：&#x27;,w_,b_)plt.plot(X,X.dot(w_) + b_,color = &#x27;green&#x27;)

4.2.3、scikit-learn实现多元线性回归
import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d.axes3d import Axes3D# 转化成矩阵x1 = np.random.randint(-150,150,size = (300,1))x2 = np.random.randint(0,300,size = (300,1))# 斜率和截距，随机生成w = np.random.randint(1,5,size = 2)b = np.random.randint(1,10,size = 1)# 根据二元一次方程计算目标值y，并加上“噪声”，数据有上下波动~y = x1 * w[0] + x2 * w[1] + b + np.random.randn(300,1)fig = plt.figure(figsize=(9,6))ax = Axes3D(fig)ax.scatter(x1,x2,y) # 三维散点图ax.view_init(elev=10, azim=-20) # 调整视角# 重新构造X，将x1、x2以及截距b，相当于系数w0，前面统一乘以1进行数据合并X = np.concatenate([x1,x2],axis = 1)# 使用scikit-learn中的线性回归求解model = LinearRegression()model.fit(X,y)w_ = model.coef_.reshape(-1)b_ = model.intercept_print(&#x27;二元一次方程真实的斜率和截距是：&#x27;,w,b)print(&#x27;通过scikit-learn求解的斜率和截距是：&#x27;,w_,b_)# # 根据求解的斜率和截距绘制线性回归线型图x = np.linspace(-150,150,100)y = np.linspace(0,300,100)z = x * w_[0] + y * w_[1] + b_ax.plot(x,y,z ,color = &#x27;green&#x27;)

]]></content>
      <categories>
        <category>program</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>二分</title>
    <url>/posts/9947c71c.html</url>
    <content><![CDATA[最小值最大化
import java.util.*;public class Main&#123;    static int N=100010;    static int[]h=new int[N];    static int[]w=new int[N];    static int n,k;    public static void main(String[] args) &#123;        Scanner scan=new Scanner(System.in);        n=scan.nextInt();        k=scan.nextInt();        h=new int[n];        w=new int[n];        for(int i=0;i&lt;n;i++) &#123;            h[i]=scan.nextInt();            w[i]=scan.nextInt();        &#125;        int l=1,r=(int)1e5+10;        while(l&lt;r) &#123;            int mid=(r+l+1)/2;            if(check(mid))l=mid;            else r=mid-1;        &#125;        System.out.println(l);&#125;    static boolean check(int x) &#123;        long res=0;        for(int i=0;i&lt;n;i++) &#123;        res+=(h[i]/x)*(w[i]/x);        &#125;        if(res&gt;=k) &#123;            return true;        &#125;        return false;    &#125;&#125;
最大值最小化
题目：共n个月,给出每个月的开销.将n个月划分成m个时间段,求m个时间段中开销最大的时间段的最小开销值。
#include&lt;bits/stdc++.h&gt;using namespace std;int n,m;vector&lt;int&gt;a;int check(int M)&#123;    int ct=0,now=0;    for(int i=0;i&lt;n;i++)&#123;        if(a[i]&gt;M) return 0;        if(now+a[i]&gt;M)&#123;            ct++;            now=0;            &#125;            now+=a[i];    &#125;    return ct&lt;m;&#125;int main()&#123;    cin&gt;&gt;n&gt;&gt;m;    a.resize(n);    int R=0,L=0;    for(int i=0;i&lt;n;i++)&#123;        cin&gt;&gt;a[i];        R+=a[i];        L=max(L,a[i]);    &#125;    R++;        while(L&lt;R)&#123;        int M=(L+R)/2;        if(check(M)) R=M;        else L=M+1;    &#125;    cout&lt;&lt;L&lt;&lt;endl;    return 0;&#125;
//更安全的求最大值最小化import java.util.Arrays;import java.util.Scanner;public class Main &#123;    static  int N=100010,n,k,t;    static  long a[]=new long[N];    public  static  boolean check(int m)&#123;        long arr[]=new long[m+5];        long s[]=new long[m+5];        long s_pow[]=new long[m+5];        for(int i=1;i&lt;=m;i++) arr[i]=a[i];        Arrays.sort(arr,1,m+1);        for(int i=1;i&lt;=m;i++)&#123;            s[i]=s[i-1]+arr[i];            s_pow[i]=s_pow[i-1]+(arr[i]*arr[i]);        &#125;        for(int i=k;i&lt;=m;i++)&#123;            long s1=s_pow[i]-s_pow[i-k];            long s2=s[i]-s[i-k];            double avg=s2*1.00/k;//把他给的方差公式 可以优化 成这个 提醒平方差公式            double res=(s1-2*avg*s2+k*avg*avg)/k;            if(res&lt;t) return  true;        &#125;        return  false;    &#125;    public static void main(String[] args) &#123;        Scanner sc=new Scanner(System.in);        n=sc.nextInt();        k=sc.nextInt();        t=sc.nextInt();        for(int i=1;i&lt;=n;i++)&#123;            a[i]=sc.nextLong();        &#125;        int l=k,r=n;        int res=-1;        while (l&lt;=r)&#123;            int mid=(l+r)&gt;&gt;1;            if(check(mid))&#123;                r=mid-1;                res=mid;            &#125;else&#123;                l=mid+1;            &#125;        &#125;        System.out.println(res);    &#125;&#125;]]></content>
      <categories>
        <category>program</category>
        <category>算法模板</category>
      </categories>
      <tags>
        <tag>二分</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/posts/4a17b156.html</url>
    <content><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.
Quick Start
Create a new post
$ hexo new &quot;My New Post&quot;
More info: Writing
Run server
$ hexo server
More info: Server
Generate static files
$ hexo generate
More info: Generating
Deploy to remote sites
$ hexo deploy
More info: Deployment
]]></content>
  </entry>
  <entry>
    <title>Floyd</title>
    <url>/posts/97c8f9ac.html</url>
    <content><![CDATA[星际旅行
问题描述
小明国庆节准备去某星系进行星际旅行，这个星系里一共有 n_n_ 个星球，其中布置了 m_m_ 道双向传送门，第 i_i_ 道传送门可以连接 ai,bi*ai_,_bi 两颗星球（ai≠bi_a**i\=_b**i* 且任意两颗星球之间最多只有一个传送门）。
他看中了一款 “旅游盲盒”，一共有 Q_Q_ 个盲盒，第 i_i_ 个盲盒里的旅行方案规定了旅行的起始星球 xi*xi _和最多可以使用传送门的次数 yi_yi*。只要从起始星球出发，使用传送门不超过规定次数能到达的所有星球都可以去旅行。
小明关心在每个方案中有多少个星球可以旅行到。小明只能在这些盲盒里随机选一个购买，他想知道能旅行到的不同星球的数量的期望是多少。
输入格式
输入共 m+Q+1_m_+Q+1 行。
第一行为三个正整数 n,m,Q_n_,m,Q 。
后面 m_m_ 行，每行两个正整数 ai,bi*ai_,_bi* 。
后面 Q_Q_ 行，每行两个整数 xi,yi*xi_,_yi* 。
输出格式
输出共一行，一个浮点数（四舍五入保留两位小数）。
样例输入
3 2 31 22 32 12 01 1
样例输出
2.00
样例说明
第一个盲盒可以旅行到 1,2,31,2,3。
第二个盲盒可以旅行到 22。
第三个盲盒可以旅行到 1,21,2。
所以期望是 (3+1+2)/3=2.00(3+1+2)/3=2.00。
解决
import java.util.*;// 1:无需package// 2: 类名必须Main, 不可修改public class Main &#123;  static int n,m,q;  static int[][]con;    public static void main(String[] args) &#123;        Scanner sc = new Scanner(System.in);        n=sc.nextInt();        m=sc.nextInt();        q=sc.nextInt();        con=new int[n+1][n+1];        for(int i=1;i&lt;=n;i++)&#123;          Arrays.fill(con[i],3010);        &#125;        for(int i=1;i&lt;=n;i++)&#123;          con[i][i]=0;        &#125;        for(int i=0;i&lt;m;i++)&#123;          int a=sc.nextInt();          int b=sc.nextInt();          con[a][b]=1;          con[b][a]=1;        &#125;        for(int k=1;k&lt;=n;k++)&#123;          for(int i=1;i&lt;=n;i++)&#123;            for(int j=1;j&lt;=n;j++)&#123;              con[i][j]=Math.min(con[i][j],con[i][k]+con[k][j]);            &#125;          &#125;        &#125;        int ans=0;        for(int i=0;i&lt;q;i++)&#123;          int x=sc.nextInt();          int y=sc.nextInt();          int count=0;          for(int j=1;j&lt;=n;j++)&#123;            if(con[x][j]&lt;=y)count++;          &#125;            ans+=count;        &#125;          System.out.printf(&quot;%.2f&quot;,(double)ans/q);    &#125;&#125;
牛的旅行
题目链接
牛的旅行
问题描述
农民John的农场里有很多牧区，有的路径连接一些特定的牧区。
一片所有连通的牧区称为一个牧场。
但是就目前而言，你能看到至少有两个牧区不连通。
现在，John想在农场里添加一条路径（注意，恰好一条）。
一个牧场的直径就是牧场中最远的两个牧区的距离（本题中所提到的所有距离指的都是最短的距离）。
考虑如下的两个牧场，每一个牧区都有自己的坐标：

图 1 是有 5 个牧区的牧场，牧区用“*”表示，路径用直线表示。
图 1 所示的牧场的直径大约是 12.07106, 最远的两个牧区是 A 和 E，它们之间的最短路径是 A-B-E。
图 2 是另一个牧场。
这两个牧场都在John的农场上。
John将会在两个牧场中各选一个牧区，然后用一条路径连起来，使得连通后这个新的更大的牧场有最小的直径。
注意，如果两条路径中途相交，我们不认为它们是连通的。
只有两条路径在同一个牧区相交，我们才认为它们是连通的。
现在请你编程找出一条连接两个不同牧场的路径，使得连上这条路径后，所有牧场（生成的新牧场和原有牧场）中直径最大的牧场的直径尽可能小。
输出这个直径最小可能值。
输入格式
第 1 行：一个整数 N, 表示牧区数；
第 2 到 N+1 行：每行两个整数 X,Y， 表示 N 个牧区的坐标。每个牧区的坐标都是不一样的。
第 N+2 行到第 2*N+1 行：每行包括 N 个数字 ( 0或1 ) 表示一个对称邻接矩阵。
例如，题目描述中的两个牧场的矩阵描述如下：
  A B C D E F G H A 0 1 0 0 0 0 0 0 B 1 0 1 1 1 0 0 0 C 0 1 0 0 1 0 0 0 D 0 1 0 0 1 0 0 0 E 0 1 1 1 0 0 0 0 F 0 0 0 0 0 0 1 0 G 0 0 0 0 0 1 0 1 H 0 0 0 0 0 0 1 0
输入数据中至少包括两个不连通的牧区。
输出格式
只有一行，包括一个实数，表示所求答案。
数字保留六位小数。
数据范围
1≤N≤1501≤N≤150, 0≤X,Y≤1050≤X,Y≤105
输入样例：
810 1015 1020 1015 1520 1530 1525 1030 100100000010111000010010000100100001110000000000100000010100000010
输出样例：
22.071068
解决
import java.util.*;class PII&#123;    int x,y;    public PII(int x,int y)&#123;        this.x=x;        this.y=y;    &#125;&#125;public class Main&#123;    static int N=155,INF=(int)1e20;    static int n;    static PII[]q=new PII[N];    static char[][]g=new char[N][N];    static double[][]dist=new double[N][N];    static double[]maxd=new double[N];    public static double get_dist(PII i,PII j)&#123;        int dx=i.x-j.x,dy=i.y-j.y;        return (double)Math.sqrt(dx*dx+dy*dy);    &#125;    public static void floyd()&#123;        for(int k=1;k&lt;=n;k++)            for(int i=1;i&lt;=n;i++)                for(int j=1;j&lt;=n;j++)&#123;                    dist[i][j]=Math.min(dist[i][j],dist[i][k]+dist[k][j]);                &#125;    &#125;    public static void main(String[]args)&#123;        Scanner sc=new Scanner(System.in);        n=sc.nextInt();        for(int i=1;i&lt;=n;i++)&#123;            int x=sc.nextInt();            int y=sc.nextInt();            q[i]=new PII(x,y);        &#125;        for(int i=1;i&lt;=n;i++)&#123;            String s=sc.next();            for(int j=1;j&lt;=n;j++)&#123;                g[i][j]=s.charAt(j-1);            &#125;        &#125;        for(int i=1;i&lt;=n;i++)&#123;            for(int j=1;j&lt;=n;j++)&#123;                if(i!=j)&#123;                    if(g[i][j]==&#x27;1&#x27;)dist[i][j]=get_dist(q[i],q[j]);                    else dist[i][j]=INF;                &#125;            &#125;        &#125;        floyd();        for(int i=1;i&lt;=n;i++)&#123;            for(int j=1;j&lt;=n;j++)&#123;                if(dist[i][j]&lt;INF)                    maxd[i]=Math.max(maxd[i],dist[i][j]);            &#125;        &#125;        double res1=0;        for(int i=1;i&lt;=n;i++)res1=Math.max(res1,maxd[i]);        double res2=INF;        for(int i=1;i&lt;=n;i++)&#123;            for(int j=1;j&lt;=n;j++)&#123;                if(dist[i][j]&gt;=INF)&#123;                    res2=Math.min(res2,get_dist(q[i],q[j])+maxd[i]+maxd[j]);                &#125;            &#125;        &#125;        System.out.printf(&quot;%.6f&quot;,Math.max(res1,res2));    &#125;&#125;]]></content>
      <categories>
        <category>program</category>
        <category>算法模板</category>
      </categories>
      <tags>
        <tag>Floyd</tag>
      </tags>
  </entry>
  <entry>
    <title>images</title>
    <url>/posts/e01fbe6a.html</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>排序和二分</title>
    <url>/posts/c5627fcb.html</url>
    <content><![CDATA[import java.io.*;import java.util.*;public class  阿一_1快排&#123;    static BufferedReader cin = new BufferedReader(new InputStreamReader(System.in));    static PrintWriter coup = new PrintWriter(new OutputStreamWriter(System.out));    static StreamTokenizer t=new StreamTokenizer(cin);    public static void main(String[] args) throws IOException &#123;        t.nextToken();        int n = (int)t.nval;        int[] array = new int[n];        for (int i = 0; i &lt; n; i++) &#123;            t.nextToken();            array[i] = (int)t.nval;        &#125;        quickSort(array, 0, n - 1);        System.out.println(&quot;排序后的数组：&quot;);        for (int i = 0; i &lt; n; i++) &#123;            coup.print(array[i] + &quot; &quot;);            coup.flush();        &#125;    &#125;public static void quickSort(int q[],int l,int r)&#123;    if(l&gt;=r) return;    int i=l-1,j=r+1,x=q[l+r&gt;&gt;1];    while(i&lt;j)&#123;        do i++;while(q[i]&lt;x);        do j--;while(q[j]&gt;x);        if(i&lt;j) swap(q,i,j);    &#125;    quickSort(q,l,j-1);    quickSort(q,j+1,r);&#125;public static void swap(int[] array, int i, int j) &#123;    int temp = array[i];    array[i] = array[j];    array[j] = temp;&#125;&#125;// public static void quickSort(int[] array, int low, int high) &#123;//         if (low &lt; high) &#123;//             int pivotIndex = partition(array, low, high);//             quickSort(array, low, pivotIndex - 1);//             quickSort(array, pivotIndex + 1, high);//         &#125;//     &#125;//     public static int partition(int[] array, int low, int high) &#123;//         int pivot = array[high]; //         int i = low - 1; //         for (int j = low; j &lt; high; j++) &#123;//             if (array[j] &lt; pivot) &#123;//                 i++;//                 swap(array, i, j);//             &#125;//         &#125;//         swap(array, i + 1, high); //     &#125;// &#125;// import java.io.BufferedReader;// import java.io.InputStreamReader;// import java.io.StreamTokenizer;// import java.util.Arrays;// import java.util.Scanner;// public class Main &#123;//  static int []a = new int[5000005];//  static int x;//  public static void main(String[] args) throws Exception&#123;//      int n;//      StreamTokenizer st = new StreamTokenizer(new BufferedReader(new InputStreamReader(System.in)));//      st.nextToken();//      n=(int)st.nval;//      // st.nextToken();//      // x=(int)st.nval;//      for(int i = 0;i&lt;n;i++) &#123;//          st.nextToken();//          a[i]=(int)st.nval;//      &#125;//      quickSort(a,0,n-1);//      for(int i=0;i&lt;n;i++)&#123;//             System.out.print(a[i] + &quot; &quot;);//         &#125;//  &#125;//  public static void quickSort(int q[],int l,int r) &#123;//      if(l&gt;=r)//          return;//      int pivot = q[l];//      int i=l,j=r;//      while(i&lt;j) &#123;//          while(i&lt;j &amp;&amp; q[j]&gt;=pivot) &#123;//              j--;//          &#125;//          while(i&lt;j &amp;&amp; q[i]&lt;=pivot) &#123;//              i++;//          &#125;//          if(i&lt;j) &#123;//              int temp=q[i];//              q[i]=q[j];//              q[j]=temp;//          &#125;//      &#125;//      q[l]=q[i];//      q[i]=pivot;//      //if(i==x) return;//      //if(i&gt;x)//      quickSort(q,l,i-1);//      //else if(i&lt;x)//      quickSort(q,i+1,r);//  &#125;// &#125;]]></content>
      <categories>
        <category>program</category>
        <category>算法模板</category>
      </categories>
      <tags>
        <tag>二分</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode刷题路线</title>
    <url>/posts/5f473243.html</url>
    <content><![CDATA[推荐链接：力扣刷题攻略路线

数学
数组
链表
字符串
哈希表
双指针
递归
栈
队列
树
图与回溯算法
贪心
动态规划

]]></content>
      <categories>
        <category>program</category>
        <category>算法刷题路线</category>
      </categories>
  </entry>
  <entry>
    <title>math</title>
    <url>/posts/a927044d.html</url>
    <content><![CDATA[求100!的约数个数和约数之和
package lanqiao;import java.util.*;public class math0 &#123;    static List&lt;Integer&gt;primes=new LinkedList&lt;Integer&gt;();    static void sPrime(int n) &#123;        boolean[]st=new boolean[n+1];        for(int i=2;i&lt;=n;i++) &#123;            if(!st[i])primes.add(i);            for(int j=0;primes.get(j)*i&lt;=n;j++) &#123;                st[primes.get(j)*i]=true;                if(i%primes.get(j)==0)break;            &#125;        &#125;    &#125;    static int minc(int n,int p) &#123;        int rxp=0;        while(n&gt;0) &#123;            n/=p;            rxp+=n;        &#125;        return rxp;    &#125;    static long qui(long a,long b) &#123;        long res=1;        while(b&gt;0) &#123;            if((b&amp;1)==1) &#123;                res*=a;            &#125;            a=a*a;            b/=2;        &#125;        return res;    &#125;    public static void main(String[] args) &#123;        Scanner scan=new Scanner(System.in);        int n=100;        sPrime(n);        int []exp=new int[primes.size()];        for(int i=0;i&lt;primes.size();i++) &#123;            int p=primes.get(i);            exp[i]=minc(n,p);        &#125;        int numy=1;        for(int e:exp) &#123;            numy*=(e+1);        &#125;        long sumy=1L;        for(int i=0;i&lt;primes.size();i++) &#123;            int p=primes.get(i);            int e=exp[i];            sumy*=(qui(p,e+1)-1)/(p-1);        &#125;        System.out.println(numy+&quot;  &quot;+sumy);        scan.close();    &#125;&#125;
质因数分解
import java.util.HashMap;import java.util.Map;public class PrimeFactorization &#123;    public static void main(String[] args) &#123;        int x = 100; // 待分解的数        Map&lt;Integer, Integer&gt; primeFactors = divide(x);        // 输出质因数及其次数        for (Map.Entry&lt;Integer, Integer&gt; entry : primeFactors.entrySet()) &#123;            System.out.println(entry.getKey() + &quot; &quot; + entry.getValue());        &#125;    &#125;    public static Map&lt;Integer, Integer&gt; divide(int x) &#123;        Map&lt;Integer, Integer&gt; primeFactors = new HashMap&lt;&gt;();        for (int i = 2; i &lt;= x / i; i++) &#123;            if (x % i == 0) &#123;                int count = 0;                while (x % i == 0) &#123;                    x /= i;                    count++;                &#125;                primeFactors.put(i, count);            &#125;        &#125;        // 如果最后剩下的x &gt; 1，说明它本身是质数        if (x &gt; 1) &#123;            primeFactors.put(x, 1);        &#125;        return primeFactors;    &#125;&#125;
欧拉函数（Euler’s Totient Function）
欧拉函数 ϕ(n)ϕ(n) 表示小于或等于 n_n_ 的正整数中与 n_n_ 互质的数的个数。 性质：

若 n_是质数，ϕ(n)=n−1_ϕ(n)=_n_−1。
若 n=pk_n_=*pk_，ϕ(n)=pk−pk−1_ϕ_(n)=_pk_−_p*_k_−1。
若 n=a×b_n_=a_×_b 且 a_a_ 和 b_b_ 互质，ϕ(n)=ϕ(a)×ϕ(b)ϕ(n)=ϕ(a)×_ϕ_(b)。

public class EulerTotient &#123;    public static int eulerTotient(int n) &#123;        int result = n;        for (int p = 2; p * p &lt;= n; p++) &#123;            if (n % p == 0) &#123;                while (n % p == 0) &#123;                    n /= p;                &#125;                result -= result / p;            &#125;        &#125;        if (n &gt; 1) &#123;            result -= result / n;        &#125;        return result;    &#125;    public static void main(String[] args) &#123;        int n = 10;        System.out.println(&quot;欧拉函数φ(&quot; + n + &quot;) = &quot; + eulerTotient(n));    &#125;&#125;
别考，求！
卢卡斯定理（Lucas Theorem）及其适用性
卢卡斯定理的表述
若 p_p_ 是质数，则对于任意整数 1≤m≤n1≤_m_≤_n_，组合数 C(n,m)C(n,m) 满足：
C(n,m)≡C(n mod p,m mod p)×C(⌊np⌋,⌊mp⌋)(modp)C(n,m)≡_C_(n_mod_p,m_mod_p)×_C_(⌊*pn_⌋,⌊_pm_⌋)(mod_p*)
其中：

C(n mod p,m mod p)C(n_mod_p,m_mod_p) 是 小规模组合数（直接用公式计算）。
C(⌊np⌋,⌊mp⌋)C(⌊*pn_⌋,⌊_pm*⌋) 是 递归计算的组合数。

适用条件

p*p* 必须是质数，否则卢卡斯定理不成立。
适用于 n*n* 和 m*m* 较大的情况（如 n,m≤1018_n_,m_≤1018，但 p_p 较小）。
不适用于非质数模数（如 p=4_p_=4 或 p=109+7_p_=109+7 但非质数）。

若p是质数，则对于任意整数 1 &lt;= m &lt;= n，有：    C(n, m) = C(n % p, m % p) * C(n / p, m / p) (mod p)int qmi(int a, int k, int p)  // 快速幂模板&#123;    int res = 1 % p;    while (k)    &#123;        if (k &amp; 1) res = (LL)res * a % p;        a = (LL)a * a % p;        k &gt;&gt;= 1;    &#125;    return res;&#125;int C(int a, int b, int p)  // 通过定理求组合数C(a, b)&#123;    if (a &lt; b) return 0;    LL x = 1, y = 1;  // x是分子，y是分母    for (int i = a, j = 1; j &lt;= b; i --, j ++ )    &#123;        x = (LL)x * i % p;        y = (LL) y * j % p;    &#125;    return x * (LL)qmi(y, p - 2, p) % p;&#125;int lucas(LL a, LL b, int p)&#123;    if (a &lt; p &amp;&amp; b &lt; p) return C(a, b, p);    return (LL)C(a % p, b % p, p) * lucas(a / p, b / p, p) % p;&#125;
扩展卢卡斯定理（Lucas Theorem）
import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;public class ExLucas &#123;    // 质因数分解 p = p1^k1 * p2^k2 * ... * pm^km    public static Map&lt;Integer, Integer&gt; factorize(int p) &#123;        Map&lt;Integer, Integer&gt; factors = new HashMap&lt;&gt;();        for (int i = 2; i * i &lt;= p; i++) &#123;            while (p % i == 0) &#123;                factors.put(i, factors.getOrDefault(i, 0) + 1);                p /= i;            &#125;        &#125;        if (p &gt; 1) factors.put(p, 1);        return factors;    &#125;    // 计算 n! 中去除 p 的因子后的值 mod p^k    public static int factorialMod(int n, int p, int pk) &#123;        if (n == 0) return 1;        int res = 1;        for (int i = 1; i &lt;= pk; i++) &#123;            if (i % p == 0) continue; // 跳过 p 的倍数            res = (res * i) % pk;        &#125;        res = powMod(res, n / pk, pk);        for (int i = 1; i &lt;= n % pk; i++) &#123;            if (i % p == 0) continue;            res = (res * i) % pk;        &#125;        return (res * factorialMod(n / p, p, pk)) % pk;    &#125;    // 快速幂算法    public static int powMod(int base, int exp, int mod) &#123;        int result = 1;        base = base % mod;        while (exp &gt; 0) &#123;            if (exp % 2 == 1) &#123;                result = (result * base) % mod;            &#125;            exp = exp &gt;&gt; 1;            base = (base * base) % mod;        &#125;        return result;    &#125;    // 计算 n! 中 p 的幂次    public static int countP(int n, int p) &#123;        int count = 0;        while (n &gt; 0) &#123;            n /= p;            count += n;        &#125;        return count;    &#125;    // 计算 C(n, m) mod p^k    public static int combModPk(int n, int m, int p, int pk) &#123;        if (m &gt; n) return 0;        int num = factorialMod(n, p, pk);        int den1 = factorialMod(m, p, pk);        int den2 = factorialMod(n - m, p, pk);        int den = (den1 * den2) % pk;        int invDen = modInverse(den, pk); // 逆元        int powP = countP(n, p) - countP(m, p) - countP(n - m, p);        return (num * invDen % pk * powMod(p, powP, pk)) % pk;    &#125;    // 计算模逆元    public static int modInverse(int a, int m) &#123;        return powMod(a, m - 2, m);    &#125;    // 中国剩余定理（CRT）合并同余方程    public static int crt(List&lt;Integer&gt; remainders, List&lt;Integer&gt; mods) &#123;        int M = 1;        for (int mod : mods) &#123;            M *= mod;        &#125;        int res = 0;        for (int i = 0; i &lt; remainders.size(); i++) &#123;            int mi = mods.get(i);            int Mi = M / mi;            int invMi = modInverse(Mi, mi);            res = (res + remainders.get(i) * Mi % M * invMi % M) % M;        &#125;        return res;    &#125;    // 扩展卢卡斯定理：计算 C(n, m) mod p（p 非质数）    public static int exLucas(int n, int m, int p) &#123;        Map&lt;Integer, Integer&gt; factors = factorize(p);        List&lt;Integer&gt; remainders = new ArrayList&lt;&gt;();        List&lt;Integer&gt; mods = new ArrayList&lt;&gt;();        for (Map.Entry&lt;Integer, Integer&gt; entry : factors.entrySet()) &#123;            int prime = entry.getKey();            int exp = entry.getValue();            int pk = (int) Math.pow(prime, exp);            int rem = combModPk(n, m, prime, pk);            remainders.add(rem);            mods.add(pk);        &#125;        return crt(remainders, mods);    &#125;    public static void main(String[] args) &#123;        int n = 100;        int m = 50;        int p = 12; // 非质数模数        int result = exLucas(n, m, p);        System.out.println(&quot;C(&quot; + n + &quot;, &quot; + m + &quot;) mod &quot; + p + &quot; = &quot; + result);    &#125;&#125;
并查集
import java.util.*;public class Main&#123;  static int[]father;  static long[]value;  public static void main(String[] args)&#123;    Scanner scan=new Scanner(System.in);    int N=scan.nextInt();    int M=scan.nextInt();    int Q=scan.nextInt();    father=new int[N+1];    value=new long[N+1];    init(N);    for(int i=0;i&lt;M;i++)&#123;      int left=scan.nextInt();      int right=scan.nextInt();      long s=scan.nextLong();      left--;      union(left,right,s);    &#125;    for(int i=0;i&lt;Q;i++)&#123;      int l=scan.nextInt();      int r=scan.nextInt();      l--;      int lFa=find(l);      int rFa=find(r);      if(lFa==rFa)&#123;        System.out.println(value[l]-value[r]);      &#125;else&#123;        System.out.println(&quot;UNKNOWN&quot;);      &#125;    &#125;    scan.close();  &#125;  static void init(int N)&#123;    for(int i=0;i&lt;=N;i++)&#123;      father[i]=i;    &#125;  &#125;  static int find(int x)&#123;    if(x!=father[x])&#123;      int tmp=father[x];      father[x]=find(father[x]);      value[x]+=value[tmp];    &#125;    return father[x];  &#125;  static void union(int left,int right,long s)&#123;    int lF=find(left);    int rF=find(right);    if(lF!=rF)&#123;      int min=Math.min(lF,rF);      int max=Math.max(lF,rF);      father[min]=max;      value[min]=Math.abs(value[right]-value[left]+s);    &#125;  &#125;&#125;
import java.util.*;// 1:无需package// 2: 类名必须Main, 不可修改public class Main &#123;  static int N=110;  static int n,Q;  static long[][]cnt;  static long[][]ans;  static long[][]lns;    public static void main(String[] args) &#123;        Scanner scan = new Scanner(System.in);        n=scan.nextInt();        Q=scan.nextInt();        cnt=new long[N][N];        ans=new long[N][N];        lns=new long[N][N];        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=n;j++)&#123;            ans[i][j]=scan.nextLong();          &#125;        &#125;        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=n;j++)&#123;            lns[i][j]=scan.nextInt();            cnt[i][j]=lns[i][j];          &#125;        &#125;        if(floyd()&gt;Q)&#123;          System.out.println(-1);          return;        &#125;        long l=0,r=10000010;        while(l&lt;r)&#123;          long mid=l+r&gt;&gt;1;          if(check(mid))r=mid;//minhua          else l=mid+1;        &#125;        System.out.println(r);        scan.close();    &#125;    static long floyd()&#123;      //最短路下线      long a=0;      for(int k=1;k&lt;=n;k++)&#123;          for(int i=1;i&lt;=n;i++)&#123;            for(int j=1;j&lt;=n;j++)&#123;              cnt[i][j]=Math.min(cnt[i][j],cnt[i][k]+cnt[k][j]);            &#125;          &#125;        &#125;        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=n;j++)&#123;            a+=cnt[i][j];          &#125;        &#125;        return a;    &#125;    static boolean check(long x)&#123;      for(int i=1;i&lt;=n;i++)&#123;        for(int j=1;j&lt;=n;j++)&#123;          cnt[i][j]=ans[i][j];        &#125;      &#125;      long h=x/n;      long s=x%n;      for(int i=1;i&lt;=n;i++)&#123;        for(int j=1;j&lt;=n;j++)&#123;          if(i==j)continue;          if(i&lt;=s)cnt[i][j]=Math.max(lns[i][j],cnt[i][j]-1-h);          else cnt[i][j]=Math.max(lns[i][j],cnt[i][j]-h);          cnt[j][i]=cnt[i][j];        &#125;      &#125;      return floyd()&lt;=Q;    &#125;&#125;
dij
package lanqiao;import java.util.*;public class dj &#123;    static int m,n;    static long[][]dis;    static boolean[][]st;    static ArrayList&lt;long[]&gt;[][]list;    public static void main(String[] args) &#123;        Scanner scan=new Scanner(System.in);        n=scan.nextInt();        m=scan.nextInt();        dis=new long[n+1][m+1];        st=new boolean[n+1][m+1];        int[][]a=new int[n+1][m+1];        list=new ArrayList[n+1][m+1];        for(int i=1;i&lt;=n;i++) &#123;            for(int j=1;j&lt;=m;j++) &#123;                list[i][j]=new ArrayList&lt;long[]&gt;();                a[i][j]=scan.nextInt();            &#125;        &#125;        for(int i=1;i&lt;=n;i++) &#123;            for(int j=1;j&lt;=m;j++) &#123;            if(i&gt;1)list[i][j].add(new long[] &#123;i-1,j,a[i-1][j]&#125;);            if(j&gt;1)list[i][j].add(new long[] &#123;i,j-1,a[i][j-1]&#125;);            if(i&lt;n)list[i][j].add(new long[] &#123;i+1,j,a[i+1][j]&#125;);            if(j&lt;m)list[i][j].add(new long[] &#123;i,j+1,a[i][j+1]&#125;);            &#125;        &#125;        dij(a[1][1]);        long max=0;        for(int i=1;i&lt;=n;i++) &#123;            for(int j=1;j&lt;=m;j++) &#123;                max=Math.max(dis[i][j],max);            &#125;        &#125;        System.out.println(max);        scan.close();    &#125;    static void dij(int start) &#123;        for(int i=1;i&lt;=n;i++) &#123;            Arrays.fill(dis[i],Long.MAX_VALUE);        &#125;        PriorityQueue&lt;long[]&gt;p=new PriorityQueue&lt;long[]&gt;(Comparator.comparing(k-&gt;k[2]));        dis[1][1]=start;        p.add(new long[]&#123;1,1,start&#125;);        while(!p.isEmpty()) &#123;            long[]t=p.poll();            int x=(int)t[0];            int y=(int)t[1];            if(st[x][y])continue;            st[x][y]=true;            for(long[]a:list[x][y]) &#123;                int x1=(int)a[0];                int y1=(int)a[1];                long w=a[2];                if(dis[x1][y1]&gt;dis[x][y]+w) &#123;                    dis[x1][y1]=dis[x][y]+w;                    p.add(new long[] &#123;x1,y1,dis[x1][y1]&#125;);                &#125;            &#125;        &#125;    &#125;&#125;
dfs
import java.util.*;// 1:无需package// 2: 类名必须Main, 不可修改public class Main &#123;  static int N=110;  static int n,Q;  static long[][]cnt;  static long[][]ans;  static long[][]lns;    public static void main(String[] args) &#123;        Scanner scan = new Scanner(System.in);        n=scan.nextInt();        Q=scan.nextInt();        cnt=new long[N][N];        ans=new long[N][N];        lns=new long[N][N];        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=n;j++)&#123;            ans[i][j]=scan.nextLong();          &#125;        &#125;        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=n;j++)&#123;            lns[i][j]=scan.nextInt();            cnt[i][j]=lns[i][j];          &#125;        &#125;        if(floyd()&gt;Q)&#123;          System.out.println(-1);          return;        &#125;        long l=0,r=10000010;        while(l&lt;r)&#123;          long mid=l+r&gt;&gt;1;          if(check(mid))r=mid;//minhua          else l=mid+1;        &#125;        System.out.println(r);        scan.close();    &#125;    static long floyd()&#123;      //最短路下线      long a=0;      for(int k=1;k&lt;=n;k++)&#123;          for(int i=1;i&lt;=n;i++)&#123;            for(int j=1;j&lt;=n;j++)&#123;              cnt[i][j]=Math.min(cnt[i][j],cnt[i][k]+cnt[k][j]);            &#125;          &#125;        &#125;        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=n;j++)&#123;            a+=cnt[i][j];          &#125;        &#125;        return a;    &#125;    static boolean check(long x)&#123;      for(int i=1;i&lt;=n;i++)&#123;        for(int j=1;j&lt;=n;j++)&#123;          cnt[i][j]=ans[i][j];        &#125;      &#125;      long h=x/n;      long s=x%n;      for(int i=1;i&lt;=n;i++)&#123;        for(int j=1;j&lt;=n;j++)&#123;          if(i==j)continue;          if(i&lt;=s)cnt[i][j]=Math.max(lns[i][j],cnt[i][j]-1-h);          else cnt[i][j]=Math.max(lns[i][j],cnt[i][j]-h);          cnt[j][i]=cnt[i][j];        &#125;      &#125;      return floyd()&lt;=Q;    &#125;&#125;]]></content>
      <categories>
        <category>program</category>
        <category>算法模板</category>
      </categories>
  </entry>
  <entry>
    <title>数学</title>
    <url>/posts/fe7e69f4.html</url>
    <content><![CDATA[筛质数

埃式筛

const int N=10001;int primes[N],cnt=0;bool st[N];void get_p(int n)&#123;    for(int i=2;i&lt;=n;i++)&#123;        if(!st[i])primes[cnt++]=i;        for(int j=i+i;j&lt;=n;j+=i)&#123;                st[j]=true;        &#125;    &#125;&#125;

线性筛

const int N=10001;int primes[N],cnt=0;bool st[N];void get_prime(int n) &#123;    for(int i=2;i&lt;=n;i++)&#123;        if(!st[i])primes[cnt++]=i;        for(int j=0;primes[j]&lt;=n/i;j++)&#123;            st[primes[j]*i]=true;            if(i%primes[j]==0)break;        &#125;    &#125;&#125;
费马逆元
import java.util.*;public class Main &#123;    static long MOD = (long)1e9 + 7;    public static void main(String[] args) &#123;        Scanner sc = new Scanner(System.in);        int q = sc.nextInt();        int maxN = 0;        int[] nList = new int[q];        int[] mList = new int[q];        // 读取所有查询并找到最大的 n        for (int i = 0; i &lt; q; i++) &#123;            int n = sc.nextInt();            int m = sc.nextInt();            nList[i] = n;            mList[i] = m;            if (n &gt; maxN) &#123;                maxN = n;            &#125;        &#125;        // 预处理阶乘和逆元        long[] factorial = new long[maxN + 1];        long[] inverse = new long[maxN + 1];        factorial[0] = 1;        for (int i = 1; i &lt;= maxN; i++) &#123;            factorial[i] = (factorial[i - 1] * i) % MOD;        &#125;        // 使用费马小定理计算逆元        inverse[maxN] = fastPower(factorial[maxN], MOD - 2, MOD);        for (int i = maxN - 1; i &gt;= 0; i--) &#123;            inverse[i] = (inverse[i + 1] * (i + 1)) % MOD;        &#125;        // 处理每个查询        for (int i = 0; i &lt; q; i++) &#123;            int n = nList[i];            int m = mList[i];            if (m &lt; 0  m &gt; n) &#123;                System.out.println(0);                continue;            &#125;            long ans = (factorial[n] * inverse[m] % MOD) * inverse[n - m] % MOD;            System.out.println(ans);        &#125;        sc.close();    &#125;    // 快速幂算法    public static long fastPower(long base, long exponent, long mod) &#123;        long result = 1;        base %= mod;        while (exponent &gt; 0) &#123;            if ((exponent &amp; 1) == 1) &#123;                result = (result * base) % mod;            &#125;            base = (base * base) % mod;            exponent &gt;&gt;= 1;        &#125;        return result;    &#125;&#125;
快速幂
public class Main &#123;    public static void main(String[] args) &#123;        Solution solution = new Solution();        // 测试用例 1: 正指数        double result1 = solution.myPow(2.0, 10);        System.out.println(&quot;2^10 = &quot; + result1); // 输出: 1024.0        // 测试用例 2: 负指数        double result2 = solution.myPow(2.0, -2);        System.out.println(&quot;2^-2 = &quot; + result2); // 输出: 0.25        // 测试用例 3: 零指数        double result3 = solution.myPow(5.0, 0);        System.out.println(&quot;5^0 = &quot; + result3); // 输出: 1.0        // 测试用例 4: 边界条件 (0^0)        double result4 = solution.myPow(0.0, 0);        System.out.println(&quot;0^0 = &quot; + result4); // 输出: 1.0        // 测试用例 5: 整数溢出测试 (Integer.MIN_VALUE)        double result5 = solution.myPow(2.0, Integer.MIN_VALUE);        System.out.println(&quot;2^&quot; + Integer.MIN_VALUE + &quot; = &quot; + result5); // 输出一个非常小的数    &#125;&#125;class Solution &#123;    public double myPow(double x, int n) &#123;        long N = n; // 将指数转换为 long 类型以避免溢出        if (N &lt; 0) &#123;            x = 1 / x; // 处理负指数            N = -N;    // 将指数转为正数        &#125;        return powIterative(x, N);    &#125;    // 迭代实现快速幂    private double powIterative(double x, long n) &#123;        double result = 1.0; // 初始化结果        while (n &gt; 0) &#123;            if (n % 2 == 1) &#123; // 如果指数是奇数                result *= x;  // 将当前的 x 乘到结果中            &#125;            x *= x; // 平方 x            n /= 2; // 将指数减半        &#125;        return result;    &#125;&#125;
欧几里得和欧拉
import java.util.Scanner;public class Main&#123;    static int mod=9901; // 定义模数，用于结果取模    public static void main(String[] args)&#123;        Scanner scan=new Scanner(System.in);        int A=scan.nextInt(),B=scan.nextInt(); // 输入A和B        int res=1; // 初始化结果为1，用于累积各质因数等比和的乘积                // 质因数分解A，从2开始试除        for(int i=2;i*i&lt;=A;i++)&#123;            if(A%i==0)&#123;                int s=0; // 记录当前质因数i的指数                while(A%i==0)&#123;                    A/=i; // 除去所有i因子                    s++;                &#125;                // 计算i^(s*B)的等比和，并累积到结果中                res=res*sum(i,s*B+1)%mod; // s*B+1项（0次方到s*B次方）            &#125;        &#125;        // 处理剩余的质因数（当A是质数时）        if(A&gt;1)res=res*sum(A,B+1)%mod; // 指数为1*B，项数为B+1        if(A==0)res=0; // 特判A=0的情况，结果直接为0        System.out.println(res);    &#125;        // 快速幂算法：计算a^k % mod    public static int quic(int a,int k)&#123;        int res=1;        a%=mod; // 先取模，防止溢出        while(k&gt;0)&#123;            if((k&amp;1)==1)&#123; // 如果当前二进制位为1，乘入结果                res=(res*a)%mod;            &#125;            a=(a*a)%mod; // a自乘，相当于计算下一位的权值            k&gt;&gt;=1; // 右移一位，处理下一位二进制        &#125;        return res;    &#125;        // 分治法计算等比数列和：1 + p + p^2 + ... + p^(k-1)    public static int sum(int p,int k)&#123;        if(k==1)return 1; // 边界条件，只有1项        if(k%2==0)&#123; // 当k为偶数时，拆分成两部分            // sum(p,k) = (1 + p^(k/2)) * sum(p, k/2)            return ((1+quic(p,k/2))*sum(p,k/2))%mod;        &#125; else &#123; // 当k为奇数时，拆分为前k-1项和最后一项            // sum(p,k) = sum(p,k-1) + p^(k-1)            return (sum(p,k-1)+quic(p,k-1))%mod;        &#125;    &#125;&#125;]]></content>
      <categories>
        <category>program</category>
        <category>算法模板</category>
      </categories>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>前缀和与差分</title>
    <url>/posts/77a88a6b.html</url>
    <content><![CDATA[一维前缀和
import java.util.Scanner;// 1:无需package// 2: 类名必须Main, 不可修改public class Main &#123;    public static void main(String[] args) &#123;        Scanner sc = new Scanner(System.in);        int n=sc.nextInt();        int q=sc.nextInt();        int []a=new int[n+1];        int []sum=new int[n+1];        for(int i=1;i&lt;=n;i++)&#123;          a[i]=sc.nextInt();          sum[i]=sum[i-1]+a[i];        &#125;        for(int i=0;i&lt;q;i++)&#123;        int l=sc.nextInt();        int r=sc.nextInt();        System.out.println(sum[r]-sum[l-1]);        &#125;        sc.close();    &#125;&#125;
二维前缀和
import java.util.Scanner;// 1:无需package// 2: 类名必须Main, 不可修改public class Main &#123;    public static void main(String[] args) &#123;        Scanner scan = new Scanner(System.in);        int n=scan.nextInt();        int m=scan.nextInt();        int q=scan.nextInt();        int [][]a=new int[n+1][m+1];        int [][]sum=new int[n+1][m+1];        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=m;j++)&#123;            a[i][j]=scan.nextInt();            sum[i][j]=sum[i-1][j]+sum[i][j-1]-sum[i-1][j-1]+a[i][j];          &#125;        &#125;        for(int i=0;i&lt;q;i++)&#123;          int x1=scan.nextInt();          int y1=scan.nextInt();          int x2=scan.nextInt();          int y2=scan.nextInt();          int res=sum[x2][y2]-sum[x1-1][y2]-sum[x2][y1-1]+sum[x1-1][y1-1];          System.out.println(res);        &#125;        scan.close();    &#125;&#125;
一维差分
import java.util.Scanner;// 1:无需package// 2: 类名必须Main, 不可修改public class Main &#123;    public static void main(String[] args) &#123;        Scanner scan = new Scanner(System.in);        int n=scan.nextInt();        int q=scan.nextInt();        int []a=new int[n+10];        int []b=new int[n+10];        int []c=new int[n+10];        for(int i=1;i&lt;=n;i++)&#123;          a[i]=scan.nextInt();          b[i]=a[i]-a[i-1];        &#125;        for(int i=1;i&lt;=q;i++)&#123;          int l=scan.nextInt();          int r=scan.nextInt();          int d=scan.nextInt();          b[l]+=d;          b[r+1]-=d;        &#125;        for(int i=1;i&lt;=n;i++)&#123;          c[i]=c[i-1]+b[i];        &#125;        for(int i=1;i&lt;=n;i++)          System.out.print(c[i]+&quot; &quot;);        scan.close();    &#125;&#125;
二维差分
import java.util.Scanner;// 1:无需package// 2: 类名必须Main, 不可修改public class Main &#123;    public static void main(String[] args) &#123;        Scanner scan = new Scanner(System.in);        int n=scan.nextInt();        int m=scan.nextInt();        int q=scan.nextInt();        int [][]a=new int[n+10][m+10];        int [][]b=new int[n+10][m+10];        int [][]c=new int[n+10][m+10];        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=m;j++)&#123;              a[i][j]=scan.nextInt();              b[i][j]=a[i][j]-a[i-1][j]-a[i][j-1]+a[i-1][j-1];          &#125;        &#125;        for(int i=1;i&lt;=q;i++)&#123;          int x1=scan.nextInt();          int y1=scan.nextInt();          int x2=scan.nextInt();          int y2=scan.nextInt();          int d=scan.nextInt();          b[x1][y1]+=d;          b[x2+1][y1]-=d;          b[x1][y2+1]-=d;          b[x2+1][y2+1]+=d;        &#125;        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=m;j++)&#123;            b[i][j]=b[i][j]+b[i][j-1]+b[i-1][j]-b[i-1][j-1];          &#125;        &#125;        for(int i=1;i&lt;=n;i++)&#123;          for(int j=1;j&lt;=m;j++)&#123;            System.out.print(b[i][j]+&quot; &quot;);          &#125;          System.out.println();        &#125;        scan.close();    &#125;&#125;]]></content>
      <categories>
        <category>program</category>
        <category>算法模板</category>
      </categories>
      <tags>
        <tag>前缀和</tag>
        <tag>差分</tag>
      </tags>
  </entry>
  <entry>
    <title>SPFA</title>
    <url>/posts/33b8c865.html</url>
    <content><![CDATA[给定一张nnn个点m条边的有向图，该图可以有自环与重边。
你需要判断从 1 号点出发，图中是否存在负权回路，存在输出 Yes；不存在输出 No。
import java.util.*;import java.io.*;// 1:无需package// 2: 类名必须Main, 不可修改public class Main &#123;    static int N=2010;    static int M=(int)1e4+10;    static long INF=0x3f3f3f3f3f3f3f3fL;    static int[]h=new int[N];    static long[]dist=new long[N];    static int[]e=new int[M*2],ne=new int[M*2];    static int[]w=new int[M*2];    static int[]cnt=new int[M*2];    static boolean[]st=new boolean[N];    static int idx,n,m;    static void add(int a,int b,int c)&#123;      e[idx]=b;      w[idx]=c;      ne[idx]=h[a];      h[a]=idx++;    &#125;    static boolean spfa()&#123;      Arrays.fill(st,false);      Arrays.fill(dist,INF);      dist[1]=0;      Queue&lt;Integer&gt;q=new LinkedList&lt;&gt;();      st[1]=true;      q.offer(1);      while(!q.isEmpty())&#123;        int t=q.poll();        st[t]=false;        for(int i=h[t];i!=-1;i=ne[i])&#123;          int j=e[i];          if(dist[j]&gt;dist[t]+w[i])&#123;            dist[j]=dist[t]+w[i];            cnt[j]=cnt[t]+1;            if(cnt[j]&gt;=n)return true;            if(!st[j])&#123;              q.offer(j);              st[j]=true;            &#125;          &#125;        &#125;      &#125;      return false;    &#125;    static BufferedReader in=new BufferedReader(new InputStreamReader(System.in));    public static void main(String[] args) throws IOException&#123;        Arrays.fill(h,-1);        String[]ss=in.readLine().split(&quot; &quot;);        n=Integer.parseInt(ss[0]);        m=Integer.parseInt(ss[1]);        for(int i=1;i&lt;=m;i++)&#123;          String[]s=in.readLine().split(&quot; &quot;);          int a=Integer.parseInt(s[0]);          int b=Integer.parseInt(s[1]);          int c=Integer.parseInt(s[2]);          add(a,b,c);        &#125;        if(spfa())System.out.println(&quot;Yes&quot;);        else System.out.println(&quot;No&quot;);    &#125;&#125;]]></content>
      <categories>
        <category>program</category>
        <category>算法模板</category>
      </categories>
      <tags>
        <tag>spfa</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度下降</title>
    <url>/posts/3c50d4b7.html</url>
    <content><![CDATA[梯度下降
线性回归预测房价

数据加载
数据介绍
数据拆分
数据建模
数据预测
数据评估

1、无约束最优化问题
1.1、无约束最优化
无约束最优化问题（unconstrained optimizationproblem）指的是从一个问题的所有可能的备选方案中，选择出依某种指标来说是最优的解决方案。从数学上说，最优化是研究在一个给定的集合S上泛函J(theta)J(\\theta)J(theta)的极小化或极大化问题：广义上，最优化包括数学规划、图和网络、组合最优化、库存论、决策论、排队论、最优控制等。狭义上，最优化仅指数学规划。
1.2、梯度下降
梯度下降法(Gradient Descent)是一个算法，但不是像多元线性回归那样是一个具体做回归任务的算法，而是一个非常通用的优化算法来帮助一些机器学习算法（都是无约束最优化问题）求解出最优解， 所谓的通用就是很多机器学习算法都是用梯度下降，甚至深度学习也是用它来求解最优解。所有优化算法的目的都是期望以最快的速度把模型参数θ求解出来，梯度下降法就是一种经典常用的优化算法。
之前利用正规方程求解的 θ 是最优解的原因是 MSE 这个损失函数是凸函数。但是，机器学习的损失函数并非都是凸函数，设置导数为 0 会得到很多个极值，不能确定唯一解。

使用正规方程 theta=(XTX)−1XTy\\theta = (X^TX)^{-1}X^Tytheta=(XTX)−1XTy 求解的另一个限制是特征维度（X_1、X_2……、X_nX\_1、X\_2……、X\_nX_1、X_2……、X_n）不能太多，矩阵逆运算的时间复杂度通常为 O(n3)O(n^3)O(n3) 。换句话说，就是如果特征数量翻倍，你的计算时间大致为原来的 232^323 倍，也就是之前时间的8倍。举个例子，2 个特征 1 秒，4 个特征就是 8 秒，8 个特征就是 64 秒，16 个特征就是 512 秒，当特征更多的时候呢？运行时间会非常漫长~
所以正规方程求出最优解并不是机器学习甚至深度学习常用的手段。
之前我们令导数为 0，反过来求解最低点 θ 是多少，而梯度下降法是一点点去逼近最优解!

其实这就跟生活中的情形很像，比如你问一个朋友的工资是多少，他说你猜？那就很难了，他说你猜完我告诉你是猜高了还是猜低了，这样你就可以奔着对的方向一直猜下去，最后总会猜对！梯度下降法就是这样的，多次尝试。并且，在试的过程中还得想办法知道是不是在猜对的路上，说白了就是得到正确的反馈再调整然后继续猜才有意义~
这个就好比道士下山，我们把 Loss （或者称为Cost，即损失）曲线看成是山谷，如果走过了，就再往回返，所以是一个迭代的过程。
1.3、梯度下降公式
这里梯度下降法的公式就是一个式子指导计算机迭代过程中如何去调整theta\\thetatheta，可以通过泰勒公式一阶展开来进行推导和证明：


\\theta^{n + 1} = \\theta^{n} - \\alpha \* gradient
其中 alpha\\alphaalpha 表示学习率，gradient 表示梯度


\\theta^{n + 1} = \\theta^{n} - \\alpha \* \\frac{\\partial J(\\theta)}{\\partial \\theta}
有些公式，使用其他字母表示：


\\theta^{n + 1} = \\theta^{n} - \\eta \* \\frac{\\partial J(\\theta)}{\\partial \\theta}


w\_j^{n + 1} = w\_j^{n} - \\eta \* \\frac{\\partial J(\\theta)}{\\partial \\theta\_j}


这里的 w_jw\_jw_j 就是 theta\\thetatheta 中的某一个 j = 0…m，这里的 eta\\etaeta 就是梯度下降图里的 learning step，很多时候也叫学习率 learning rate，很多时候也用 alpha\\alphaalpha 表示，这个学习率我们可以看作是下山迈的步子的大小，步子迈的大下山就快。

学习率一般都是正数，如果在山左侧（曲线左半边）梯度是负的，那么这个负号就会把 w_jw\_jw_j 往大了调， 如果在山右侧（曲线右半边）梯度就是正的，那么负号就会把 w_jw\_jw_j 往小了调。每次 w_jw\_jw_j 调整的幅度就是 \\eta \* gradient，就是横轴上移动的距离。
因此，无论在左边，还是在右边，梯度下降都可以快速找到最优解，实现快速下山~
如果特征或维度越多，那么这个公式用的次数就越多，也就是每次迭代要应用的这个式子多次（多少特征，就应用多少次），所以其实上面的图不是特别准，因为 theta\\thetatheta 对应的是很多维度，应该每一个维度都可以画一个这样的图，或者是一个多维空间的图。

w\_0^{n + 1} = w\_0^{n} - \\eta \* \\frac{\\partial J(\\theta)}{\\partial \\theta\_0}
w\_1^{n + 1} = w\_1^{n} - \\eta \* \\frac{\\partial J(\\theta)}{\\partial \\theta\_1}
……
w\_m^{n + 1} = w\_m^{n} - \\eta \* \\frac{\\partial J(\\theta)}{\\partial \\theta\_m}


所以观察上图我们可以发现不是某一个 theta_0\\theta\_0theta_0 或 theta_1\\theta\_1theta_1 找到最小值就是最优解，而是它们一起找到 J(theta)J(\\theta)J(theta) 最小值才是最优解。
1.4、学习率
根据我们上面讲的梯度下降公式，我们知道 eta\\etaeta 是学习率，设置大的学习率 w_jw\_jw_j 每次调整的幅度就大，设置小的学习率 w_jw\_jw_j 每次调整的幅度就小，然而如果步子迈的太大也会有问题，俗话说步子大了容易扯着蛋！学习率大，可能一下子迈过了，到另一边去了（从曲线左半边跳到右半边），继续梯度下降又迈回来， 使得来来回回震荡。步子太小呢，就像蜗牛一步步往前挪，也会使得整体迭代次数增加。

学习率的设置是门一门学问，一般我们会把它设置成一个比较小的正整数，0.1、0.01、0.001、0.0001，都是常见的设定数值（然后根据情况调整）。一般情况下学习率在整体迭代过程中是不变，但是也可以设置成随着迭代次数增多学习率逐渐变小，因为越靠近山谷我们就可以步子迈小点，可以更精准的走入最低点，同时防止走过。还有一些深度学习的优化算法会自己控制调整学习率这个值，后面学习过程中这些策略在讲解代码中我们会一一讲到。

1.5、全局最优化

上图显示了梯度下降的两个主要挑战：

若随机初始化，算法从左侧起步，那么会收敛到一个局部最小值，而不是全局最小值；
若随机初始化，算法从右侧起步，那么需要经过很长时间才能越过Plateau（函数停滞带，梯度很小），如果停下得太早，则永远达不到全局最小值；

而线性回归的模型MSE损失函数恰好是个凸函数，凸函数保证了只有一个全局最小值，其次是个连续函数，斜率不会发生陡峭的变化，因此即便是乱走，梯度下降都可以趋近全局最小值。
上图损失函数是非凸函数，梯度下降法是有可能落到局部最小值的，所以其实步长不能设置的太小太稳健，那样就很容易落入局部最优解，虽说局部最小值也没大问题， 因为模型只要是堪用的就好嘛，但是我们肯定还是尽量要奔着全局最优解去！
1.6、梯度下降步骤
梯度下降流程就是“猜”正确答案的过程:


1、“瞎蒙”，Random 随机数生成 theta\\thetatheta，随机生成一组数值 w_0、w_1……w_nw\_0、w\_1……w\_nw_0、w_1……w_n ，期望 mu\\mumu 为 0 方差 sigma\\sigmasigma 为 1 的正太分布数据。


2、求梯度 g ，梯度代表曲线某点上的切线的斜率，沿着切线往下就相当于沿着坡度最陡峭的方向下降


3、if g &lt; 0, theta\\thetatheta 变大，if g &gt; 0, theta\\thetatheta 变小


4、判断是否收敛 convergence，如果收敛跳出迭代，如果没有达到收敛，回第 2 步再次执行2~4步
收敛的判断标准是：随着迭代进行损失函数Loss，变化非常微小甚至不再改变，即认为达到收敛



1.7、代码模拟梯度下降


梯度下降优化算法，比正规方程，应用更加广泛


什么是梯度？

梯度就是导数对应的值！



下降？

涉及到优化问题，最小二乘法



梯度下降呢？

梯度方向下降，速度最快的~



接下来，我们使用代码来描述上面梯度下降的过程：
方程如下：
f(x)=(x−3.5)2−4.5x+10f(x) = (x - 3.5)^2 - 4.5x + 10f(x)=(x−3.5)2−4.5x+10

使用梯度下降的思想，来一步步逼近，函数的最小值。
import numpy as npimport matplotlib.pyplot as pltf = lambda x : (x - 3.5)**2 -4.5*x + 10# 导函数d = lambda x :2*(x - 3.5) - 4.5 # 梯度 == 导数# 梯度下降的步幅，比例，（学习率，幅度）step = 0.1# 求解当x等于多少的时候，函数值最小。求解目标值：随机生成的# 相等于：&#x27;瞎蒙&#x27; ----&gt; 方法 ----&gt; 优化x = np.random.randint(0,12,size = 1)[0]# 梯度下降，每下降一步，每走一步，目标值，都会更新。# 更新的这个新值和上一步的值，差异，如果差异很小（万分之一）# 梯度下降退出last_x = x + 0.02 # 记录上一步的值，首先让last_x和x有一定的差异！！！# 精确率，真实计算，都是有误差，自己定义precision = 1e-4print(&#x27;+++++++++++++++++++++&#x27;, x)x_ = [x]while True:    # 退出条件，精确度，满足了    if np.abs(x - last_x) &lt; precision:        break         # 更新    last_x = x    x -= step*d(x) # 更新，减法：最小值    x_.append(x)    print(&#x27;--------------------&#x27;,x)# 数据可视化plt.rcParams[&#x27;font.family&#x27;] = &#x27;Kaiti SC&#x27;plt.figure(figsize=(9,6))x = np.linspace(5.75 - 5, 5.75 + 5, 100)y = f(x)plt.plot(x,y,color = &#x27;green&#x27;)plt.title(&#x27;梯度下降&#x27;,size = 24,pad = 15)x_ = np.array(x_)y_ = f(x_)plt.scatter(x_, y_,color = &#x27;red&#x27;)plt.savefig(&#x27;./图片/5-梯度下降.jpg&#x27;,dpi = 200)
函数的最优解是：5.75。你可以发现，随机赋值的变量 x ，无论大于5.75，还是小于5.75，经过梯度下降，最终都慢慢靠近5.75这个最优解！


注意：

梯度下降存在一定误差，不是完美解~
在误差允许的范围内，梯度下降所求得的机器学习模型，是堪用的！
梯度下降的步幅step，不能太大，俗话说步子不能迈的太大！
精确度，可以根据实际情况调整
while True循环里面，持续进行梯度下降：

theta=theta−etafracpartialpartialthetaJ(theta)\\theta = \\theta - \\eta \\frac{\\partial}{\\partial \\theta}J(\\theta)theta=theta−etafracpartialpartialthetaJ(theta) 其中的 $\eta $ 叫做学习率
x=x−etafracpartialpartialxf(x)x = x - \\eta\\frac{\\partial}{\\partial x}f(x)x=x−etafracpartialpartialxf(x)
x = x - step\*\\frac{\\partial}{\\partial x} f(x) 其中的 $step $ 叫做学习率
x = x - step \* f'(x)

while 循环退出条件是：x更新之后和上一次相差绝对值小于特定精确度！

2、梯度下降方法
2.1、三种梯度下降不同
梯度下降分三类：批量梯度下降BGD（Batch Gradient Descent）、小批量梯度下降MBGD（Mini-Batch Gradient Descent）、随机梯度下降SGD（Stochastic Gradient Descent）。

三种梯度下降有什么不同呢？我们从梯度下降步骤开始讲起，梯度下降步骤分一下四步：


1、随机赋值，Random 随机数生成 theta\\thetatheta，随机一组数值 w_0、w_1……w_nw\_0、w\_1……w\_nw_0、w_1……w_n


2、求梯度 g ，梯度代表曲线某点上的切线的斜率，沿着切线往下就相当于沿着坡度最陡峭的方向下降


3、if g &lt; 0, theta\\thetatheta 变大，if g &gt; 0, theta\\thetatheta 变小


4、判断是否收敛 convergence，如果收敛跳出迭代，如果没有达到收敛，回第 2 步再次执行2~4步
收敛的判断标准是：随着迭代进行损失函数Loss，变化非常微小甚至不再改变，即认为达到收敛


三种梯度下降不同，体现在第二步中：


BGD是指在每次迭代使用所有样本来进行梯度的更新


MBGD是指在每次迭代使用一部分样本（所有样本500个，使用其中32个样本）来进行梯度的更新


SGD是指每次迭代随机选择一个样本来进行梯度更新


2.2、线性回归梯度更新公式
回顾上一讲公式！
最小二乘法公式如下：
J(theta)=frac12sumlimitsi=1n(htheta(x(i))−y(i))2J(\\theta) = \\frac{1}{2}\\sum\\limits_{i = 1}^n(h_{\\theta}(x^{(i)}) - y^{(i)})^2J(theta)=frac12sumlimitsi=1n​(htheta​(x(i))−y(i))2
矩阵写法：
J(theta)=frac12(Xtheta−y)T(Xtheta−y)J(\\theta) = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y)J(theta)=frac12(Xtheta−y)T(Xtheta−y)
接着我们来讲解如何求解上面梯度下降的第 2 步，即我们要推导出损失函数的导函数来。


\\theta\_j^{n + 1} = \\theta\_j^{n} - \\eta \* \\frac{\\partial J(\\theta)}{\\partial \\theta\_j} 其中 j 表示第 j 个系数


fracpartialJ(theta)partialtheta_j=fracpartialpartialthetajfrac12(htheta(x)−y)2\\frac{\\partial J(\\theta)}{\\partial \\theta\_j} = \\frac{\\partial}{\\partial \\theta_j}\\frac{1}{2}(h_{\\theta}(x) - y)^2fracpartialJ(theta)partialtheta_j=fracpartialpartialthetaj​frac12(htheta​(x)−y)2


= \\frac{1}{2}\*2(h\_{\\theta}(x) - y)\\frac{\\partial}{\\partial \\theta_j}(h_{\\theta}(x) - y) (1)
=(h_theta(x)−y)fracpartialpartialthetaj(sumlimitsi=0ntheta_ix_i−y)= (h\_{\\theta}(x) - y)\\frac{\\partial}{\\partial \\theta_j}(\\sum\\limits_{i = 0}^n\\theta\_ix\_i - y)=(h_theta(x)−y)fracpartialpartialthetaj​(sumlimitsi=0n​theta_ix_i−y) (2)
=(h_theta(x)−y)x_j= (h\_{\\theta}(x) - y)x\_j=(h_theta(x)−y)x_j (3)
x2x^2x2的导数就是 2x，根据链式求导法则，我们可以推出上面第（1）步。然后是多元线性回归，所以 h_theta(x)h\_{\\theta}(x)h_theta(x) 就 是 thetaTx\\theta^TxthetaTx 即是w_0x_0+w_1x_1+……+w_nxnw\_0x\_0 + w\_1x\_1 + …… + w\_nx_nw_0x_0+w_1x_1+……+w_nxn​ 即sumlimitsi=0ntheta_ix_i\\sum\\limits_{i = 0}^n\\theta\_ix\_isumlimitsi=0n​theta_ix_i。到这里我们是对 theta_j\\theta\_jtheta_j 来求偏导，那么和 w_jw\_jw_j 没有关系的可以忽略不计，所以只剩下 x_jx\_jx_j。
我们可以得到结论就是 theta_j\\theta\_jtheta_j 对应的梯度与预测值 haty\\hat{y}haty 和真实值 y 有关，这里 haty\\hat{y}haty 和 y 是列向量（即多个数据），同时还与 theta_j\\theta\_jtheta_j 对应的特征维度 x_jx\_jx_j 有关，这里 x_jx\_jx_j 是原始数据集矩阵的第 j 列。如果我们分别去对每个维度 theta_0、theta_1……theta_n\\theta\_0、\\theta\_1……\\theta\_ntheta_0、theta_1……theta_n 求偏导，即可得到所有维度对应的梯度值。

g0=(htheta(x)−y)x_0g_0 = (h_{\\theta}(x) - y)x\_0g0​=(htheta​(x)−y)x_0
g1=(htheta(x)−y)x_1g_1 = (h_{\\theta}(x) - y)x\_1g1​=(htheta​(x)−y)x_1
……
gj=(htheta(x)−y)x_jg_j = (h_{\\theta}(x) - y)x\_jgj​=(htheta​(x)−y)x_j

总结：
\\theta\_j^{n + 1} = \\theta_j^{n} - \\eta \* (h_{\\theta}(x) - y )x\_j
2.3、批量梯度下降BGD
批量梯度下降法是最原始的形式，它是指在每次迭代使用所有样本来进行梯度的更新。每次迭代参数更新公式如下：
\\theta\_j^{n + 1} = \\theta_j^{n} - \\eta \*\\frac{1}{n}\\sum\\limits_{i = 1}^{n} (h\_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)}
去掉 frac1n\\frac{1}{n}frac1n 也可以，因为它是一个常量，可以和 eta\\etaeta 合并
\\theta\_j^{n + 1} = \\theta_j^{n} - \\eta\*\\sum\\limits_{i = 1}^{n} (h\_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)}

矩阵写法：
\\theta^{n + 1} = \\theta^{n} - \\eta \* X^T(X\\theta -y)
其中 𝑖 = 1, 2, …, n 表示样本数， 𝑗 = 0, 1……表示特征数，这里我们使用了偏置项，即解决x_0(i)=1x\_0^{(i)} = 1x_0(i)=1。
注意这里更新时存在一个求和函数，即为对所有样本进行计算处理！
优点：   （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。   （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。 缺点：   （1）当样本数目 n 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。
从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：

2.4、随机梯度下降SGD
随机梯度下降法不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。每次迭代参数更新公式如下：
\\theta\_j^{n + 1} = \\theta_j^{n} - \\eta \*(h_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)}
批量梯度下降算法每次都会使用全部训练样本，因此这些计算是冗余的，因为每次都使用完全相同的样本集。而随机梯度下降算法每次只随机选择一个样本来更新模型参数，因此每次的学习是非常快速的。
优点：   （1）由于不是在全部训练数据上的更新计算，而是在每轮迭代中，随机选择一条数据进行更新计算，这样每一轮参数的更新速度大大加快。   缺点：   （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。   （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。
解释一下为什么SGD收敛速度比BGD要快：

这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）。
而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被迭代30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。
也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。

从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程就会盲目一些。其迭代的收敛曲线示意图可以表示如下：

2.5、小批量梯度下降MBGD
小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代使用总样本中的一部分（batch_size）样本来对参数进行更新。这里我们假设 batch_size = 20，样本数 n = 1000 。实现了更新速度与更新次数之间的平衡。每次迭代参数更新公式如下：
\\theta\_j^{n + 1} = \\theta_j^{n} - \\eta \*\\frac{1}{batch\_size}\\sum\\limits_{i = 1}^{batch\_size} (h\_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)}
相对于随机梯度下降算法，小批量梯度下降算法降低了收敛波动性， 即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。
一般情况下，小批量梯度下降是梯度下降的推荐变体，特别是在深度学习中。每次随机选择2的幂数个样本来进行学习，例如：8、16、32、64、128、256。因为计算机的结构就是二进制的。但是也要根据具体问题而选择，实践中可以进行多次试验， 选择一个更新速度与更次次数都较适合的样本数。
MBGD梯度下降迭代的收敛曲线更加温柔一些：

2.6、梯度下降优化
虽然梯度下降算法效果很好，并且广泛使用，但是不管用上面三种哪一种，都存在一些挑战与问题，我们可以从以下几点进行优化:


选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。


学习速率调整，试图在每次更新过程中， 改变学习速率。从经验上看，**学习率在一开始要保持大些来保证收敛速度，在收敛到最优点附近时要小些以避免来回震荡。**比较简单的学习率调整可以通过 **学习率衰减（Learning Rate Decay）**的方式来实现。假设初始化学习率为 eta_0\\eta\_0eta_0，在第 t 次迭代时的学习率 eta_t\\eta\_teta_t。常用的衰减方式为可以设置为 按迭代次数 进行衰减，迭代次数越大，学习率越小！



模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的，或者每个特征有着不同的统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。


对于非凸目标函数，容易陷入那些次优的局部极值点中，如在神经网路中。那么如何避免呢。
简单的问题，一般使用随机梯度下降即可解决。在深度学习里，对梯度下降进行了很多改进，比如：自适应梯度下降。在深度学习章节，我们会具体介绍。


轮次和批次
轮次：epoch，轮次顾名思义是把我们已有的训练集数据学习多少轮，迭代多少次。
批次：batch，批次这里指的的我们已有的训练集数据比较多的时候，一轮要学习太多数据， 那就把一轮次要学习的数据分成多个批次，一批一批数据的学习。
就好比，你要背诵一片《赤壁赋》，很长。你在背诵的时候，一段段的背诵，就是批次batch。花费了一天终于背诵下来了，以后的9天，每天都进行一轮背诵复习，这就是轮次epoch。这样，《赤壁赋》的背诵效果，就非常牢固了。
在进行，机器学习训练时，我们也要合理选择轮次和批次~


3、代码实战梯度下降
3.1、批量梯度下降BGD
这里我们使用了偏置项，即解决x_0(i)=1x\_0^{(i)} = 1x_0(i)=1。一元一次线性回归问题。
import numpy as np# 1、创建数据集X，yX = np.random.rand(100, 1)w,b = np.random.randint(1,10,size = 2)y = w * X  + b + np.random.randn(100, 1)# 2、使用偏置项x_0 = 1，更新XX = np.c_[X,np.ones((100, 1))]# 3、创建超参数轮次epoches = 10000# 4、定义一个函数来调整学习率t0, t1 = 5, 1000def learning_rate_schedule(t):    return t0/(t+t1)# 5、初始化 W0...Wn，标准正太分布创建Wθ = np.random.randn(2, 1)# 6、判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛for i in range(epoches):    # 根据公式计算梯度    g = X.T.dot(X.dot(θ) - y)    # 应用梯度下降的公式去调整 θ 值    learning_rate = learning_rate_schedule(i)    θ = θ - learning_rate * gprint(&#x27;真实斜率和截距是：&#x27;,w,b)print(&#x27;梯度下降计算斜率和截距是：&#x27;,θ)
这里我们使用了偏置项，即解决x_0(i)=1x\_0^{(i)} = 1x_0(i)=1。多元一次线性回归问题。
import numpy as np# 1、创建数据集X，yX = np.random.rand(100, 3)w = np.random.randint(1,10,size = (3,1))b = np.random.randint(1,10,size = 1)y = X.dot(w)  + b + np.random.randn(100, 1)# 2、使用偏置项x_0 = 1，更新XX = np.c_[X,np.ones((100, 1))]# 3、创建超参数轮次epoches = 10000# 4、定义一个函数来调整学习率t0, t1 = 5, 500def learning_rate_schedule(t):    return t0/(t+t1)# 5、初始化 W0...Wn，标准正太分布创建Wθ = np.random.randn(4, 1)# 6、判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛for i in range(epoches):    # 根据公式计算梯度    g = X.T.dot(X.dot(θ) - y)    # 应用梯度下降的公式去调整 θ 值    learning_rate = learning_rate_schedule(i)    θ = θ - learning_rate * gprint(&#x27;真实斜率和截距是：&#x27;,w,b)print(&#x27;梯度下降计算斜率和截距是：&#x27;,θ)
3.2、随机梯度下降SGD
这里我们使用了偏置项，即解决x_0(i)=1x\_0^{(i)} = 1x_0(i)=1。一元一次线性回归问题。
import numpy as np# 1、创建数据集X，yX = 2*np.random.rand(100, 1)w,b = np.random.randint(1,10,size = 2)y = w * X + b + np.random.randn(100, 1)# 2、使用偏置项x_0 = 1，更新XX = np.c_[X, np.ones((100, 1))]# 3、创建超参数轮次、样本数量epochs = 10000n = 100# 4、定义一个函数来调整学习率t0, t1 = 5, 500def learning_rate_schedule(t):    return t0/(t+t1)# 5、初始化 W0...Wn，标准正太分布创建Wθ = np.random.randn(2, 1)# 6、多次for循环实现梯度下降，最终结果收敛for epoch in range(epochs):    # 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序    index = np.arange(n) # 0 ~99    np.random.shuffle(index)    X = X[index] # 打乱顺序    y = y[index]    for i in range(n):        X_i = X[[i]]        y_i = y[[i]]        g = X_i.T.dot(X_i.dot(θ)-y_i)        learning_rate = learning_rate_schedule(epoch*n + i)        θ = θ - learning_rate * gprint(&#x27;真实斜率和截距是：&#x27;,w,b)print(&#x27;梯度下降计算斜率和截距是：&#x27;,θ)
这里我们使用了偏置项，即解决x_0(i)=1x\_0^{(i)} = 1x_0(i)=1。多元一次线性回归问题。
import numpy as np# 1、创建数据集X，yX = 2*np.random.rand(100, 5)w = np.random.randint(1,10,size = (5,1))b = np.random.randint(1,10,size = 1)y = X.dot(w) + b + np.random.randn(100, 1)# 2、使用偏置项x_0 = 1，更新XX = np.c_[X, np.ones((100, 1))]# 3、创建超参数轮次、样本数量epochs = 10000n = 100# 4、定义一个函数来调整学习率t0, t1 = 5, 500def learning_rate_schedule(t):    return t0/(t+t1)# 5、初始化 W0...Wn，标准正太分布创建Wθ = np.random.randn(6, 1)# 6、多次for循环实现梯度下降，最终结果收敛for epoch in range(epochs):    # 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序    index = np.arange(n) # 0 ~99    np.random.shuffle(index)    X = X[index] # 打乱顺序    y = y[index]    for i in range(n):        X_i = X[[i]]        y_i = y[[i]]        g = X_i.T.dot(X_i.dot(θ)-y_i)        learning_rate = learning_rate_schedule(epoch*n + i)        θ = θ - learning_rate * gprint(&#x27;真实斜率和截距是：&#x27;,w,b)print(&#x27;梯度下降计算斜率和截距是：&#x27;,θ)
3.3、小批量梯度下降MBGD
这里我们使用了偏置项，即解决x_0(i)=1x\_0^{(i)} = 1x_0(i)=1。一元一次线性回归问题。
import numpy as np# 1、创建数据集X，yX = np.random.rand(100, 1)w,b = np.random.randint(1,10,size = 2)y = w * X + b + np.random.randn(100, 1)# 2、使用偏置项x_0 = 1，更新XX = np.c_[X, np.ones((100, 1))]# 3、定义一个函数来调整学习率t0, t1 = 5, 500def learning_rate_schedule(t):    return t0/(t+t1)# 4、创建超参数轮次、样本数量、小批量数量epochs = 100n = 100batch_size = 16num_batches = int(n / batch_size)# 5、初始化 W0...Wn，标准正太分布创建Wθ = np.random.randn(2, 1)# 6、多次for循环实现梯度下降，最终结果收敛for epoch in range(epochs):    # 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序    index = np.arange(n)    np.random.shuffle(index)    X = X[index]    y = y[index]    for i in range(num_batches):        # 一次取一批数据16个样本        X_batch = X[i * batch_size : (i + 1)*batch_size]        y_batch = y[i * batch_size : (i + 1)*batch_size]        g = X_batch.T.dot(X_batch.dot(θ)-y_batch)        learning_rate = learning_rate_schedule(epoch * n + i)        θ = θ - learning_rate * gprint(&#x27;真实斜率和截距是：&#x27;,w,b)print(&#x27;梯度下降计算斜率和截距是：&#x27;,θ)
这里我们使用了偏置项，即解决x_0(i)=1x\_0^{(i)} = 1x_0(i)=1。多元一次线性回归问题。
import numpy as np# 1、创建数据集X，yX = np.random.rand(100, 3)w = np.random.randint(1,10,size = (3,1))b = np.random.randint(1,10,size = 1)y = X.dot(w) + b + np.random.randn(100, 1)# 2、使用偏置项 X_0 = 1，更新XX = np.c_[X, np.ones((100, 1))]# 3、定义一个函数来调整学习率t0, t1 = 5, 500def learning_rate_schedule(t):    return t0/(t+t1)# 4、创建超参数轮次、样本数量、小批量数量epochs = 10000n = 100batch_size = 16num_batches = int(n / batch_size)# 5、初始化 W0...Wn，标准正太分布创建Wθ = np.random.randn(4, 1)# 6、多次for循环实现梯度下降，最终结果收敛for epoch in range(epochs):    # 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序    index = np.arange(n)    np.random.shuffle(index)    X = X[index]    y = y[index]    for i in range(num_batches):        # 一次取一批数据16个样本        X_batch = X[i * batch_size : (i + 1)*batch_size]        y_batch = y[i * batch_size : (i + 1)*batch_size]        g = X_batch.T.dot(X_batch.dot(θ)-y_batch)        learning_rate = learning_rate_schedule(epoch * n + i)        θ = θ - learning_rate * gprint(&#x27;真实斜率和截距是：&#x27;,w,b)print(&#x27;梯度下降计算斜率和截距是：&#x27;,θ)]]></content>
      <categories>
        <category>program</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>树状数组</title>
    <url>/posts/59a0de58.html</url>
    <content><![CDATA[树状数组
import java.util.*;public class ttree &#123;    static int n,s;    static int []a;    static long res=0;    static List&lt;Integer&gt;[]g;    static int[]tr;    static void modify(int i,int val) &#123;        while(i&lt;=n) &#123;            tr[i]+=val;            i+=i&amp;-i;        &#125;    &#125;    static int query(int i) &#123;        int sum=0;        while(i&gt;0) &#123;            sum+=tr[i];            i-=i&amp;-i;        &#125;        return sum;    &#125;    static int sum(int i) &#123;        return query(n)-query(i);    &#125;    static void dfs(int u,int fa) &#123;        modify(a[u],1);        for(int v:g[u]) &#123;            if(v!=fa) &#123;                dfs(v,u);            &#125;        &#125;        for(int k=2;k*a[u]&lt;=n;k++) &#123;            int t=k*a[u];            res-=query(t)-query(t-1);        &#125;        res+=sum(a[u]);        modify(a[u], -1);    &#125;//  static void dfs(int u, int p) &#123;//      int before = t0.query(a[u] - 1); // 进入 u 前，&lt;a[u] 的节点数//      t0.add(a[u], 1); // 记录 u//      for (int v : g[u]) &#123;//          if (v != p) &#123;//              dfs(v, u); // 递归处理子节点//          &#125;//      &#125;//      int after = t0.query(a[u] - 1); // 当前 &lt;a[u] 的节点数（包括子树）//      res += (after - before); // 新增的 &lt;a[u] 的节点（即 u 的子节点中满足条件的）//      t0.add(a[u], -1); // 回溯//  &#125;    public static void main(String[] args) &#123;        Scanner scan=new Scanner(System.in);        n=scan.nextInt();        s=scan.nextInt();        a=new int[n+1];        g=new ArrayList[n+1];        tr=new int[n+1];        Arrays.fill(tr,0);        for(int i=1;i&lt;=n;i++) &#123;            a[i]=scan.nextInt();            g[i]=new ArrayList&lt;&gt;();        &#125;        for(int i=0;i&lt;n-1;i++) &#123;            int u=scan.nextInt();            int v=scan.nextInt();            g[u].add(v);            g[v].add(u);        &#125;        dfs(s,-1);        System.out.println(res);    &#125;&#125;
楼兰壁画
树状数组
import java.util.*;import java.io.*;class Tree &#123;    int[] tree;    int N;    public Tree(int N) &#123;        this.N = N;        tree = new int[N + 1];    &#125;    public int lowBit(int i) &#123;        return i &amp; -i;    &#125;    public void add(int i, int val) &#123;        while (i &lt;= N) &#123;            tree[i] += val;            i += lowBit(i);        &#125;    &#125;    public int query(int i) &#123;        int res = 0;        while (i &gt; 0) &#123;            res += tree[i];            i -= lowBit(i);        &#125;        return res;    &#125;    public int sum(int i) &#123;        return query(N) - query(i);    &#125;&#125;public class Main &#123;    static int N = (int) 2e5 + 10;    static int n;    static int[] a = new int[N];    static int[] up = new int[N];    static int[] down = new int[N];    public static void main(String[] args) &#123;        Scanner sc = new Scanner(System.in);        n = sc.nextInt();        for (int i = 1; i &lt;= n; i++) &#123;            a[i] = sc.nextInt();        &#125;        Tree tree0 = new Tree(N);        for (int i = 1; i &lt;= n; i++) &#123;            int y = a[i];            up[i] = tree0.sum(y); // 右边比当前元素大的数的个数            down[i] = tree0.query(y - 1); // 左边比当前元素小的数的个数            tree0.add(y, 1);        &#125;        Arrays.fill(tree0.tree, 0); // 清空树状数组        long res1 = 0, res2 = 0;        for (int i = n; i &gt;= 1; i--) &#123;            int y = a[i];            res1 += up[i] * (long) tree0.sum(y); // 右边比当前元素大的数的个数乘以左边比当前元素大的数的个数            res2 += down[i] * (long) tree0.query(y - 1); // 左边比当前元素小的数的个数乘以右边比当前元素小的数的个数            tree0.add(y, 1);        &#125;        System.out.println(res1 + &quot; &quot; + res2);    &#125;&#125;
一个树状数组问题
import java.io.*;import java.util.*;class Tree&#123;    long[]tree;    int N;    public Tree(int N)&#123;        this.N=N;        tree=new long[N+1];    &#125;    public int lowBit(int i)&#123;        return i&amp;-i;    &#125;    public void add(int i,int v)&#123;        while(i&lt;=N)&#123;        tree[i]+=v;        i+=lowBit(i);        &#125;    &#125;    public long query(int i)&#123;        long res=0;        while(i&gt;0)&#123;        res+=tree[i];        i-=lowBit(i);        &#125;        return res;    &#125;    public long sum(int i)&#123;        return query(N)-query(i);    &#125;&#125;public class Main&#123;    static int N=(int)1e5+10;    static int []a=new int[N];    static int n,m;    public static void main(String[] args)&#123;        Scanner sc=new Scanner(System.in);        Tree tree0 = new Tree(N);        n=sc.nextInt();        m=sc.nextInt();        for(int i=1;i&lt;=n;i++)&#123;            a[i]=sc.nextInt();            tree0.add(i,a[i]-a[i-1]);        &#125;        for(int i=0;i&lt;m;i++)&#123;            String s=sc.next();            if(s.equals(&quot;Q&quot;))&#123;                int x=sc.nextInt();                System.out.println(tree0.query(x));            &#125;else&#123;                int l=sc.nextInt();                int r=sc.nextInt();                int d=sc.nextInt();                tree0.add(l,d);                tree0.add(r+1,-d);            &#125;        &#125;    &#125; &#125;]]></content>
      <categories>
        <category>program</category>
        <category>算法模板</category>
      </categories>
      <tags>
        <tag>树状数组</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度下降优化</title>
    <url>/posts/2d61ce16.html</url>
    <content><![CDATA[梯度下降优化
1、归一化 Normalization
1.1、归一化目的
梯度下降的原理和应用，我们已经在前面课程中进行了学习，大家仔细观察下图。
![]()
不同方向的陡峭度是不一样的，即不同维度的数值大小是不同。也就是说梯度下降的快慢是不同的：
如果维度多了，就是超平面（了解一下霍金所说的宇宙十一维空间），很难画出来了，感受一下下面这张图的空间维度情况。
![]()
如果拿多元线性回归举例的话，因为多元线性回归的损失函数 MSE 是凸函数，所以我们可以把损失函数看成是一个碗。然后下面的图就是从碗上方去俯瞰！哪里是损失最小的地方呢？当然对应的就是碗底的地方！所以下图碗中心的地方颜色较浅的区域就是损失函数最小的地方。
![]()
上面两张图都是进行梯度下降，你有没有发现，略有不同啊？两张图形都是鸟瞰图，左边的图形做了归一化处理，右边是没有做归一化的俯瞰图。
啥是归一化呢？请带着疑问跟我走~
我们先来说一下为什么没做归一化是右侧图示，举个例子假如我们客户数据信息，有两个维度，一个是用户的年龄，一个是用户的月收入，目标变量是快乐程度。
name
age
salary
happy
路博通
36
7000
100
马老师
42
20000
180
赵老师
22
30000
164
……
……
……
……
我们可以里面写出线性回归公式， y=theta_1x_1+theta_2x_2+by = \\theta\_1x\_1 + \\theta\_2x\_2 + by=theta_1x_1+theta_2x_2+b ，那么这样每一条样本不同维度对应的数量级不同，原因是每个维度对应的物理含义不同嘛，但是计算机能理解 36 和 7000 分别是年龄和收入吗？计算机只是拿到一堆数字而已。
我们把 x_1x\_1x_1 看成是年龄，x_2x\_2x_2 看成是收入， y 对应着快乐程度。机器学习就是在知道 X，y的情况下解方程组调整出最优解的过程。根据公式我们也可以发现 y 是两部分贡献之和，按常理来说，一开始并不知道两个部分谁更重要的情况下，可以想象为两部分对 y 的贡献是一样的即 theta_1x_1=theta_2x_2\\theta\_1x\_1 = \\theta\_2x\_2theta_1x_1=theta_2x_2 ，如果 x_1llx_2x\_1 \\ll x\_2x_1llx_2 ，那么最终 theta_1ggtheta_2\\theta\_1 \\gg \\theta\_2theta_1ggtheta_2 （远大于）。
这样是不是就比较好理解为什么之前右侧示图里为什么 theta_1&gt;theta_2\\theta\_1 &gt; \\theta\_2theta_1&gt;theta_2 ，看起来就是椭圆。再思考一下，梯度下降第 1 步的操作，是不是所有的维度 theta\\thetatheta 都是根据在期望 mu\\mumu 为 0 方差 sigma\\sigmasigma 为 1 的正太分布随机生成的，说白了就是一开始的 theta_1\\theta\_1theta_1 和 theta_2\\theta\_2theta_2 数值是差不多的。所以可以发现 theta_1\\theta\_1theta_1 从初始值到目标位置 theta_1target\\theta\_1^{target}theta_1target 的距离要远大于 theta_2\\theta\_2theta_2 从初始值到目标位置theta_2target\\theta\_2^{target}theta_2target。
因为 x_1llx_2x\_1 \\ll x\_2x_1llx_2，根据梯度公式 gj=(htheta(x)−y)x_jg_j= (h_{\\theta}(x) - y)x\_jgj​=(htheta​(x)−y)x_j ，得出 g_1llg_2g\_1 \\ll g\_2g_1llg_2。根据梯度下降公式：\\theta\_j^{n+1} = \\theta\_j^n - \\eta \* g\_j 可知，每次调整 theta_1\\theta\_1theta_1 的幅度 ll\\llll （远小于） theta_2\\theta\_2theta_2 的调整幅度。
总结一下 ，根据上面得到的两个结论 ，它俩之间是互相矛盾的 ，意味着最后 theta_2\\theta\_2theta_2 需要比 theta_1\\theta\_1theta_1 更少的迭代次数就可以收敛，而我们要最终求得最优解，就必须每个维度 theta\\thetatheta 都收敛才可以，所以会出现 theta_2\\theta\_2theta_2 等待 theta_1\\theta\_1theta_1 收敛的情况。讲到这里对应图大家应该可以理解为什么右图是先顺着 theta_2\\theta\_2theta_2 的坐标轴往下走再往右走的原因了吧。
结论:
归一化的一个目的是，使得梯度下降在不同维度 theta\\thetatheta 参数（不同数量级）上，可以步调一致协同的进行梯度下降。这就好比社会主义，一小部分人先富裕起来了，先富带后富，这需要一定的时间，先富的这批人等待其他的人富裕起来；但是，更好途经是实现共同富裕，最后每个人都不能落下， 优化的步伐是一致的。
![]()
经过归一化处理，收敛的速度，明显快了！
1.2、归一化本质
做归一化的目的是要实现**“共同富裕”**，而之所以梯度下降优化时不能达到步调一致的根本原因其实还是 x_1x\_1x_1 和 x_2x\_2x_2 的数量级不同。所以什么是归一化？
答案自然就出来了，就是把 x_1x\_1x_1 和 x_2x\_2x_2 的数量级统一，扩展一点说，如果有更多特征维度，就要把各个特征维度 x_1、x_2、……、x_nx\_1、x\_2、……、x\_nx_1、x_2、……、x_n 的数量级统一，来做到无量纲化。
1.3、最大值最小值归一化
也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间。转换函数如下：
X^\* = \\frac{X - X\_min}{X\_max -X\_min}
其实我们很容易发现使用最大值最小值归一化（min-max标准化）的时候，优点是一定可以把数值归一到 0 ~ 1 之间，缺点是如果有一个离群值（比如马云的财富），正如我们举的例子一样，会使得一个数值为 1，其它数值都几乎为 0，所以受离群值的影响比较大！
代码演示：
import numpy as npx_1 = np.random.randint(1,10,size = 10)x_2 = np.random.randint(100,300,size = 10)x = np.c_[x_1,x_2]print(&#x27;归一化之前的数据：&#x27;)display(x)x_ = (x - x.min(axis = 0)) / (x.max(axis = 0) - x.min(axis = 0))print(&#x27;归一化之后的数据：&#x27;)display(x_)
使用scikit-learn函数：
import numpy as npfrom sklearn.preprocessing import MinMaxScalerx_1 = np.random.randint(1,10,size = 10)x_2 = np.random.randint(100,300,size = 10)x = np.c_[x_1,x_2]print(&#x27;归一化之前的数据：&#x27;)display(x)min_max_scaler = MinMaxScaler()x_ = min_max_scaler.fit_transform(x)print(&#x27;归一化之后的数据：&#x27;)display(x_)
1.4、0-均值标准化
这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化，也叫做Z-score标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，转化函数为：
X^\* = \\frac{X - \\mu}{\\sigma}
其中μ为所有样本数据的均值，σ为所有样本数据的标准差。
mu=frac1nsumlimits_i=1nx_i\\mu = \\frac{1}{n}\\sum\\limits\_{i = 1}^nx\_imu=frac1nsumlimits_i=1nx_i
sigma=sqrtfrac1nsumlimits_i=1n(x_i−mu)2\\sigma = \\sqrt{\\frac{1}{n}\\sum\\limits\_{i = 1}^n(x\_i - \\mu)^2}sigma=sqrtfrac1nsumlimits_i=1n(x_i−mu)2
相对于最大值最小值归一化来说，因为标准归一化除以了标准差，而标准差的计算会考虑到所有样本数据，所以受到离群值的影响会小一些，这就是除以方差的好处！但是，0-均值标准化不一定会把数据缩放到 0 ~ 1 之间了。既然是0均值，也就意味着，有正有负！
import numpy as npx_1 = np.random.randint(1,10,size = 10)x_2 = np.random.randint(100,300,size = 10)x = np.c_[x_1,x_2]print(&#x27;归一化之前的数据：&#x27;)display(x)x_ = (x - x.mean(axis = 0)) / x.std(axis = 0)print(&#x27;归一化之后的数据：&#x27;)display(x_)
使用scikit-learn函数：
import numpy as npfrom sklearn.preprocessing import StandardScalerx_1 = np.random.randint(1,10,size = 10)x_2 = np.random.randint(100,300,size = 10)x = np.c_[x_1,x_2]print(&#x27;归一化之前的数据：&#x27;)display(x)standard_scaler = StandardScaler()x_ = standard_scaler.fit_transform(x)print(&#x27;归一化之后的数据：&#x27;)display(x_)
那为什么要减去均值呢？其实做均值归一化还有一个特殊的好处（对比最大值最小值归一化，全部是正数0~1），我们来看一下梯度下降的式子，你就会发现 alpha\\alphaalpha 是正数，不管 A 是正还是负（ A 就是 haty−y=h_theta(x)−y\\hat{y} - y = h\_{\\theta}(x) - yhaty−y=h_theta(x)−y），对于所有的维度 X，比如这里的 x_1x\_1x_1 和 x_2x\_2x_2 来说，alpha\\alphaalpha 乘上 A 都是一样的符号，那么每次迭代的时候 w_1t+1w\_1^{t+1}w_1t+1 和 w_2t+1w\_2^{t+1}w_2t+1 的更新幅度符号也必然是一样的，这样就会像下图有右侧所示：要想从 w_tw\_tw_t 更新到 w^\* 就必然要么 w_1w\_1w_1 和 w_2w\_2w_2 同时变大再同时变小，或者就 w_1w\_1w_1 和 w_2w\_2w_2 同时变小再同时变大。不能如图上所示蓝色的最优解路径，即 w_1w\_1w_1 变小的同时 w_2w\_2w_2 变大！
![]()
那我们如何才能做到让 w_1w\_1w_1 变小的时候 w_2w\_2w_2 变大呢？归其根本还是数据集 X 矩阵（经过min-max归一化）中的数据均为正数。所以如果我们可以让 x_1x\_1x_1 和 x_2x\_2x_2 它们符号不同，比如有正有负，其实就可以在做梯度下降的时候有更多的可能性去让更新尽可能沿着最优解路径去走。
结论：0-均值标准化处理数据之后，属性有正有负，可以让梯度下降沿着最优路径进行~
注意：
我们在做特征工程的时候，很多时候如果对训练集的数据进行了预处理，比如这里讲的归一化，那么未来对测试集的时候，和模型上线来新的数据的时候，都要进行相同的数据预处理流程，而且所使用的均值和方差是来自当时训练集的均值和方差!
因为我们人工智能要干的事情就是从训练集数据中找规律，然后利用找到的规律去预测新产生的数据。这也就是说假设训练集和测试集以及未来新来的数据是属于同分布的！从代码上面来说如何去使用训练集的均值和方差呢？就需要把 scaler 对象持久化， 回头模型上线的时候再加载进来去对新来的数据进行处理。
import joblibjoblib.dump(standard_scaler,&#x27;scale&#x27;) # 持久化standard_scaler = joblib.load(&#x27;scale&#x27;) # 加载standard_scaler.transform(x) # 使用
2、正则化 Regularization
2.1、过拟合欠拟合

欠拟合（under fit）：还没有拟合到位，训练集和测试集的准确率都还没有到达最高，学的还不到位。
过拟合（over fit）：拟合过度，训练集的准确率升高的同时，测试集的准确率反而降低。学的过度了（走火入魔），做过的卷子都能再次答对（死记硬背），考试碰到新的没见过的题就考不好（不会举一反三）。
恰到好处（just right）：过拟合前，训练集和测试集准确率都达到巅峰。好比，学习并不需要花费很多时间，理解的很好，考试的时候可以很好的把知识举一反三。

![]()
正则化就是防止过拟合，增加模型的鲁棒性，鲁棒是 Robust 的音译，也就是强壮的意思。就像计算机软件在面临攻击、网络过载等情况下能够不死机不崩溃，这就是软件的鲁棒性。鲁棒性调优就是让模型拥有更好的鲁棒性，也就是让模型的泛化能力和推广 能力更加的强大。
举例子说明：下面两个式子描述同一条直线那个更好？
y=0.3x_1+0.4x_2+0.5y = 0.3x\_1 + 0.4x\_2 + 0.5y=0.3x_1+0.4x_2+0.5
y=3x_1+4x_2+5y = 3x\_1 + 4x\_2 + 5y=3x_1+4x_2+5
第一个更好，因为下面的公式是上面的十倍，当 w 越小公式的容错的能力就越好。因为把测试数据带入公式中如果测试集原来是 [32, 128] 在带入的时候发生了一些偏差，比如说变成 [30, 120] ，第二个模型结果就会比第一个模型结果的偏差大的多。公式中 y=WTxy = W^Txy=WTx ，当 x 有一点错误，这个错误会通过 w 放大。但是 w 不能太小，当 w 太小时（比如都趋近0），模型就没有意义了，无法应用。想要有一定的容错率又要保证正确率就要由正则项来发挥作用了！
所以正则化(鲁棒性调优)的本质就是牺牲模型在训练集上的正确率来提高推广、泛化能力， W 在数值上越小越好，这样能抵抗数值的扰动。同时为了保证模型的正确率 W 又不能极小。 故而人们将原来的损失函数加上一个惩罚项，这里面损失函数就是原来固有的损失函数，比如回归的话通常是 MSE，分类的话通常是 cross entropy 交叉熵，然后在加上一部分惩罚项来使得计算出来的模型 W 相对小一些来带来泛化能力。
常用的惩罚项有L1 正则项或者 L2 正则项：

L_1=w1=sumlimitsi=1nw_iL\_1 = w_1 = \\sum\\limits_{i = 1}^nw\_iL_1=w1​=sumlimitsi=1n​w_i​ 对应曼哈顿距离
L_2=w2=sqrtsumlimitsi=1n(w_i)2L\_2 = w_2 = \\sqrt{\\sum\\limits_{i = 1}^n(w\_i)^2}L_2=w2​=sqrtsumlimitsi=1n​(w_i)2 对应欧氏距离

其实 L1 和 L2 正则的公式数学里面的意义就是范数，代表空间中向量到原点的距离：
L\_p = X_p = \\sqrt\[p\]{\\sum\\limits_{i = 1}^nx\_i^p} , X = (x\_1,x\_2,……x\_n)
![]()
当我们把多元线性回归损失函数加上 L2 正则的时候，就诞生了 Ridge 岭回归。当我们把多元线性回归损失函数加上 L1 正则的时候，就孕育出来了 Lasso 回归。其实 L1 和 L2 正则项惩罚项可以加到任何算法的损失函数上面去提高计算出来模型的泛化能力的。
2.2、套索回归（Lasso）
先从线性回归开始，其损失函数如下：
J(theta)=frac12sumlimitsi=1n(htheta(x(i))−y(i))2J(\\theta) = \\frac{1}{2}\\sum\\limits_{i = 1}^n(h_{\\theta}(x^{(i)}) - y^{(i)})^2J(theta)=frac12sumlimitsi=1n​(htheta​(x(i))−y(i))2
L1正则化的损失函数，令J_0=J(theta)J\_0 = J(\\theta)J_0=J(theta)：
J = J_0 + \\alpha \* \\sum\\limits_{i = 1}^nw\_i
令 L_1 = \\alpha \* \\sum\\limits_{i = 1}^nw\_i ：
J=J_0+L_1J = J\_0 + L\_1J=J_0+L_1
其中 J_0J\_0J_0 是原始的损失函数，加号后面的一项是L1正则化项， alpha\\alphaalpha 是正则化系数。注意到 L1正则化是权值的绝对值之和。JJJ 是带有绝对值符号的函数，因此 JJJ 是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数 J_0J\_0J_0 后面添加L1正则项时，相当于对 J_0J\_0J_0 做了一个约束。令L_1 = \\alpha \* \\sum\\limits_{i = 1}^nw\_i ，则 J=J_0+L_1J = J\_0 + L\_1J=J_0+L_1 ，此时我们的任务变成在 L_1L\_1L_1 约束下求出 J_0J\_0J_0 取最小值的解。考虑二维的情况，即只有两个权值 w_1、w_2w\_1、w\_2w_1、w_2 ，此时 L_1=w_1+w_2L\_1 = w\_1 + w\_2L_1=w_1+w_2。 对于梯度下降法，求解 J_0J\_0J_0 过程可以画出等值线，同时 L1 正则化的函数 L_1L\_1L_1 也可以在 w_1、w_2w\_1、w\_2w_1、w_2所在的平面上画出来：
![]()
图中等值线是J_0J\_0J_0的等值线，是椭圆形。黑色方框是 L_1L\_1L_1 函数的图形，L_1=w_1+w_2L\_1 = w\_1 + w\_2L_1=w_1+w_2 这个函数画出来，就是一个方框。
在图中，当 J_0J\_0J_0 等值线与 L_1L\_1L_1 图形首次相交的地方就是最优解。上图中 J_0J\_0J_0 与 L_1L\_1L_1 在 L_1L\_1L_1 的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是 (w_1,w_2)=(0,w)(w\_1,w\_2) = (0,w)(w_1,w_2)=(0,w) 。可以直观想象，因为 L_1L\_1L_1 函数有很多『突出的角』（二维情况下四个，多维情况下更多）， J_0J\_0J_0 与这些角接触的机率会远大于与 L_1L\_1L_1 其它部位接触的机率（这是很直觉的想象，突出的角比直线的边离等值线更近写），而在这些角上，会有很多权值等于0（因为角就在坐标轴上），这就是为什么 L1 正则化可以产生稀疏模型（很多权重等于0了），进而可以用于特征选择。
而正则化前面的系数 alpha\\alphaalpha，可以控制 L_1L\_1L_1 图形的大小。alpha\\alphaalpha 越小，L_1L\_1L_1 的图形越大（上图中的黑色方框）；alpha\\alphaalpha 越大，L_1L\_1L_1 的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优解的值(w_1,w_2)=(0,w)(w\_1,w\_2) = (0,w)(w_1,w_2)=(0,w) 中的 w 可以取到很小的值的原因所在。
代码演示 alpha\\alphaalpha 取值大小对黑色方框的尺寸影响：
import matplotlib.pyplot as plt# α 的值是：1# 1 = x + y# y = 1 -xf = lambda x : 1- xx = np.linspace(0,1,100)plt.axis(&#x27;equal&#x27;)plt.plot(x, f(x), color = &#x27;green&#x27;)# α 的值是：3# 1 = 3 * x + 3 * y# y = 1/3 -xf2 = lambda x : 1/3 - x x2 = np.linspace(0,1/3,100)plt.plot(x2, f2(x2),color = &#x27;red&#x27;)# 一些列设置plt.xlim(-2,2)plt.ylim(-2,2)ax = plt.gca()ax.spines[&#x27;right&#x27;].set_color(&#x27;None&#x27;)  # 将图片的右框隐藏ax.spines[&#x27;top&#x27;].set_color(&#x27;None&#x27;)  # 将图片的上边框隐藏ax.spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;, 0)) # x轴出现在y轴的-1 位置ax.spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;, 0))plt.savefig(&#x27;&#x27;,dpi = 200)
![]()
权重更新规则如下：

损失函数：

J(theta)=frac12sumlimitsi=1n(htheta(x(i))−y(i))2J(\\theta) = \\frac{1}{2}\\sum\\limits_{i = 1}^n(h_{\\theta}(x^{(i)}) - y^{(i)})^2J(theta)=frac12sumlimitsi=1n​(htheta​(x(i))−y(i))2
L_1  =  \\alpha \* \\sum\\limits_{i = 1}^nw\_i
J=J_0+L_1J = J\_0 + L\_1J=J_0+L_1

更新规则：

\\theta\_j^{n + 1} = \\theta\_j^{n} - \\eta \* \\frac{\\partial}{\\partial \\theta\_j}J
\\theta\_j^{n + 1} = \\theta\_j^{n} - \\eta \* \\frac{\\partial}{\\partial \\theta\_j}(J\_0 + L\_1)
fracpartialpartialtheta_jJ0=sumlimitsi=1n(h_theta(x(i))−y(i))x_j(i) \\frac{\\partial}{\\partial \\theta\_j}J_0 =\\sum\\limits_{i = 1}^{n} (h\_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)} fracpartialpartialtheta_jJ0​=sumlimitsi=1n​(h_theta(x(i))−y(i))x_j(i) 
\\frac{\\partial}{\\partial \\theta\_j}L\_1 = \\alpha \* sgn(w\_i) 
其中 J_0J\_0J_0 即是线性回归的损失函数，L_1L\_1L_1 是添加的正则项。sgn(w_i)sgn(w\_i)sgn(w_i) 表示符号函数、指示函数，值为：1 或 -1。
sgn(w\_i) = \\begin{cases}1, &w\_i > 0\\-1,&w\_i < 0\\end{cases}
注意当 w_i=0w\_i = 0w_i=0 时不可导。
综上所述，L1正则化权重更新如下：
\\theta\_j^{n + 1} = \\theta_j^{n} -\\eta\\sum\\limits_{i = 1}^{n} (h\_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)} - \\eta_\\alpha_ sgn(w\_i)

Lasso回归和线性回归相比，多了一项：-\\eta _\\alpha_ sgn(w\_i)
$\eta $ 大于零，表示梯度下降学习率
alpha\\alphaalpha 大于零，表示L1正则化系数
当w_iw\_iw_i为正时候 sgn(w_i)=1sgn(w\_i) = 1sgn(w_i)=1，直接减去 \\eta \* \\alpha （大于0），所以正的 w_iw\_iw_i 变小了
当w_iw\_iw_i为负时候 sgn(w_i)=−1sgn(w\_i) = -1sgn(w_i)=−1，相当于直接加上 \\eta \* \\alpha （小于0），所以负的 w_iw\_iw_i​ 变大了，绝对值变小，向0靠近

有的书本上公式会这样写，其中 lambda\\lambdalambda 表示L1正则化系数：
\\theta\_j^{n + 1} = \\theta_j^{n} -\\eta\\sum\\limits_{i = 1}^{n} (h\_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)} - \\eta_\\lambda_ sgn(w\_i)
2.3、岭回归（Ridge）
也是先从线性回归开始，其损失函数如下：
J(theta)=frac12sumlimitsi=1n(htheta(x(i))−y(i))2J(\\theta) = \\frac{1}{2}\\sum\\limits_{i = 1}^n(h_{\\theta}(x^{(i)}) - y^{(i)})^2J(theta)=frac12sumlimitsi=1n​(htheta​(x(i))−y(i))2
L2正则化的损失函数（对L2范数，进行了平方运算），令J_0=J(theta)J\_0 = J(\\theta)J_0=J(theta)：
J = J_0 + \\alpha \* \\sum\\limits_{i = 1}^n(w\_i)^2
令 L_2 = \\alpha \* \\sum\\limits_{i = 1}^n(w\_i)^2 ：
J=J_0+L_2J = J\_0 + L\_2J=J_0+L_2
同样可以画出他们在二维平面上的图形，如下：
![]()
二维平面下 L2 正则化的函数图形是个圆（绝对值的平方和，是个圆），与方形相比，被磨去了棱角。因此 J_0J\_0J_0 与 L_2L\_2L_2 相交时使得 w_1、w_2w\_1、w\_2w_1、w_2 等于零的机率小了许多（这个也是一个很直观的想象），这就是为什么L2正则化不具有稀疏性的原因，因为不太可能出现多数 w 都为0的情况（这种情况就叫稀疏性）！
权重更新规则如下：

损失函数：

J(theta)=frac12sumlimitsi=1n(htheta(x(i))−y(i))2J(\\theta) = \\frac{1}{2}\\sum\\limits_{i = 1}^n(h_{\\theta}(x^{(i)}) - y^{(i)})^2J(theta)=frac12sumlimitsi=1n​(htheta​(x(i))−y(i))2
L_2  =  \\alpha \* \\sum\\limits_{i = 1}^n(w\_i)^2
J=J_0+L_2J = J\_0 + L\_2J=J_0+L_2

更新规则：

\\theta\_j^{n + 1} = \\theta\_j^{n} - \\eta \* \\frac{\\partial}{\\partial \\theta\_j}J
\\theta\_j^{n + 1} = \\theta\_j^{n} - \\eta \* \\frac{\\partial}{\\partial \\theta\_j}(J\_0 + L\_2)
$\frac{\partial}{\partial \theta_j}J_0 =\sum\limits_{i = 1}^{n} (h_{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)} $
fracpartialpartialtheta_jL_2=2alphaw_i \\frac{\\partial}{\\partial \\theta\_j}L\_2 = 2\\alpha w\_i fracpartialpartialtheta_jL_2=2alphaw_i 
其中 J_0J\_0J_0 即是线性回归的损失函数，L_2L\_2L_2 是添加的正则项。
综上所述，L2正则化权重更新如下（2alpha2\\alpha2alpha 也是常数项，可以合并到一起用整体 alpha\\alphaalpha 替代）：
\\theta\_j^{n + 1} = \\theta_j^{n}(1-\\eta _\\alpha) -\\eta_ \\sum\\limits_{i = 1}^{n} (h\_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)}
其中 alpha\\alphaalpha 就是正则化参数，eta\\etaeta 表示学习率。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代， theta_j\\theta\_jtheta_j 都要先乘以一个小于1的因子（即 (1-\\eta \* \\alpha) ），从而使得 theta_j\\theta\_jtheta_j 加速减小，因此总的来看，theta\\thetatheta 相比不加L2正则项的线性回归可以获得更小的值。从而，实现了防止过拟合的效果，增加模型的鲁棒性~
有的书本上，公式写法可能不同：其中 lambda\\lambdalambda 表示正则化参数。
\\theta\_j^{n + 1} = \\theta_j^{n}(1-\\eta _\\lambda) -\\eta_ \\sum\\limits_{i = 1}^{n} (h\_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)}
3、线性回归衍生算法
接下来，我们一起学习一下scikit-learn中为我们提供的线性回归衍生算法，根据上面所学的原理，对比线性回归加深理解。
3.1、Ridge算法使用
这是scikit-learn官网给出的岭回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。
undersetwminXw−y_22+alphaw_22\\underset{w}\\min X w - y\_2^2 + \\alpha w\_2^2undersetwminXw−y_22+alphaw_22
L2正则化和普通线性回归系数对比：
import numpy as npfrom sklearn.linear_model import Ridgefrom sklearn.linear_model import SGDRegressor# 1、创建数据集X，yX = 2*np.random.rand(100, 5)w = np.random.randint(1,10,size = (5,1))b = np.random.randint(1,10,size = 1)y = X.dot(w) + b + np.random.randn(100, 1)print(&#x27;原始方程的斜率：&#x27;,w.ravel())print(&#x27;原始方程的截距：&#x27;,b)ridge = Ridge(alpha= 1, solver=&#x27;sag&#x27;)ridge.fit(X, y)print(&#x27;岭回归求解的斜率：&#x27;,ridge.coef_)print(&#x27;岭回归求解的截距：&#x27;,ridge.intercept_)# 线性回归梯度下降方法sgd = SGDRegressor(penalty=&#x27;l2&#x27;,alpha=0,l1_ratio=0)sgd.fit(X, y.reshape(-1,))print(&#x27;随机梯度下降求解的斜率是：&#x27;,sgd.coef_)print(&#x27;随机梯度下降求解的截距是：&#x27;,sgd.intercept_)
结论：

和没有正则项约束线性回归对比，可知L2正则化，将方程系数进行了缩小
alpha\\alphaalpha 增大求解出来的方程斜率变小
Ridge回归源码解析：

alpha：正则项系数
fit_intercept：是否计算 w_0w\_0w_0 截距项
normalize：是否做归一化
max_iter：最大迭代次数
tol：结果的精确度
solver：优化算法的选择



3.2、Lasso算法使用
这是scikit-learn官网给出的套索回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。
undersetwminfrac12n_textsamplesXw−y_22+alphaw_1\\underset{w}\\min { \\frac{1}{2n\_{\\text{samples}}} X w - y\_2 ^ 2 + \\alpha w\_1}undersetwminfrac12n_textsamplesXw−y_22+alphaw_1
公式中多了一项：frac12n_samples\\frac{1}{2n\_{samples}}frac12n_samples这是一个常数项，去掉之后，也不会影响损失函数公式计算。在岭回归中，就没有这项。
L1正则化和普通线性回归系数对比：
import numpy as npfrom sklearn.linear_model import Lassofrom sklearn.linear_model import SGDRegressor# 1、创建数据集X，yX = 2*np.random.rand(100, 20)w = np.random.randn(20,1)b = np.random.randint(1,10,size = 1)y = X.dot(w) + b + np.random.randn(100, 1)print(&#x27;原始方程的斜率：&#x27;,w.ravel())print(&#x27;原始方程的截距：&#x27;,b)lasso = Lasso(alpha= 0.5)lasso.fit(X, y)print(&#x27;套索回归求解的斜率：&#x27;,lasso.coef_)print(&#x27;套索回归求解的截距：&#x27;,lasso.intercept_)# 线性回归梯度下降方法sgd = SGDRegressor(penalty=&#x27;l2&#x27;,alpha=0, l1_ratio=0)sgd.fit(X, y.reshape(-1,))print(&#x27;随机梯度下降求解的斜率是：&#x27;,sgd.coef_)print(&#x27;随机梯度下降求解的截距是：&#x27;,sgd.intercept_)
结论：

和没有正则项约束线性回归对比，可知L1正则化，将方程系数进行了缩减，部分系数为0，产生稀疏模型
alpha\\alphaalpha 越大，模型稀疏性越强，越多的参数为0
Lasso回归源码解析：

alpha：正则项系数
fit_intercept：是否计算 w_0w\_0w_0 截距项
normalize：是否做归一化
precompute：bool 类型，默认值为False，决定是否提前计算Gram矩阵来加速计算
max_iter：最大迭代次数
tol：结果的精确度
warm_start：bool类型，默认值为False。如果为True，那么使⽤用前⼀次训练结果继续训练。否则从头开始训练



3.3、Elastic-Net算法使用
这是scikit-learn官网给出的弹性网络回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。
undersetwminfrac12n_textsamplesXw−y_22+alpharhow_1+fracalpha(1−rho)2w_22\\underset{w}\\min { \\frac{1}{2n\_{\\text{samples}}} X w - y\_2 ^ 2 + \\alpha \\rho w\_1 + \\frac{\\alpha(1-\\rho)}{2} w\_2 ^ 2}undersetwminfrac12n_textsamplesXw−y_22+alpharhow_1+fracalpha(1−rho)2w_22
Elastic-Net 回归，即岭回归和Lasso技术的混合。弹性网络是一种使用 L1， L2 范数作为先验正则项训练的线性回归模型。 这种组合允许学习到一个只有少量参数是非零稀疏的模型，就像 Lasso 一样，但是它仍然保持一些像 Ridge 的正则性质。我们可利用 l1_ratio 参数控制 L1 和 L2 的凸组合。
弹性网络在很多特征互相联系（相关性，比如身高和体重就很有关系）的情况下是非常有用的。Lasso 很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。
在实践中，Lasso 和 Ridge 之间权衡的一个优势是它允许在迭代过程中继承 Ridge 的稳定性。
弹性网络回归和普通线性回归系数对比：
import numpy as npfrom sklearn.linear_model import ElasticNetfrom sklearn.linear_model import SGDRegressor# 1、创建数据集X，yX = 2*np.random.rand(100, 20)w = np.random.randn(20,1)b = np.random.randint(1,10,size = 1)y = X.dot(w) + b + np.random.randn(100, 1)print(&#x27;原始方程的斜率：&#x27;,w.ravel())print(&#x27;原始方程的截距：&#x27;,b)model = ElasticNet(alpha= 1, l1_ratio = 0.7)model.fit(X, y)print(&#x27;弹性网络回归求解的斜率：&#x27;,model.coef_)print(&#x27;弹性网络回归求解的截距：&#x27;,model.intercept_)# 线性回归梯度下降方法sgd = SGDRegressor(penalty=&#x27;l2&#x27;,alpha=0, l1_ratio=0)sgd.fit(X, y.reshape(-1,))print(&#x27;随机梯度下降求解的斜率是：&#x27;,sgd.coef_)print(&#x27;随机梯度下降求解的截距是：&#x27;,sgd.intercept_)
结论：

和没有正则项约束线性回归对比，可知Elastic-Net网络模型，融合了L1正则化L2正则化
Elastic-Net 回归源码解析：

alpha：混合惩罚项的常数
l1_ratio：弹性网混合参数，0 &lt;= l1_ratio &lt;= 1，对于 l1_ratio = 0，惩罚项是L2正则惩罚。对于 l1_ratio = 1是L1正则惩罚。对于 0
fit_intercept：是否计算 w_0w\_0w_0 截距项
normalize：是否做归一化
precompute：bool 类型，默认值为False，决定是否提前计算Gram矩阵来加速计算
max_iter：最大迭代次数
tol：结果的精确度
warm_start：bool类型，默认值为False。如果为True，那么使⽤用前⼀次训练结果继续训练。否则从头开始训练



4、多项式回归
4.1、多项式回归基本概念
升维的目的是为了去解决欠拟合的问题的，也就是为了提高模型的准确率为目的的，因为当维度不够时，说白了就是对于预测结果考虑的因素少的话，肯定不能准确的计算出模型。
![]()
在做升维的时候，最常见的手段就是将已知维度进行相乘（或者自乘）来构建新的维度，如下图所示。普通线性方程，无法拟合规律，必须是多项式，才可以完美拟合曲线规律，图中是二次多项式。
![]()
对于多项式回归来说主要是为了扩展线性回归算法来适应更广泛的数据集，比如我们数据集有两个维度 x_1、x_2x\_1、x\_2x_1、x_2，那么用多元线性回归公式就是：haty=w_0+w_1x_1+w_2x_2\\hat{y} = w\_0 + w\_1x\_1 + w\_2x\_2haty=w_0+w_1x_1+w_2x_2，当我们使用二阶多项式升维的时候，数据集就从原来的 x_1、x_2x\_1、x\_2x_1、x_2扩展成了x_1、x_2、x_12、x_22、x_1x_2x\_1、x\_2、x\_1^2、x\_2^2、x\_1x\_2x_1、x_2、x_12、x_22、x_1x_2 。因此多元线性回归就得去多计算三个维度所对应的w值：haty=w_0+w_1x_1+w_2x_2+w_3x_12+w_4x_22+w_5x_1x_2\\hat{y} = w\_0 + w\_1x\_1 + w\_2x\_2 + w\_3x\_1^2 + w\_4x\_2^2 + w\_5x\_1x\_2haty=w_0+w_1x_1+w_2x_2+w_3x_12+w_4x_22+w_5x_1x_2 。
此时拟合出来的方程就是曲线，可以解决一些线性回归的欠拟合问题！
4.2、多项式回归实战1.0
import numpy as npimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegression# 1、创建数据，并进行可视化X = np.linspace(-1,11,num = 100)y = (X - 5)**2 + 3*X -12 + np.random.randn(100)X = X.reshape(-1,1)plt.scatter(X,y)# 2、创建预测数据X_test = np.linspace(-2,12,num = 200).reshape(-1,1)# 3、不进行升维 + 普通线性回归model_1 = LinearRegression()model_1.fit(X,y)y_test_1 = model_1.predict(X_test)plt.plot(X_test,y_test,color = &#x27;red&#x27;)# 4、多项式升维 + 普通线性回归X = np.concatenate([X,X**2],axis = 1)model_2 = LinearRegression()model_2.fit(X,y)# 5、测试数据处理，并预测X_test = np.concatenate([X_test,X_test**2],axis = 1)y_test_2 = model_2.predict(X_test)# 6、数据可视化，切片操作plt.plot(X_test[:,0],y_test_2,color = &#x27;green&#x27;)
结论：

不进行多项式升维，拟合出来的曲线，是线性的直线，和目标曲线无法匹配
使用np.concatenate()进行简单的，幂次合并，注意数据合并的方向axis = 1
数据可视化时，注意切片，因为数据升维后，多了平方这一维

![]()
4.3、多项式回归实战2.0
import numpy as npimport matplotlib.pyplot as pltfrom sklearn.preprocessing import PolynomialFeatures,StandardScalerfrom sklearn.linear_model import SGDRegressor# 1、创建数据，并进行可视化X = np.linspace(-1,11,num = 100)y = (X - 5)**2 + 3*X -12 + np.random.randn(100)X = X.reshape(-1,1)plt.scatter(X,y)# 2、创建预测数据X_test = np.linspace(-2,12,num = 200).reshape(-1,1)# 3、使用PolynomialFeatures进行特征升维poly = PolynomialFeatures()poly.fit(X,y)X = poly.transform(X)s = StandardScaler()X = s.fit_transform(X)# model = SGDRegressor(penalty=&#x27;l2&#x27;,eta0 = 0.0001,max_iter = 10000)model = SGDRegressor(penalty=&#x27;l2&#x27;,eta0 = 0.01)model.fit(X,y)# 4、预测数据X_test = poly.transform(X_test)X_test_norm = s.transform(X_test)y_test = model.predict(X_test_norm)plt.plot(X_test[:,1],y_test,color = &#x27;green&#x27;)
结论：

eta0表示学习率，设置合适的学习率，才能拟合成功
多项式升维，需要对数据进行Z-score归一化处理，效果更佳出色
SGD随机梯度下降需要调整参数，以使模型适应数据

5、代码实战天猫双十一销量预测
天猫双十一，从2009年开始举办，第一届成交额仅仅0.5亿，后面呈现了爆发式的增长，那么这些增长是否有规律呢？是怎么样的规律，该如何分析呢？我们使用多项式回归一探究竟！
![]()
数据可视化，历年天猫双十一销量数据：
import numpy as npfrom sklearn.linear_model import SGDRegressorimport matplotlib.pyplot as pltplt.rcParams[&#x27;font.size&#x27;] = 18plt.figure(figsize=(9,6))# 创建数据，年份数据2009 ~ 2019X = np.arange(2009,2020)y = np.array([0.5,9.36,52,191,350,571,912,1207,1682,2135,2684])plt.bar(X,y,width = 0.5,color = &#x27;green&#x27;)plt.plot(X,y,color = &#x27;red&#x27;)_ = plt.xticks(ticks = X)
![]()
有图可知，在一定时间内，随着经济的发展，天猫双十一销量与年份的关系是多项式关系！假定，销量和年份之间关系是三次幂关系：
f(x)=w_1x+w_2x2+w_3x3+bf(x) = w\_1x + w\_2x^2 + w\_3x^3 + bf(x)=w_1x+w_2x2+w_3x3+b
import numpy as npfrom sklearn.linear_model import SGDRegressorimport matplotlib.pyplot as pltfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.preprocessing import StandardScalerplt.figure(figsize=(12,9))# 1、创建数据，年份数据2009 ~ 2019X = np.arange(2009,2020)y = np.array([0.5,9.36,52,191,350,571,912,1207,1682,2135,2684])# 2、年份数据，均值移除，防止某一个特征列数据天然的数值太大而影响结果X = X - X.mean()X = X.reshape(-1,1)# 3、构建多项式特征，3次幂poly = PolynomialFeatures(degree=3)X = poly.fit_transform(X)s = StandardScaler()X_norm = s.fit_transform(X)# 4、创建模型model = SGDRegressor(penalty=&#x27;l2&#x27;,eta0 = 0.5,max_iter = 5000)model.fit(X_norm,y)# 5、数据预测X_test = np.linspace(-5,6,100).reshape(-1,1)X_test = poly.transform(X_test)X_test_norm = s.transform(X_test)y_test = model.predict(X_test_norm)# 6、数据可视化plt.plot(X_test[:,1],y_test,color = &#x27;green&#x27;)plt.bar(X[:,1],y)plt.bar(6,y_test[-1],color = &#x27;red&#x27;)plt.ylim(0,4096)plt.text(6,y_test[-1] + 100,round(y_test[-1],1),ha = &#x27;center&#x27;)_ = plt.xticks(np.arange(-5,7),np.arange(2009,2021))
![]()
结论：

数据预处理，均值移除。如果特征基准值和分散度不同在某些算法（例如回归算法，KNN等）上可能会大大影响了模型的预测能力。通过均值移除，大大增强数据的离散化程度。
多项式升维，需要对数据进行Z-score归一化处理，效果更佳出色
SGD随机梯度下降需要调整参数，以使模型适应多项式数据
从2020年开始，天猫双十一统计的成交额改变了规则为11.1日~11.11日的成交数据（之前的数据为双十一当天的数据），2020年成交额为4980亿元
可以，经济发展有其客观规律，前11年高速发展（曲线基本可以反应销售规律），到2020年是一个转折点

6、代码实战中国人寿保费预测
6.1、数据加载与介绍
import numpy as npimport pandas as pddata = pd.read_excel(&#x27;./中国人寿.xlsx&#x27;)print(data.shape)data.head()
数据介绍：

共计1338条保险数据，每条数据7个属性
最后一列charges是保费
前面6列是特征，分别为：年龄、性别、体重指数、小孩数量、是否抽烟、所在地区

6.2、EDA数据探索
import seaborn as sns# 性别对保费影响sns.kdeplot(data[&#x27;charges&#x27;],shade = True,hue = data[&#x27;sex&#x27;])# 地区对保费影响sns.kdeplot(data[&#x27;charges&#x27;],shade = True,hue = data[&#x27;region&#x27;])# 吸烟对保费影响sns.kdeplot(data[&#x27;charges&#x27;],shade = True,hue = data[&#x27;smoker&#x27;])# 孩子数量对保费影响sns.kdeplot(data[&#x27;charges&#x27;],shade = True,hue = data[&#x27;children&#x27;],palette=&#x27;Set1&#x27;)
总结：

不同性别对保费影响不大，不同性别的保费的概率分布曲线基本重合，因此这个特征无足轻重，可以删除
地区同理
吸烟与否对保费的概率分布曲线差别很大，整体来说不吸烟更加健康，那么保费就低，这个特征很重要
家庭孩子数量对保费有一定影响

6.3、特征工程
data = data.drop([&#x27;region&#x27;, &#x27;sex&#x27;], axis=1)data.head() # 删除不重要特征# 体重指数，离散化转换，体重两种情况：标准、肥胖def convert(df,bmi):    df[&#x27;bmi&#x27;] = &#x27;fat&#x27; if df[&#x27;bmi&#x27;] &gt;= bmi else &#x27;standard&#x27;    return dfdata = data.apply(convert, axis = 1, args=(30,))data.head()# 特征提取，离散型数据转换为数值型数据data = pd.get_dummies(data)data.head()# 特征和目标值抽取X = data.drop(&#x27;charges&#x27;, axis=1) # 训练数据y = data[&#x27;charges&#x27;] # 目标值X.head()
6.4、特征升维
from sklearn.linear_model import LinearRegressionfrom sklearn.linear_model import ElasticNetfrom sklearn.metrics import mean_squared_error,mean_squared_log_error# 数据拆分from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import PolynomialFeaturesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)# 特征升维poly = PolynomialFeatures(degree= 2, include_bias = False)X_train_poly = poly.fit_transform(X_train)X_test_poly = poly.fit_transform(X_test)
6.5、模型训练与评估
普通线性回归：
model_1 = LinearRegression()model_1.fit(X_train_poly, y_train)print(&#x27;测试数据得分：&#x27;,model_1.score(X_train_poly,y_train))print(&#x27;预测数据得分：&#x27;,model_1.score(X_test_poly,y_test))print(&#x27;训练数据均方误差：&#x27;,np.sqrt(mean_squared_error(y_train,model_1.predict(X_train_poly))))print(&#x27;测试数据均方误差：&#x27;,np.sqrt(mean_squared_error(y_test,model_1.predict(X_test_poly))))print(&#x27;训练数据对数误差：&#x27;,np.sqrt(mean_squared_log_error(y_train,model_1.predict(X_train_poly))))print(&#x27;测试数据对数误差：&#x27;,np.sqrt(mean_squared_log_error(y_test,model_1.predict(X_test_poly))))
弹性网络回归：
model_2 = ElasticNet(alpha = 0.3,l1_ratio = 0.5,max_iter = 50000)model_2.fit(X_train_poly,y_train)print(&#x27;测试数据得分：&#x27;,model_2.score(X_train_poly,y_train))print(&#x27;预测数据得分：&#x27;,model_2.score(X_test_poly,y_test))print(&#x27;训练数据均方误差为：&#x27;,np.sqrt(mean_squared_error(y_train,model_2.predict(X_train_poly))))print(&#x27;测试数据均方误差为：&#x27;,np.sqrt(mean_squared_error(y_test,model_2.predict(X_test_poly))))print(&#x27;训练数据对数误差为：&#x27;,np.sqrt(mean_squared_log_error(y_train,model_2.predict(X_train_poly))))print(&#x27;测试数据对数误差为：&#x27;,np.sqrt(mean_squared_log_error(y_test,model_2.predict(X_test_poly))))
结论：

进行EDA数据探索，可以查看无关紧要特征
进行特征工程：删除无用特征、特征离散化、特征提取。这对机器学习都至关重要
对于简单的数据（特征比较少）进行线性回归，一般需要进行特征升维
选择不同的算法，进行训练和评估，从中筛选优秀算法

]]></content>
      <categories>
        <category>program</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>蓝桥杯-高塔</title>
    <url>/posts/a38c4cc3.html</url>
    <content><![CDATA[问题描述 小蓝正在玩一个攀登高塔的游戏。高塔的层数是无限的，但游戏最多只有 n n 回合。
小蓝一开始拥有 m m 点能量，在每个回合都有一个值 A i A i ​ 表示小蓝的角色状态。小蓝每回合可以选择消费任意点能量 C i C i ​ （最低消费 1 1 点，没有上限），他在这回合将最多可以向上攀爬 A i ⋅ C i A i ​ ⋅C i ​ 层。实际攀爬的层数取决于小蓝自己在这回合的表现，不过最差也会向上爬一层。
当某回合小蓝的能量点数耗尽，那么在完成这个回合后，游戏结束。 n n 回合结束后，不管能量还有没有剩余，游戏都会直接结束。
给出小蓝每回合的 A i A i ​ 和自己一开始的能量点数 m m。小蓝想知道有多少种不同的可能出现的游玩过程。如果小蓝在两种游玩过程中的任一对应回合花费的能量点数不同或该回合结束时所处层数不同，那么这两种游玩过程就被视为不同。
输入格式 输入的第一行包含两个整数 n n, m m，用一个空格分隔。
第二行包含 n n 个整数 A i A i ​ ，相邻整数之间使用一个空格分隔，表示小蓝每回合的状态值。
输出格式 输出一行包含一个整数表示给定条件下不同游玩过程的数量。由于答案可能很大，你只需要输出答案对 998244353 998244353 取模的结果。
样例输入 9 15 3 2 5 7 1 4 6 8 3 copy 样例输出 392149233 copy 评测用例规模与约定 对于 40 40% 的评测用例， n ≤ 300 n≤300， m ≤ 500 m≤500；
对于所有评测用例， 1 ≤ n ≤ 2 × 1 0 5 1≤n≤2×10 5 ， n ≤ m ≤ 1 0 18 n≤m≤10 18 ， 1 ≤ A i ≤ 1 0 9 1≤A i ​ ≤10 9 。
]]></content>
      <categories>
        <category>program</category>
        <category>一些题解</category>
      </categories>
  </entry>
  <entry>
    <title>杂谈笔记</title>
    <url>/posts/5f473243.html</url>
    <content><![CDATA[之前一些函数名笔记
一些matlab函数名；
可能python也适合用
矩阵运算操作求矩阵的转置 (A)'；
求矩阵的逆 inv(A)；
求矩阵的模det(A)；
2.数运算操作e的次方 exp(A)指数函数；
exp(x) 以e为底数 ；对数函数 log(x) 自然对数，即以e为底数的对数；
log10(x) 常用对数，即以10为底数的对数； log2(x) 以2为底数的x的对数；
开方函数 sqrt(x) 表示x的算术平方根； 绝对值函数 abs(x) 表示实数的绝对值以及复数的模；
三角函数（自变量的单位为弧度）
sin(x) 正弦函数 ；cos(x) 余弦函数； tan(x) 正切函数 ；cot(x) 余切函数 ；sec(x) 正割函数 ；csc(x) 余割函数；
反三角函数 asin(x) 反正弦函数； acos(x) 反余弦函数 ；atan(x) 反正切函数 ；acot(x) 反余切函数 ；asec(x) 反正割函数； acsc(x) 反余割函数；
双曲函数 sinh(x) 双曲正弦函数； cosh(x) 双曲余弦函数； tanh(x) 双曲正切函数； coth(x) 双曲余切函数； sech(x) 双曲正割函数 ；csch(x) 双曲余割函数； 反双曲函数 asinh(x) 反双曲正弦函数； acosh(x) 反双曲余弦函数； atanh(x) 反双曲正切函数； acoth(x) 反双曲余切函数 ；asech(x) 反双曲正割函数； acsch(x) 反双曲余割函数 ；
求角度函数 atan2(y,x) 以坐标原点为顶点，x轴正半轴为始边，从原点到点（x，y）的射线为终边的角，其单位为弧度；
数论函数 gcd(a,b) 两个整数的最大公约数 ；lcm(a,b) 两个整数的最小公倍数 ；
排列组合函数 factorial(n) 阶乘函数，表示n的阶乘；
复数函数 real(z) 实部函数；imag(z) 虚部函数 ；abs(z) 求复数z的模； angle(z) 求复数z的辐角；conj(z) 求复数z的共轭复数 ；
求整函数与截尾函数 ceil(x) 表示大于或等于实数x的最小整数 ；
floor(x) 表示小于或等于实数x的最大整数；
round(x) 最接近x的整数；
最大、最小函数 max([a，b，c，．．．])
求最大数 ；min([a，b，c，．．])
求最小数 ；符号函数 sign(x)
一些简短的算法笔记
n第k位数字：n&gt;&gt;k&amp;1
返回n的最后一位1：lowbit(n) = n &amp; -n
C++的nth_element函数，cin.tie(0)或者ios::sync_with_stdio(false);
KMP扩展：BM算法;Sunday算法。
并查集按秩合并
Arrays.fill(arr,-1);C++memset();memcopy
c++reference
Character.isDigit(‘c’)—false
需要注意的是 在windows中按一下回车键 一共有两个字符 “\n\r” 而read()只能读取一个字符，所以如要要用read来达到吸收回车的目的，需要用两个read(); 如果用readLine()的话会将&quot;\n\r&quot;全部吸收 ， 所以只需要一个readLine()来吸收回车.
readLine()用回车来进行下一步操作//cin.sval;只适合吸收非数字字符，不吸收空格。
int num2 = Integer.parseInt(str);
String ss = String.valueOf(n);
数字字符串用BufferedReader
//字符串转化字符数组过程中从下表零开始
并查集按秩合并
cin.nextToken();
String st=cin.sval;
str=st.toCharArray();
int res = Integer.MAX_VALUE;
g[i][j] = Integer.parseInt(String.valueOf(s.charAt(j)));
in.readLine();
g[i][j] = s.charAt(j) - '0';



浮点数
//double a;
//String str=String.format(&quot;%.2f&quot;,a)//!!!!!!!
//cout.print(str);
//或者 cout,printf(&quot;%.6f&quot;,a);

//检查Segm fault错误
System.exit(0);

//全新输入 
String []str=in.readLine().split(&quot; &quot;);
n=Integer.parseInt(str[0]);
m=Integer.parseInt(str[1])

]]></content>
      <categories>
        <category>program</category>
        <category>算法刷题路线</category>
      </categories>
  </entry>
  <entry>
    <title>梯度下降优化</title>
    <url>/posts/2d61ce16.html</url>
    <content><![CDATA[梯度下降优化
1、归一化 Normalization
1.1、归一化目的
  梯度下降的原理和应用，我们已经在前面课程中进行了学习，大家仔细观察下图。

  不同方向的陡峭度是不一样的，即不同维度的数值大小是不同。也就是说梯度下降的快慢是不同的：



 如果维度多了，就是超平面（了解一下霍金所说的宇宙十一维空间），很难画出来了，感受一下下面这张图的空间维度情况。

  如果拿多元线性回归举例的话，因为多元线性回归的损失函数 MSE 是凸函数，所以我们可以把损失函数看成是一个碗。然后下面的图就是从碗上方去俯瞰！哪里是损失最小的地方呢？当然对应的就是碗底的地方！所以下图碗中心的地方颜色较浅的区域就是损失函数最小的地方。

  上面两张图都是进行梯度下降，你有没有发现，略有不同啊？两张图形都是鸟瞰图，左边的图形做了归一化处理，右边是没有做归一化的俯瞰图。
  啥是归一化呢？请带着疑问跟我走~
  我们先来说一下为什么没做归一化是右侧图示，举个例子假如我们客户数据信息，有两个维度，一个是用户的年龄，一个是用户的月收入，目标变量是快乐程度。



name
age
salary
happy




路博通
36
7000
100


马老师
42
20000
180


赵老师
22
30000
164


……
……
……
……



  我们可以里面写出线性回归公式， y=θ1x1+θ2x2+by = \theta_1x_1 + \theta_2x_2 + by=θ1​x1​+θ2​x2​+b ，那么这样每一条样本不同维度对应的数量级不同，原因是每个维度对应的物理含义不同嘛，但是计算机能理解 36 和 7000 分别是年龄和收入吗？计算机只是拿到一堆数字而已。
  我们把 x1x_1x1​ 看成是年龄，x2x_2x2​ 看成是收入， y 对应着快乐程度。机器学习就是在知道 X，y的情况下解方程组调整出最优解的过程。根据公式我们也可以发现 y 是两部分贡献之和，按常理来说，一开始并不知道两个部分谁更重要的情况下，可以想象为两部分对 y 的贡献是一样的即 θ1x1=θ2x2\theta_1x_1 = \theta_2x_2θ1​x1​=θ2​x2​ ，如果 x1≪x2x_1 \ll x_2x1​≪x2​ ，那么最终 θ1≫θ2\theta_1 \gg \theta_2θ1​≫θ2​ （远大于）。
  这样是不是就比较好理解为什么之前右侧示图里为什么 θ1&gt;θ2\theta_1 &gt; \theta_2θ1​&gt;θ2​ ，看起来就是椭圆。再思考一下，梯度下降第 1 步的操作，是不是所有的维度 θ\thetaθ 都是根据在期望 μ\muμ 为 0 方差 σ\sigmaσ 为 1 的正太分布随机生成的，说白了就是一开始的 θ1\theta_1θ1​ 和 θ2\theta_2θ2​ 数值是差不多的。所以可以发现 θ1\theta_1θ1​ 从初始值到目标位置 θ1target\theta_1^{target}θ1target​ 的距离要远大于 θ2\theta_2θ2​ 从初始值到目标位置θ2target\theta_2^{target}θ2target​。
  因为 x1≪x2x_1 \ll x_2x1​≪x2​，根据梯度公式 gj=(hθ(x)−y)xjg_j= (h_{\theta}(x) - y)x_jgj​=(hθ​(x)−y)xj​ ，得出 g1≪g2g_1 \ll g_2g1​≪g2​。根据梯度下降公式：θjn+1=θjn−η∗gj\theta_j^{n+1} = \theta_j^n - \eta * g_jθjn+1​=θjn​−η∗gj​ 可知，每次调整 θ1\theta_1θ1​ 的幅度 ≪\ll≪ （远小于） θ2\theta_2θ2​ 的调整幅度。
  总结一下 ，根据上面得到的两个结论 ，它俩之间是互相矛盾的 ，意味着最后 θ2\theta_2θ2​ 需要比 θ1\theta_1θ1​ 更少的迭代次数就可以收敛，而我们要最终求得最优解，就必须每个维度 θ\thetaθ 都收敛才可以，所以会出现 θ2\theta_2θ2​ 等待 θ1\theta_1θ1​ 收敛的情况。讲到这里对应图大家应该可以理解为什么右图是先顺着 θ2\theta_2θ2​ 的坐标轴往下走再往右走的原因了吧。
结论:
  归一化的一个目的是，使得梯度下降在不同维度 θ\thetaθ 参数（不同数量级）上，可以步调一致协同的进行梯度下降。这就好比社会主义，一小部分人先富裕起来了，先富带后富，这需要一定的时间，先富的这批人等待其他的人富裕起来；但是，更好途经是实现共同富裕，最后每个人都不能落下， 优化的步伐是一致的。

经过归一化处理，收敛的速度，明显快了！
1.2、归一化本质
  做归一化的目的是要实现**“共同富裕”**，而之所以梯度下降优化时不能达到步调一致的根本原因其实还是 x1x_1x1​ 和 x2x_2x2​ 的数量级不同。所以什么是归一化？
  答案自然就出来了，就是把 x1x_1x1​ 和 x2x_2x2​ 的数量级统一，扩展一点说，如果有更多特征维度，就要把各个特征维度 x1、x2、……、xnx_1、x_2、……、x_nx1​、x2​、……、xn​ 的数量级统一，来做到无量纲化。
1.3、最大值最小值归一化
  也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间。转换函数如下：
$X^* = \frac{X - X_min}{X_max -X_min} $
  其实我们很容易发现使用最大值最小值归一化（min-max标准化）的时候，优点是一定可以把数值归一到 0 ~ 1 之间，缺点是如果有一个离群值（比如马云的财富），正如我们举的例子一样，会使得一个数值为 1，其它数值都几乎为 0，所以受离群值的影响比较大！
代码演示：
import numpy as npx_1 = np.random.randint(1,10,size = 10)x_2 = np.random.randint(100,300,size = 10)x = np.c_[x_1,x_2]print(&#x27;归一化之前的数据：&#x27;)display(x)x_ = (x - x.min(axis = 0)) / (x.max(axis = 0) - x.min(axis = 0))print(&#x27;归一化之后的数据：&#x27;)display(x_)
使用scikit-learn函数：
import numpy as npfrom sklearn.preprocessing import MinMaxScalerx_1 = np.random.randint(1,10,size = 10)x_2 = np.random.randint(100,300,size = 10)x = np.c_[x_1,x_2]print(&#x27;归一化之前的数据：&#x27;)display(x)min_max_scaler = MinMaxScaler()x_ = min_max_scaler.fit_transform(x)print(&#x27;归一化之后的数据：&#x27;)display(x_)
1.4、0-均值标准化
  这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化，也叫做Z-score标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，转化函数为：
$X^* = \frac{X - \mu}{\sigma} $
其中μ为所有样本数据的均值，σ为所有样本数据的标准差。
$\mu = \frac{1}{n}\sum\limits_{i = 1}^nx_i $
$\sigma = \sqrt{\frac{1}{n}\sum\limits_{i = 1}^n(x_i - \mu)^2} $
  相对于最大值最小值归一化来说，因为标准归一化除以了标准差，而标准差的计算会考虑到所有样本数据，所以受到离群值的影响会小一些，这就是除以方差的好处！但是，0-均值标准化不一定会把数据缩放到 0 ~ 1 之间了。既然是0均值，也就意味着，有正有负！
import numpy as npx_1 = np.random.randint(1,10,size = 10)x_2 = np.random.randint(100,300,size = 10)x = np.c_[x_1,x_2]print(&#x27;归一化之前的数据：&#x27;)display(x)x_ = (x - x.mean(axis = 0)) / x.std(axis = 0)print(&#x27;归一化之后的数据：&#x27;)display(x_)
使用scikit-learn函数：
import numpy as npfrom sklearn.preprocessing import StandardScalerx_1 = np.random.randint(1,10,size = 10)x_2 = np.random.randint(100,300,size = 10)x = np.c_[x_1,x_2]print(&#x27;归一化之前的数据：&#x27;)display(x)standard_scaler = StandardScaler()x_ = standard_scaler.fit_transform(x)print(&#x27;归一化之后的数据：&#x27;)display(x_)
  那为什么要减去均值呢？其实做均值归一化还有一个特殊的好处（对比最大值最小值归一化，全部是正数0~1），我们来看一下梯度下降的式子，你就会发现 α\alphaα 是正数，不管 A 是正还是负（ A 就是 y^−y=hθ(x)−y\hat{y} - y = h_{\theta}(x) - yy^​−y=hθ​(x)−y），对于所有的维度 X，比如这里的 x1x_1x1​ 和 x2x_2x2​ 来说，α\alphaα 乘上 A 都是一样的符号，那么每次迭代的时候 w1t+1w_1^{t+1}w1t+1​ 和 w2t+1w_2^{t+1}w2t+1​ 的更新幅度符号也必然是一样的，这样就会像下图有右侧所示：要想从 wtw_twt​ 更新到 w∗w^*w∗ 就必然要么 w1w_1w1​ 和 w2w_2w2​ 同时变大再同时变小，或者就 w1w_1w1​ 和 w2w_2w2​ 同时变小再同时变大。不能如图上所示蓝色的最优解路径，即 w1w_1w1​ 变小的同时 w2w_2w2​ 变大！

  那我们如何才能做到让 w1w_1w1​ 变小的时候 w2w_2w2​ 变大呢？归其根本还是数据集 X 矩阵（经过min-max归一化）中的数据均为正数。所以如果我们可以让 x1x_1x1​ 和 x2x_2x2​ 它们符号不同，比如有正有负，其实就可以在做梯度下降的时候有更多的可能性去让更新尽可能沿着最优解路径去走。
  结论：0-均值标准化处理数据之后，属性有正有负，可以让梯度下降沿着最优路径进行~
注意：
  我们在做特征工程的时候，很多时候如果对训练集的数据进行了预处理，比如这里讲的归一化，那么未来对测试集的时候，和模型上线来新的数据的时候，都要进行相同的数据预处理流程，而且所使用的均值和方差是来自当时训练集的均值和方差!
  因为我们人工智能要干的事情就是从训练集数据中找规律，然后利用找到的规律去预测新产生的数据。这也就是说假设训练集和测试集以及未来新来的数据是属于同分布的！从代码上面来说如何去使用训练集的均值和方差呢？就需要把 scaler 对象持久化， 回头模型上线的时候再加载进来去对新来的数据进行处理。
import joblibjoblib.dump(standard_scaler,&#x27;scale&#x27;) # 持久化standard_scaler = joblib.load(&#x27;scale&#x27;) # 加载standard_scaler.transform(x) # 使用
2、正则化 Regularization
2.1、过拟合欠拟合

欠拟合（under fit）：还没有拟合到位，训练集和测试集的准确率都还没有到达最高，学的还不到位。
过拟合（over fit）：拟合过度，训练集的准确率升高的同时，测试集的准确率反而降低。学的过度了（走火入魔），做过的卷子都能再次答对（死记硬背），考试碰到新的没见过的题就考不好（不会举一反三）。
恰到好处（just right）：过拟合前，训练集和测试集准确率都达到巅峰。好比，学习并不需要花费很多时间，理解的很好，考试的时候可以很好的把知识举一反三。


  正则化就是防止过拟合，增加模型的鲁棒性，鲁棒是 Robust 的音译，也就是强壮的意思。就像计算机软件在面临攻击、网络过载等情况下能够不死机不崩溃，这就是软件的鲁棒性。鲁棒性调优就是让模型拥有更好的鲁棒性，也就是让模型的泛化能力和推广 能力更加的强大。
  举例子说明：下面两个式子描述同一条直线那个更好？
y=0.3x1+0.4x2+0.5y = 0.3x_1 + 0.4x_2 + 0.5y=0.3x1​+0.4x2​+0.5
y=3x1+4x2+5y = 3x_1 + 4x_2 + 5y=3x1​+4x2​+5
  第一个更好，因为下面的公式是上面的十倍，当 w 越小公式的容错的能力就越好。因为把测试数据带入公式中如果测试集原来是 [32, 128] 在带入的时候发生了一些偏差，比如说变成 [30, 120] ，第二个模型结果就会比第一个模型结果的偏差大的多。公式中 y=WTxy = W^Txy=WTx ，当 x 有一点错误，这个错误会通过 w 放大。但是 w 不能太小，当 w 太小时（比如都趋近0），模型就没有意义了，无法应用。想要有一定的容错率又要保证正确率就要由正则项来发挥作用了！
  所以正则化(鲁棒性调优)的本质就是牺牲模型在训练集上的正确率来提高推广、泛化能力， W 在数值上越小越好，这样能抵抗数值的扰动。同时为了保证模型的正确率 W 又不能极小。 故而人们将原来的损失函数加上一个惩罚项，这里面损失函数就是原来固有的损失函数，比如回归的话通常是 MSE，分类的话通常是 cross entropy 交叉熵，然后在加上一部分惩罚项来使得计算出来的模型 W 相对小一些来带来泛化能力。
  常用的惩罚项有L1 正则项或者 L2 正则项：

L1=∣∣w∣∣1=∑i=1n∣wi∣L_1 = ||w||_1 = \sum\limits_{i = 1}^n|w_i|L1​=∣∣w∣∣1​=i=1∑n​∣wi​∣​              对应曼哈顿距离
L2=∣∣w∣∣2=∑i=1n(wi)2L_2 = ||w||_2 = \sqrt{\sum\limits_{i = 1}^n(w_i)^2}L2​=∣∣w∣∣2​=i=1∑n​(wi​)2​        对应欧氏距离

其实 L1 和 L2 正则的公式数学里面的意义就是范数，代表空间中向量到原点的距离：
Lp=∣∣X∣∣p=∑i=1nxipp,X=(x1,x2,……xn)L_p = ||X||_p = \sqrt[p]{\sum\limits_{i = 1}^nx_i^p} , X = (x_1,x_2,……x_n)Lp​=∣∣X∣∣p​=pi=1∑n​xip​​,X=(x1​,x2​,……xn​)

  当我们把多元线性回归损失函数加上 L2 正则的时候，就诞生了 Ridge 岭回归。当我们把多元线性回归损失函数加上 L1 正则的时候，就孕育出来了 Lasso 回归。其实 L1 和 L2 正则项惩罚项可以加到任何算法的损失函数上面去提高计算出来模型的泛化能力的。
2.2、套索回归（Lasso）
先从线性回归开始，其损失函数如下：
$J(\theta) = \frac{1}{2}\sum\limits_{i = 1}^n(h_{\theta}(x^{(i)}) - y^{(i)})^2 $
L1正则化的损失函数，令 $J_0 = J(\theta) $：
$J = J_0 + \alpha * \sum\limits_{i = 1}^n|w_i| $
令   $L_1 = \alpha * \sum\limits_{i = 1}^n|w_i| $ ：
$J = J_0 + L_1 $
  其中 J0J_0J0​ 是原始的损失函数，加号后面的一项是L1正则化项， α\alphaα 是正则化系数。注意到 L1正则化是权值的绝对值之和。JJJ 是带有绝对值符号的函数，因此 JJJ 是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数 J0J_0J0​ 后面添加L1正则项时，相当于对 J0J_0J0​ 做了一个约束。令L1=α∗∑i=1n∣wi∣L_1 = \alpha * \sum\limits_{i = 1}^n|w_i|L1​=α∗i=1∑n​∣wi​∣ ，则 J=J0+L1J = J_0 + L_1J=J0​+L1​ ，此时我们的任务变成在 L1L_1L1​ 约束下求出 J0J_0J0​ 取最小值的解。考虑二维的情况，即只有两个权值 w1、w2w_1、w_2w1​、w2​ ，此时 L1=∣w1∣+∣w2∣L_1 = |w_1| + |w_2|L1​=∣w1​∣+∣w2​∣。 对于梯度下降法，求解 J0J_0J0​ 过程可以画出等值线，同时 L1 正则化的函数 L1L_1L1​ 也可以在 w1、w2w_1、w_2w1​、w2​所在的平面上画出来：

  图中等值线是J0J_0J0​的等值线，是椭圆形。黑色方框是 L1L_1L1​ 函数的图形，L1=∣w1∣+∣w2∣L_1 = |w_1| + |w_2|L1​=∣w1​∣+∣w2​∣ 这个函数画出来，就是一个方框。
  在图中，当 J0J_0J0​ 等值线与 L1L_1L1​ 图形首次相交的地方就是最优解。上图中 J0J_0J0​ 与 L1L_1L1​ 在 L1L_1L1​ 的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是 (w1,w2)=(0,w)(w_1,w_2) = (0,w)(w1​,w2​)=(0,w) 。可以直观想象，因为 L1L_1L1​ 函数有很多『突出的角』（二维情况下四个，多维情况下更多）， J0J_0J0​ 与这些角接触的机率会远大于与 L1L_1L1​ 其它部位接触的机率（这是很直觉的想象，突出的角比直线的边离等值线更近写），而在这些角上，会有很多权值等于0（因为角就在坐标轴上），这就是为什么 L1 正则化可以产生稀疏模型（很多权重等于0了），进而可以用于特征选择。
  而正则化前面的系数 α\alphaα，可以控制 L1L_1L1​ 图形的大小。α\alphaα 越小，L1L_1L1​ 的图形越大（上图中的黑色方框）；α\alphaα 越大，L1L_1L1​ 的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优解的值(w1,w2)=(0,w)(w_1,w_2) = (0,w)(w1​,w2​)=(0,w) 中的 w 可以取到很小的值的原因所在。
代码演示 α\alphaα 取值大小对黑色方框的尺寸影响：
import matplotlib.pyplot as plt# α 的值是：1# 1 = x + y# y = 1 -xf = lambda x : 1- xx = np.linspace(0,1,100)plt.axis(&#x27;equal&#x27;)plt.plot(x, f(x), color = &#x27;green&#x27;)# α 的值是：3# 1 = 3 * x + 3 * y# y = 1/3 -xf2 = lambda x : 1/3 - x x2 = np.linspace(0,1/3,100)plt.plot(x2, f2(x2),color = &#x27;red&#x27;)# 一些列设置plt.xlim(-2,2)plt.ylim(-2,2)ax = plt.gca()ax.spines[&#x27;right&#x27;].set_color(&#x27;None&#x27;)  # 将图片的右框隐藏ax.spines[&#x27;top&#x27;].set_color(&#x27;None&#x27;)  # 将图片的上边框隐藏ax.spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;, 0)) # x轴出现在y轴的-1 位置ax.spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;, 0))plt.savefig(&#x27;./图片/13-alpha对方框影响.png&#x27;,dpi = 200)

权重更新规则如下：

损失函数：

$J(\theta) = \frac{1}{2}\sum\limits_{i = 1}^n(h_{\theta}(x^{(i)}) - y^{(i)})^2 $
$L_1  =  \alpha * \sum\limits_{i = 1}^n|w_i| $
$J = J_0 + L_1 $

更新规则：

$\theta_j^{n + 1} = \theta_j^{n} - \eta * \frac{\partial}{\partial \theta_j}J $
$\theta_j^{n + 1} = \theta_j^{n} - \eta * \frac{\partial}{\partial \theta_j}(J_0 + L_1) $
$\frac{\partial}{\partial \theta_j}J_0 =\sum\limits_{i = 1}^{n} (h_{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)}  $
$\frac{\partial}{\partial \theta_j}L_1 = \alpha * sgn(w_i)  $
其中  J0J_0J0​ 即是线性回归的损失函数，L1L_1L1​ 是添加的正则项。sgn(wi)sgn(w_i)sgn(wi​)  表示符号函数、指示函数，值为：1 或 -1。
sgn(wi)={1,wi&gt;0−1,wi&lt;0sgn(w_i) = \begin{cases}1, &amp;w_i &gt; 0\\-1,&amp;w_i &lt; 0\end{cases}sgn(wi​)={1,−1,​wi​&gt;0wi​&lt;0​
注意当 wi=0w_i = 0wi​=0 时不可导。
综上所述，L1正则化权重更新如下：
$\theta_j^{n + 1} = \theta_j^{n} -\eta\sum\limits_{i = 1}^{n} (h_{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)} - \eta*\alpha * sgn(w_i) $

Lasso回归和线性回归相比，多了一项：−η∗α∗sgn(wi)-\eta * \alpha * sgn(w_i)−η∗α∗sgn(wi​)
$\eta $ 大于零，表示梯度下降学习率
α\alphaα 大于零，表示L1正则化系数
当wiw_iwi​为正时候  sgn(wi)=1sgn(w_i) = 1sgn(wi​)=1，直接减去 η∗α\eta * \alphaη∗α （大于0），所以正的 wiw_iwi​ 变小了
当wiw_iwi​​为负时候  sgn(wi)=−1sgn(w_i) = -1sgn(wi​)=−1​，相当于直接加上 η∗α\eta * \alphaη∗α​ （大于0），所以负的 wiw_iwi​​​ 变大了，绝对值变小，向0靠近

有的书本上公式会这样写，其中 λ\lambdaλ 表示L1正则化系数：
$\theta_j^{n + 1} = \theta_j^{n} -\eta\sum\limits_{i = 1}^{n} (h_{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)} - \eta*\lambda * sgn(w_i) $
2.3、岭回归（Ridge）
也是先从线性回归开始，其损失函数如下：
$J(\theta) = \frac{1}{2}\sum\limits_{i = 1}^n(h_{\theta}(x^{(i)}) - y^{(i)})^2 $
L2正则化的损失函数（对L2范数，进行了平方运算），令J0=J(θ)J_0 = J(\theta)J0​=J(θ)：
$J = J_0 + \alpha * \sum\limits_{i = 1}^n(w_i)^2 $
令  L2=α∗∑i=1n(wi)2L_2 = \alpha * \sum\limits_{i = 1}^n(w_i)^2L2​=α∗i=1∑n​(wi​)2 ：
$J = J_0 + L_2 $
同样可以画出他们在二维平面上的图形，如下：

二维平面下 L2 正则化的函数图形是个圆（绝对值的平方和，是个圆），与方形相比，被磨去了棱角。因此 J0J_0J0​ 与  L2L_2L2​ 相交时使得 w1、w2w_1、w_2w1​、w2​ 等于零的机率小了许多（这个也是一个很直观的想象），这就是为什么L2正则化不具有稀疏性的原因，因为不太可能出现多数 w 都为0的情况（这种情况就叫稀疏性）！
权重更新规则如下：

损失函数：

$J(\theta) = \frac{1}{2}\sum\limits_{i = 1}^n(h_{\theta}(x^{(i)}) - y^{(i)})^2 $
$L_2  =  \alpha * \sum\limits_{i = 1}^n(w_i)^2 $
$J = J_0 + L_2 $

更新规则：

$\theta_j^{n + 1} = \theta_j^{n} - \eta * \frac{\partial}{\partial \theta_j}J $
$\theta_j^{n + 1} = \theta_j^{n} - \eta * \frac{\partial}{\partial \theta_j}(J_0 + L_2) $
$\frac{\partial}{\partial \theta_j}J_0 =\sum\limits_{i = 1}^{n} (h_{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)}  $
$\frac{\partial}{\partial \theta_j}L_2 = 2\alpha w_i  $
其中 J0J_0J0​ 即是线性回归的损失函数，L2L_2L2​ 是添加的正则项。
综上所述，L2正则化权重更新如下（2α2\alpha2α 也是常数项，可以合并到一起用整体 α\alphaα  替代）：
$\theta_j^{n + 1} = \theta_j^{n}(1-\eta * \alpha) -\eta *\sum\limits_{i = 1}^{n} (h_{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)} $
其中 α\alphaα 就是正则化参数，η\etaη 表示学习率。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代， θj\theta_jθj​ 都要先乘以一个小于1的因子（即 (1−η∗α)(1-\eta * \alpha)(1−η∗α) ），从而使得 θj\theta_jθj​ 加速减小，因此总的来看，θ\thetaθ 相比不加L2正则项的线性回归可以获得更小的值。从而，实现了防止过拟合的效果，增加模型的鲁棒性~
有的书本上，公式写法可能不同：其中 λ\lambdaλ 表示正则化参数。
$\theta_j^{n + 1} = \theta_j^{n}(1-\eta * \lambda) -\eta *\sum\limits_{i = 1}^{n} (h_{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)} $
3、线性回归衍生算法
  接下来，我们一起学习一下scikit-learn中为我们提供的线性回归衍生算法，根据上面所学的原理，对比线性回归加深理解。
3.1、Ridge算法使用
这是scikit-learn官网给出的岭回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。
$\underset{w}\min || X w - y||_2^2 + \alpha ||w||_2^2 $
L2正则化和普通线性回归系数对比：
import numpy as npfrom sklearn.linear_model import Ridgefrom sklearn.linear_model import SGDRegressor# 1、创建数据集X，yX = 2*np.random.rand(100, 5)w = np.random.randint(1,10,size = (5,1))b = np.random.randint(1,10,size = 1)y = X.dot(w) + b + np.random.randn(100, 1)print(&#x27;原始方程的斜率：&#x27;,w.ravel())print(&#x27;原始方程的截距：&#x27;,b)ridge = Ridge(alpha= 1, solver=&#x27;sag&#x27;)ridge.fit(X, y)print(&#x27;岭回归求解的斜率：&#x27;,ridge.coef_)print(&#x27;岭回归求解的截距：&#x27;,ridge.intercept_)# 线性回归梯度下降方法sgd = SGDRegressor(penalty=&#x27;l2&#x27;,alpha=0,l1_ratio=0)sgd.fit(X, y.reshape(-1,))print(&#x27;随机梯度下降求解的斜率是：&#x27;,sgd.coef_)print(&#x27;随机梯度下降求解的截距是：&#x27;,sgd.intercept_)
结论：

和没有正则项约束线性回归对比，可知L2正则化，将方程系数进行了缩小
α\alphaα  增大求解出来的方程斜率变小
Ridge回归源码解析：

alpha：正则项系数
fit_intercept：是否计算 w0w_0w0​ 截距项
normalize：是否做归一化
max_iter：最大迭代次数
tol：结果的精确度
solver：优化算法的选择



3.2、Lasso算法使用
这是scikit-learn官网给出的套索回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。
$\underset{w}\min { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha ||w||_1} $
公式中多了一项：$\frac{1}{2n_{samples}} $这是一个常数项，去掉之后，也不会影响损失函数公式计算。在岭回归中，就没有这项。
L1正则化和普通线性回归系数对比：
import numpy as npfrom sklearn.linear_model import Lassofrom sklearn.linear_model import SGDRegressor# 1、创建数据集X，yX = 2*np.random.rand(100, 20)w = np.random.randn(20,1)b = np.random.randint(1,10,size = 1)y = X.dot(w) + b + np.random.randn(100, 1)print(&#x27;原始方程的斜率：&#x27;,w.ravel())print(&#x27;原始方程的截距：&#x27;,b)lasso = Lasso(alpha= 0.5)lasso.fit(X, y)print(&#x27;套索回归求解的斜率：&#x27;,lasso.coef_)print(&#x27;套索回归求解的截距：&#x27;,lasso.intercept_)# 线性回归梯度下降方法sgd = SGDRegressor(penalty=&#x27;l2&#x27;,alpha=0, l1_ratio=0)sgd.fit(X, y.reshape(-1,))print(&#x27;随机梯度下降求解的斜率是：&#x27;,sgd.coef_)print(&#x27;随机梯度下降求解的截距是：&#x27;,sgd.intercept_)
结论：

和没有正则项约束线性回归对比，可知L1正则化，将方程系数进行了缩减，部分系数为0，产生稀疏模型
α\alphaα  越大，模型稀疏性越强，越多的参数为0
Lasso回归源码解析：

alpha：正则项系数
fit_intercept：是否计算 w0w_0w0​ 截距项
normalize：是否做归一化
precompute：bool 类型，默认值为False，决定是否提前计算Gram矩阵来加速计算
max_iter：最大迭代次数
tol：结果的精确度
warm_start：bool类型，默认值为False。如果为True，那么使⽤用前⼀次训练结果继续训练。否则从头开始训练



3.3、Elastic-Net算法使用
这是scikit-learn官网给出的弹性网络回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。
$\underset{w}\min { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 +
\frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2} $
  Elastic-Net 回归，即岭回归和Lasso技术的混合。弹性网络是一种使用 L1， L2 范数作为先验正则项训练的线性回归模型。 这种组合允许学习到一个只有少量参数是非零稀疏的模型，就像 Lasso 一样，但是它仍然保持一些像 Ridge 的正则性质。我们可利用 l1_ratio 参数控制 L1 和 L2 的凸组合。
  弹性网络在很多特征互相联系（相关性，比如身高和体重就很有关系）的情况下是非常有用的。Lasso 很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。
  在实践中，Lasso 和 Ridge 之间权衡的一个优势是它允许在迭代过程中继承 Ridge 的稳定性。
弹性网络回归和普通线性回归系数对比：
import numpy as npfrom sklearn.linear_model import ElasticNetfrom sklearn.linear_model import SGDRegressor# 1、创建数据集X，yX = 2*np.random.rand(100, 20)w = np.random.randn(20,1)b = np.random.randint(1,10,size = 1)y = X.dot(w) + b + np.random.randn(100, 1)print(&#x27;原始方程的斜率：&#x27;,w.ravel())print(&#x27;原始方程的截距：&#x27;,b)model = ElasticNet(alpha= 1, l1_ratio = 0.7)model.fit(X, y)print(&#x27;弹性网络回归求解的斜率：&#x27;,model.coef_)print(&#x27;弹性网络回归求解的截距：&#x27;,model.intercept_)# 线性回归梯度下降方法sgd = SGDRegressor(penalty=&#x27;l2&#x27;,alpha=0, l1_ratio=0)sgd.fit(X, y.reshape(-1,))print(&#x27;随机梯度下降求解的斜率是：&#x27;,sgd.coef_)print(&#x27;随机梯度下降求解的截距是：&#x27;,sgd.intercept_)
结论：

和没有正则项约束线性回归对比，可知Elastic-Net网络模型，融合了L1正则化L2正则化
Elastic-Net 回归源码解析：

alpha：混合惩罚项的常数
l1_ratio：弹性网混合参数，0 &lt;= l1_ratio &lt;= 1，对于 l1_ratio = 0，惩罚项是L2正则惩罚。对于 l1_ratio = 1是L1正则惩罚。对于 0
fit_intercept：是否计算 w0w_0w0​ 截距项
normalize：是否做归一化
precompute：bool 类型，默认值为False，决定是否提前计算Gram矩阵来加速计算
max_iter：最大迭代次数
tol：结果的精确度
warm_start：bool类型，默认值为False。如果为True，那么使⽤用前⼀次训练结果继续训练。否则从头开始训练



4、多项式回归
4.1、多项式回归基本概念
  升维的目的是为了去解决欠拟合的问题的，也就是为了提高模型的准确率为目的的，因为当维度不够时，说白了就是对于预测结果考虑的因素少的话，肯定不能准确的计算出模型。

  在做升维的时候，最常见的手段就是将已知维度进行相乘（或者自乘）来构建新的维度，如下图所示。普通线性方程，无法拟合规律，必须是多项式，才可以完美拟合曲线规律，图中是二次多项式。

  对于多项式回归来说主要是为了扩展线性回归算法来适应更广泛的数据集，比如我们数据集有两个维度 x1、x2x_1、x_2x1​、x2​，那么用多元线性回归公式就是：y^=w0+w1x1+w2x2\hat{y} = w_0 + w_1x_1 + w_2x_2y^​=w0​+w1​x1​+w2​x2​，当我们使用二阶多项式升维的时候，数据集就从原来的 x1、x2x_1、x_2x1​、x2​扩展成了x1、x2、x12、x22、x1x2x_1、x_2、x_1^2、x_2^2、x_1x_2x1​、x2​、x12​、x22​、x1​x2​ 。因此多元线性回归就得去多计算三个维度所对应的w值：y^=w0+w1x1+w2x2+w3x12+w4x22+w5x1x2\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2 + w_5x_1x_2y^​=w0​+w1​x1​+w2​x2​+w3​x12​+w4​x22​+w5​x1​x2​ 。
  此时拟合出来的方程就是曲线，可以解决一些线性回归的欠拟合问题！
4.2、多项式回归实战1.0
import numpy as npimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegression# 1、创建数据，并进行可视化X = np.linspace(-1,11,num = 100)y = (X - 5)**2 + 3*X -12 + np.random.randn(100)X = X.reshape(-1,1)plt.scatter(X,y)# 2、创建预测数据X_test = np.linspace(-2,12,num = 200).reshape(-1,1)# 3、不进行升维 + 普通线性回归model_1 = LinearRegression()model_1.fit(X,y)y_test_1 = model_1.predict(X_test)plt.plot(X_test,y_test,color = &#x27;red&#x27;)# 4、多项式升维 + 普通线性回归X = np.concatenate([X,X**2],axis = 1)model_2 = LinearRegression()model_2.fit(X,y)# 5、测试数据处理，并预测X_test = np.concatenate([X_test,X_test**2],axis = 1)y_test_2 = model_2.predict(X_test)# 6、数据可视化，切片操作plt.plot(X_test[:,0],y_test_2,color = &#x27;green&#x27;)
结论：

不进行多项式升维，拟合出来的曲线，是线性的直线，和目标曲线无法匹配
使用np.concatenate()进行简单的，幂次合并，注意数据合并的方向axis = 1
数据可视化时，注意切片，因为数据升维后，多了平方这一维


4.3、多项式回归实战2.0
import numpy as npimport matplotlib.pyplot as pltfrom sklearn.preprocessing import PolynomialFeatures,StandardScalerfrom sklearn.linear_model import SGDRegressor# 1、创建数据，并进行可视化X = np.linspace(-1,11,num = 100)y = (X - 5)**2 + 3*X -12 + np.random.randn(100)X = X.reshape(-1,1)plt.scatter(X,y)# 2、创建预测数据X_test = np.linspace(-2,12,num = 200).reshape(-1,1)# 3、使用PolynomialFeatures进行特征升维poly = PolynomialFeatures()poly.fit(X,y)X = poly.transform(X)s = StandardScaler()X = s.fit_transform(X)# model = SGDRegressor(penalty=&#x27;l2&#x27;,eta0 = 0.0001,max_iter = 10000)model = SGDRegressor(penalty=&#x27;l2&#x27;,eta0 = 0.01)model.fit(X,y)# 4、预测数据X_test = poly.transform(X_test)X_test_norm = s.transform(X_test)y_test = model.predict(X_test_norm)plt.plot(X_test[:,1],y_test,color = &#x27;green&#x27;)
结论：

eta0表示学习率，设置合适的学习率，才能拟合成功
多项式升维，需要对数据进行Z-score归一化处理，效果更佳出色
SGD随机梯度下降需要调整参数，以使模型适应数据




5、代码实战天猫双十一销量预测
  天猫双十一，从2009年开始举办，第一届成交额仅仅0.5亿，后面呈现了爆发式的增长，那么这些增长是否有规律呢？是怎么样的规律，该如何分析呢？我们使用多项式回归一探究竟！

数据可视化，历年天猫双十一销量数据：
import numpy as npfrom sklearn.linear_model import SGDRegressorimport matplotlib.pyplot as pltplt.rcParams[&#x27;font.size&#x27;] = 18plt.figure(figsize=(9,6))# 创建数据，年份数据2009 ~ 2019X = np.arange(2009,2020)y = np.array([0.5,9.36,52,191,350,571,912,1207,1682,2135,2684])plt.bar(X,y,width = 0.5,color = &#x27;green&#x27;)plt.plot(X,y,color = &#x27;red&#x27;)_ = plt.xticks(ticks = X)

有图可知，在一定时间内，随着经济的发展，天猫双十一销量与年份的关系是多项式关系！假定，销量和年份之间关系是三次幂关系：
$f(x) = w_1x + w_2x^2 + w_3x^3 + b $
import numpy as npfrom sklearn.linear_model import SGDRegressorimport matplotlib.pyplot as pltfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.preprocessing import StandardScalerplt.figure(figsize=(12,9))# 1、创建数据，年份数据2009 ~ 2019X = np.arange(2009,2020)y = np.array([0.5,9.36,52,191,350,571,912,1207,1682,2135,2684])# 2、年份数据，均值移除，防止某一个特征列数据天然的数值太大而影响结果X = X - X.mean()X = X.reshape(-1,1)# 3、构建多项式特征，3次幂poly = PolynomialFeatures(degree=3)X = poly.fit_transform(X)s = StandardScaler()X_norm = s.fit_transform(X)# 4、创建模型model = SGDRegressor(penalty=&#x27;l2&#x27;,eta0 = 0.5,max_iter = 5000)model.fit(X_norm,y)# 5、数据预测X_test = np.linspace(-5,6,100).reshape(-1,1)X_test = poly.transform(X_test)X_test_norm = s.transform(X_test)y_test = model.predict(X_test_norm)# 6、数据可视化plt.plot(X_test[:,1],y_test,color = &#x27;green&#x27;)plt.bar(X[:,1],y)plt.bar(6,y_test[-1],color = &#x27;red&#x27;)plt.ylim(0,4096)plt.text(6,y_test[-1] + 100,round(y_test[-1],1),ha = &#x27;center&#x27;)_ = plt.xticks(np.arange(-5,7),np.arange(2009,2021))

结论：

数据预处理，均值移除。如果特征基准值和分散度不同在某些算法（例如回归算法，KNN等）上可能会大大影响了模型的预测能力。通过均值移除，大大增强数据的离散化程度。
多项式升维，需要对数据进行Z-score归一化处理，效果更佳出色
SGD随机梯度下降需要调整参数，以使模型适应多项式数据
从2020年开始，天猫双十一统计的成交额改变了规则为11.1日~11.11日的成交数据（之前的数据为双十一当天的数据），2020年成交额为4980亿元
可以，经济发展有其客观规律，前11年高速发展（曲线基本可以反应销售规律），到2020年是一个转折点

6、代码实战中国人寿保费预测
6.1、数据加载与介绍
import numpy as npimport pandas as pddata = pd.read_excel(&#x27;./中国人寿.xlsx&#x27;)print(data.shape)data.head()
数据介绍：

共计1338条保险数据，每条数据7个属性
最后一列charges是保费
前面6列是特征，分别为：年龄、性别、体重指数、小孩数量、是否抽烟、所在地区

6.2、EDA数据探索
import seaborn as sns# 性别对保费影响sns.kdeplot(data[&#x27;charges&#x27;],shade = True,hue = data[&#x27;sex&#x27;])# 地区对保费影响sns.kdeplot(data[&#x27;charges&#x27;],shade = True,hue = data[&#x27;region&#x27;])# 吸烟对保费影响sns.kdeplot(data[&#x27;charges&#x27;],shade = True,hue = data[&#x27;smoker&#x27;])# 孩子数量对保费影响sns.kdeplot(data[&#x27;charges&#x27;],shade = True,hue = data[&#x27;children&#x27;],palette=&#x27;Set1&#x27;)






总结：

不同性别对保费影响不大，不同性别的保费的概率分布曲线基本重合，因此这个特征无足轻重，可以删除
地区同理
吸烟与否对保费的概率分布曲线差别很大，整体来说不吸烟更加健康，那么保费就低，这个特征很重要
家庭孩子数量对保费有一定影响

6.3、特征工程
data = data.drop([&#x27;region&#x27;, &#x27;sex&#x27;], axis=1)data.head() # 删除不重要特征# 体重指数，离散化转换，体重两种情况：标准、肥胖def convert(df,bmi):    df[&#x27;bmi&#x27;] = &#x27;fat&#x27; if df[&#x27;bmi&#x27;] &gt;= bmi else &#x27;standard&#x27;    return dfdata = data.apply(convert, axis = 1, args=(30,))data.head()# 特征提取，离散型数据转换为数值型数据data = pd.get_dummies(data)data.head()# 特征和目标值抽取X = data.drop(&#x27;charges&#x27;, axis=1) # 训练数据y = data[&#x27;charges&#x27;] # 目标值X.head()
6.4、特征升维
from sklearn.linear_model import LinearRegressionfrom sklearn.linear_model import ElasticNetfrom sklearn.metrics import mean_squared_error,mean_squared_log_error# 数据拆分from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import PolynomialFeaturesX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)# 特征升维poly = PolynomialFeatures(degree= 2, include_bias = False)X_train_poly = poly.fit_transform(X_train)X_test_poly = poly.fit_transform(X_test)
6.5、模型训练与评估
普通线性回归：
model_1 = LinearRegression()model_1.fit(X_train_poly, y_train)print(&#x27;测试数据得分：&#x27;,model_1.score(X_train_poly,y_train))print(&#x27;预测数据得分：&#x27;,model_1.score(X_test_poly,y_test))print(&#x27;训练数据均方误差：&#x27;,np.sqrt(mean_squared_error(y_train,model_1.predict(X_train_poly))))print(&#x27;测试数据均方误差：&#x27;,np.sqrt(mean_squared_error(y_test,model_1.predict(X_test_poly))))print(&#x27;训练数据对数误差：&#x27;,np.sqrt(mean_squared_log_error(y_train,model_1.predict(X_train_poly))))print(&#x27;测试数据对数误差：&#x27;,np.sqrt(mean_squared_log_error(y_test,model_1.predict(X_test_poly))))
弹性网络回归：
model_2 = ElasticNet(alpha = 0.3,l1_ratio = 0.5,max_iter = 50000)model_2.fit(X_train_poly,y_train)print(&#x27;测试数据得分：&#x27;,model_2.score(X_train_poly,y_train))print(&#x27;预测数据得分：&#x27;,model_2.score(X_test_poly,y_test))print(&#x27;训练数据均方误差为：&#x27;,np.sqrt(mean_squared_error(y_train,model_2.predict(X_train_poly))))print(&#x27;测试数据均方误差为：&#x27;,np.sqrt(mean_squared_error(y_test,model_2.predict(X_test_poly))))print(&#x27;训练数据对数误差为：&#x27;,np.sqrt(mean_squared_log_error(y_train,model_2.predict(X_train_poly))))print(&#x27;测试数据对数误差为：&#x27;,np.sqrt(mean_squared_log_error(y_test,model_2.predict(X_test_poly))))
结论：

进行EDA数据探索，可以查看无关紧要特征
进行特征工程：删除无用特征、特征离散化、特征提取。这对机器学习都至关重要
对于简单的数据（特征比较少）进行线性回归，一般需要进行特征升维
选择不同的算法，进行训练和评估，从中筛选优秀算法

]]></content>
      <categories>
        <category>program</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>聚类算法</title>
    <url>/posts/35f08535.html</url>
    <content><![CDATA[聚类算法
1、聚类介绍
1.1、聚类作用


知识发现 发现事物之间的潜在关系


异常值检测


特征提取 数据压缩的例子


1.2、有监督与无监督学习
有监督：


给定训练集 X 和 标签Y


选择模型

学习（目标函数的最优化）
生成模型（本质上是一组参数、方程）



根据生成的一组参数进行预测分类等任务
无监督：


拿到的数据只有X ，没有标签，只能根据X的相似程度做一些事情。


Clustering 聚类

对于大量未标注的数据集，按照内在相似性来分为多个类别（簇） 目标：类别内相似度大，类别间相似小。
也可以用来改变数据的维度，可以将聚类结果作为一个维度添加到训练数据中。



降维算法，数据特征变少


1.3、聚类算法

1.4、相似度

1.5、数据间的相似度


每一条数据都可以理解为多维空间中的一个点。


可以根据点和点之间的距离来评价数据间的相似度


近朱者赤近墨者黑！


欧氏距离：

二维空间：

d=(x1−x2)2+(y1−y2)2d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}d=(x1​−x2​)2+(y1​−y2​)2​

三维空间：

d=(x1−x2)2+(y1−y2)2+(z1−z2)2d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 + (z_1 - z_2)^2}d=(x1​−x2​)2+(y1​−y2​)2+(z1​−z2​)2​

闵可夫斯基距离

d(X,Y)=∣x1−y1∣p+∣x2−y2∣p+…+∣xn−yn∣ppd(X,Y) = \sqrt[p]{|x_1 - y_1|^p + |x_2 - y_2|^p + … + |x_n - y_n|^p}d(X,Y)=p∣x1​−y1​∣p+∣x2​−y2​∣p+…+∣xn​−yn​∣p​


P = 1 曼哈顿距离

d(X,Y)=∣x1−y1∣+∣x2−y2∣+…+∣xn−yn∣d(X,Y) = |x_1 - y_1| + |x_2 - y_2| + … + |x_n - y_n|d(X,Y)=∣x1​−y1​∣+∣x2​−y2​∣+…+∣xn​−yn​∣
在二维空间中可以看出，这种距离是计算两点之间的直角边距离，相当于城市中出租汽车沿城市街道拐直角前进而不能走两点连接间的最短距离。



P = 2 欧氏距离，就是两点之间的直线距离（以下简称欧氏距离）


P = 无穷，切比雪夫距离 ，哪个维度差值最大就是哪个差值作为距离。



1.6、余弦距离
将数据看做空间中的点的时候，评价远近可以用欧氏距离或者余弦距离。
计算过程如下：

将数据映射为高维空间中的点（向量）
计算向量间的余弦值
取值范围[-1,+1] 越趋近于1代表越相似，越趋近于-1代表方向相反，0代表正交

cosθ=a⋅b∣∣a∣∣2∣∣b∣∣2cos\theta = \frac{a \cdot b}{||a||_2||b||_2}cosθ=∣∣a∣∣2​∣∣b∣∣2​a⋅b​
cosθ=x1x2+y1y2x12+y12×x22+y22cos\theta = \frac{x_1x_2 + y_1y_2}{\sqrt{x_1^2 + y_1^2} \times \sqrt{x_2^2 + y_2^2}}cosθ=x12​+y12​​×x22​+y22​​x1​x2​+y1​y2​​

余弦相似度可以评价文章的相似度，从而实现对文章，进行分类。

2、Kmeans
2.1、聚类原理


将N个样本映射到K个簇中


每个簇至少有一个样本


基本思路：


先给定K个划分，迭代样本与簇的隶属关系，每次都比前一次好一些


迭代若干次就能得到比较好的结果


2.2、Kmeans算法原理
算法步骤：


选择K个初始的簇中心

怎么选？



逐个计算每个样本到簇中心的距离，将样本归属到距离最小的那个簇中心的簇中


每个簇内部计算平均值，更新簇中心


开始迭代



聚类过程如下：

2.3、KMeans优缺点


优点：

简单，效果不错



缺点

对异常值敏感
对初始值敏感
对某些分布聚类效果不好



2.4、Kmeans损失函数
∑i=0nmin⁡μj∈C(∣∣xi−μj∣∣2)\sum\limits_{i=0}^{n}\underset{\mu_j \in C}\min(||x_i - \mu_j||^2)i=0∑n​μj​∈Cmin​(∣∣xi​−μj​∣∣2)


其中μj=1∣Cj∣∑x∈Cjx\mu_j = \frac{1}{|C_j|}\sum\limits_{x \in C_j}xμj​=∣Cj​∣1​x∈Cj​∑​x是簇的均值向量，或者说是质心。


其中∣∣xi−μj∣∣2||x_i - \mu_j||^2∣∣xi​−μj​∣∣2代表每个样本点到均值点的距离（其实也是范数）。


2.5、Kmeans执行过程

2.6、Kmeans初步使用
亚洲国家队划分：
import numpy as npimport matplotlib.pyplot as pltfrom sklearn.cluster import KMeansimport pandas as pd# 需要将亚洲国家队，分成三个类别# 只有历年的统计数据，没有目标值（类别，等级）data = pd.read_csv(&#x27;./AsiaFootball.txt&#x27;)data# 执行多次，分类结果会有所不同kmeans = KMeans(n_clusters=3)# 无监督，只需要给数据X就可以kmeans.fit(data.iloc[:,1:])y_ = kmeans.predict(data.iloc[:,1:])# 聚类算法预测、划分的类别c = data[&#x27;国家&#x27;].valuesfor i in range(3):    cond = y_ == i#索引条件    print(&#x27;类别是%d的国家有：&#x27;%(i),c[cond])

类别是0的国家有： [&#x27;中国&#x27; &#x27;伊拉克&#x27; &#x27;卡塔尔&#x27; &#x27;阿联酋&#x27; &#x27;泰国&#x27; &#x27;越南&#x27; &#x27;阿曼&#x27; &#x27;印尼&#x27;]类别是1的国家有： [&#x27;伊朗&#x27; &#x27;沙特&#x27; &#x27;乌兹别克斯坦&#x27; &#x27;巴林&#x27; &#x27;朝鲜&#x27;]类别是2的国家有： [&#x27;日本&#x27; &#x27;韩国&#x27;]

数据可视化：
plt.rcParams[&#x27;font.family&#x27;] = &#x27;STKaiti&#x27;fig = plt.figure(figsize=(12,9))ax = plt.axes(projection = &#x27;3d&#x27;)ax.scatter(data.iloc[:,1],data.iloc[:,2],data.iloc[:,3],c = y_,s = 100)ax.set_xlabel(&#x27;2006年世界杯&#x27;,fontsize = 18)ax.set_ylabel(&#x27;2010年世界杯&#x27;,fontsize = 18)ax.set_zlabel(&#x27;2007亚洲杯&#x27;,fontsize = 18)# 调整视图角度ax.view_init(elev = 20,azim = -60)

2.7、Kmeans聚类算法K值选择
轮廓系数：
针对某个样本的轮廓系数s为：s=b−amax(a,b)s= \frac{b - a}{max(a, b)}s=max(a,b)b−a​

a：某个样本与其所在簇内其他样本的平均距离
b：某个样本与其他簇样本的平均距离

聚类总的轮廓系数SC为：SC=1N∑i=1NsiSC = \frac{1}{N}\sum\limits_{i = 1}^Ns_iSC=N1​i=1∑N​si​，所有样本的sis_isi​的均值称为聚类结果的轮廓系数，是该聚类是否合理、有效的度量。
import numpy as npfrom sklearn import datasetsfrom sklearn.cluster import KMeansimport matplotlib.pyplot as plt# 聚类：轮廓系数，对聚类的评价指标，对应数学公式from sklearn.metrics import silhouette_score# 创建数据# 假数据，数据X划分成3类X,y = datasets.make_blobs(centers=3)plt.scatter(X[:,0],X[:,1],c = y)# 指定不同的k，寻找最佳聚类类别数目# 可以画图，一目了然，数据简单，属性只有两个，所以可以画图# 属性多，无法可视化，评价指标# 轮廓系数plt.rcParams[&#x27;font.sans-serif&#x27;] = &#x27;KaiTi&#x27;plt.rcParams[&#x27;font.size&#x27;] = 18plt.rcParams[&#x27;axes.unicode_minus&#x27;] = Falsescore = []for i in range(2,7):    kmeans = KMeans(n_clusters=i)    kmeans.fit(X)    y_ = kmeans.predict(X)# 预测类别 == 标签#     plt.scatter(X[:,0],X[:,1],c = y_)    score.append(silhouette_score(X,y_))#     print(&#x27;当聚类类别是6的时候，评价指标轮廓系数： &#x27;,silhouette_score(X,y_))plt.plot(range(2,7),score)plt.xlabel(&#x27;K值&#x27;)plt.ylabel(&#x27;轮廓系数&#x27;,c = &#x27;red&#x27;)# 结论：，当k值是3的时候，轮廓系数最大，这个时候，说明划分效果最好！
效果图如下：（注意数据随机生成，数据展示的图片效果不是固定的~）


调整兰德系数：
公式了解一下即可：
RI=a+bC2nsamples\text{RI} = \frac{a + b}{C_2^{n_{samples}}}RI=C2nsamples​​a+b​
用C表示实际的类别划分，K表示聚类结果。
定义a 为在C中被划分为同一类，在K中被划分为同一簇的实例对数量。
定义b为在C中被划分为不同类别，在K中被划分为不同簇的实例对数量。
兰德系数范围是[0,1]
调整兰德系数为：
ARI=RI−E[RI]max⁡(RI)−E[RI]\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}ARI=max(RI)−E[RI]RI−E[RI]​
范围为[-1,1]
from sklearn.metrics import adjusted_rand_score# 调整兰德系数score = []for i in range(2,7):    kmeans = KMeans(n_clusters=i)    kmeans.fit(X)    y_ = kmeans.predict(X)# 预测类别 == 标签    score.append(adjusted_rand_score(y,y_))plt.plot(range(2,7),score)plt.xlabel(&#x27;K值&#x27;)plt.ylabel(&#x27;调整兰德系数&#x27;,c = &#x27;red&#x27;)# 结论：，当k值是3的时候，轮廓系数最大，这个时候，说明划分效果最好！

2.8、Kmeans图像压缩
import matplotlib.pyplot as plt # plt 用于显示图片from sklearn.cluster import KMeansimport numpy as npplt.figure(figsize=(8,4))# 加载图片显示原图pixel = plt.imread(&#x27;11-bird.png&#x27;)plt.subplot(1,2,1)plt.imshow(pixel)# 聚类运算，压缩图片pixel = pixel.reshape((128*128 , 3))kmeans = KMeans(n_clusters=8).fit(pixel)# 聚类结果合成新图片newPixel = kmeans.cluster_centers_[kmeans.labels_].reshape(128,128,3)plt.subplot(1,2,2)plt.imshow(newPixel)

3、DBSCAN
3.1、算法介绍
DBSCAN（Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法）是一种基于密度的空间聚类算法。 该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。

下面这些点是分布在样本空间的众多样本，现在我们的目标是把这些在样本空间中距离相近的聚成一类。我们发现A点附近的点密度较大，红色的圆圈根据一定的规则在这里滚啊滚，最终收纳了A附近的5个点，标记为红色也就是定为同一个簇。其它没有被收纳的根据一样的规则成簇。（形象来说，我们可以认为这是系统在众多样本点中随机选中一个，围绕这个被选中的样本点画一个圆，规定这个圆的半径以及圆内最少包含的样本点，如果在指定半径内有足够多的样本点在内，那么这个圆圈的圆心就转移到这个内部样本点，继续去圈附近其它的样本点，类似传销一样，继续去发展下线。等到这个滚来滚去的圈发现所圈住的样本点数量少于预先指定的值，就停止了。那么我们称最开始那个点为核心点，如A，停下来的那个点为边界点，如B、C，没得滚的那个点为离群点，如N）。

基于密度这点有什么好处呢，我们知道Kmeans聚类算法只能处理球形的簇，也就是一个聚成实心的团（这是因为算法本身计算平均距离的局限）。但往往现实中还会有各种形状，比如下面两张图，环形和不规则形，这个时候，那些传统的聚类算法显然就悲剧了。于是就思考，样本密度大的成一类呗。这就是DBSCAN聚类算法。
3.2、DBSCAN算法原理

网站动画解析
3.3、DBSCAN参数解析
参数一eps：
DBSCAN算法参数，即我们的eps邻域的距离阈值，和样本距离超过eps的样本点不在eps邻域内。默认值是0.5.一般需要通过在多组值里面选择一个合适的阈值。eps过大，则更多的点会落在核心对象的eps邻域，此时我们的类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。
参数二min_samples：
DBSCAN算法参数，即样本点要成为核心对象所需要的eps邻域的样本数阈值。默认值是5。一般需要通过在多组值里面选择一个合适的阈值。通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。
参数与三metrics：

3.4、DBSCAN使用示例
import numpy as npfrom sklearn import datasetsfrom sklearn.cluster import KMeans,DBSCANimport matplotlib.pyplot as plt# y中是两类：0,1X,y = datasets.make_circles(n_samples=1000,noise=0.05,factor = 0.5)# centers = [(1.5,1.5)] 元组，代表着，中心点的坐标值# y1一类：0 + 2X1,y1 = datasets.make_blobs(n_samples=500,n_features=2,centers=[(1.5,1.5)],cluster_std=0.2)# 将circle和散点进行了数据合并X = np.concatenate([X,X1])y = np.concatenate([y,y1 + 2])plt.scatter(X[:,0],X[:,1],c = y)

# 根据距离，划分‘势力范围’kmeans = KMeans(3)kmeans.fit(X)y_ = kmeans.labels_plt.scatter(X[:,0],X[:,1],c = y_)

dbscan = DBSCAN(eps = 0.2,min_samples=3)dbscan.fit(X)y_ = dbscan.labels_plt.scatter(X[:,0],X[:,1],c = y_)

4、分层聚类
4.1、算法介绍
分层聚类输出层次结构，这种结构比平面聚类返回的非结构化聚类集更具信息性。
分层聚类法（hierarchical cluster method）一译“系统聚类法”。聚类分析的一种方法。其做法是开始时把每个样品作为一类，然后把最靠近的样品（即距离最小的群品）首先聚为小类，再将已聚合的小类按其类间距离再合并，不断继续下去，最后把一切子类都聚合到一个大类。

一般来说，当考虑聚类效率时，我们选择平面聚类，当平面聚类的潜在问题（不够结构化，预定数量的聚类，非确定性）成为关注点时，我们选择层次聚类。 此外，许多研究人员认为，层次聚类比平面聚类产生更好的聚类。

4.2、算法原理
层次聚类(Hierarchical Clustering)是聚类算法的一种，通过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最低层，树的顶层是一个聚类的根节点。创建聚类树有自下而上合并和自上而下分裂两种方法。
我们着重看一下自底向上的合并算法：

两个组合数据点间的距离:

Single Linkage

方法是将两个组合数据点中距离最近的两个数据点间的距离作为这两个组合数据点的距离。这种方法容易受到极端值的影响。两个很相似的组合数据点可能由于其中的某个极端的数据点距离较近而组合在一起。


Complete Linkage

complete Linkage的计算方法与Single Linkage相反，将两个组合数据点中距离最远的两个数据点间的距离作为这两个组合数据点的距离。Complete Linkage的问题也与Single Linkage相反，两个不相似的组合数据点可能由于其中的极端值距离较远而无法组合在一起。


Average Linkage

Average Linkage的计算方法是计算两个组合数据点中的每个数据点与其他所有数据点的距离。将所有距离的均值作为两个组合数据点间的距离。这种方法计算量比较大，但结果比前两种方法更合理。
我们使用Average Linkage计算组合数据点间的距离。下面是计算组合数据点(A,F)到(B,C)的距离，这里分别计算了(A,F)和(B,C)两两间距离的均值。




4.3、参数介绍


n_clusters
划分类别数目


linkage
度量两个子类的相似度时所依据的距离

Single Linkage：将两个数据点集中距离最近的两个数据点间的距离作为这两个点集的距离。
Complete Linkage：将两个点集中距离最远的两个数据点间的距离作为这两个点集的距离。
上述两种方法容易受到极端值的影响，计算大样本集效率较高。
Average Linkage：计算两个点集中的每个数据点与其他所有数据点的距离。将所有距离的均值作为两个点集间的距离。这种方法计算量比较大，不过这种度量方法更合理
Ward：最小化簇内方差。



connectivity
连接性约束，作用：只有相邻的簇才能合并在一起，进行聚类！


4.4、算法案例
导包
import numpy as npimport matplotlib.pyplot as pltimport mpl_toolkits.mplot3d.axes3d as p3from sklearn.cluster import AgglomerativeClusteringfrom sklearn.datasets import make_swiss_roll
创建数据
X,y = datasets.make_swiss_roll(n_samples=1500,noise = 0.05)fig = plt.figure(figsize=(12,9))a3 = fig.add_subplot(projection = &#x27;3d&#x27;)a3.scatter(X[:,0],X[:,1],X[:,2],c = y)a3.view_init(10,-80)

Kmeans聚类效果
# Kmeans只负责分类，随机性，类别是数字几，不固定clf = KMeans(n_clusters=6)clf.fit(X)y_ = clf.labels_fig = plt.figure(figsize=(12,9))a3 = plt.subplot(projection = &#x27;3d&#x27;)a3.scatter(X[:,0],X[:,1],X[:,2],c = y_)a3.view_init(10,-80)

分层聚类
agg = AgglomerativeClustering(n_clusters=6,linkage=&#x27;ward&#x27;)# 最近的距离，作为标准，agg.fit(X)y_ = agg.labels_fig = plt.figure(figsize=(12,9))a3 = plt.subplot(projection = &#x27;3d&#x27;)a3.scatter(X[:,0],X[:,1],X[:,2],c = y_)a3.view_init(10,-80)

对于这种非欧几何的数据下，可见如果没有设置连接性约束，将会忽视其数据本身的结构，强制在欧式空间下聚类，于是很容易形成了上图这种跨越流形的不同褶皱。
分层聚类改进（连接性约束，对局部结构进行约束）
from sklearn.neighbors import kneighbors_graph# graph图形的意思# 邻居数量变少，认为，条件宽松conn = kneighbors_graph(X,n_neighbors=10) #采用邻居，进行约束agg = AgglomerativeClustering(n_clusters=6,connectivity=conn,linkage=&#x27;ward&#x27;)# 最近的距离，作为标准，agg.fit(X)y_ = agg.labels_fig = plt.figure(figsize=(12,9))a3 = fig.add_subplot(projection = &#x27;3d&#x27;)a3.scatter(X[:,0],X[:,1],X[:,2],c = y_)a3.view_init(10,-80)

]]></content>
      <categories>
        <category>program</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>闲言碎语</title>
    <url>/2.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>about</title>
    <url>/about/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>音乐馆</title>
    <url>/3.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>分类</title>
    <url>/g.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>categories</title>
    <url>/categories/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/manifest.json</url>
    <content><![CDATA[{"name":"molittle","short_name":"molittle","theme_color":"#3b70fc","background_color":"#3b70fc","display":"standalone","scope":"/","start_url":"/","icons":[{"src":"/img/siteicon/16.png","sizes":"16x16","type":"image/png"},{"src":"/img/siteicon/32.png","sizes":"32x32","type":"image/png"},{"src":"/img/siteicon/48.png","sizes":"48x48","type":"image/png"},{"src":"/img/siteicon/64.png","sizes":"64x64","type":"image/png"},{"src":"/img/siteicon/128.png","sizes":"128x128","type":"image/png"},{"src":"/img/siteicon/144.png","sizes":"144x144","type":"image/png"},{"src":"/img/siteicon/512.png","sizes":"512x512","type":"image/png"}],"splash_pages":null}]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/css/custom.css</url>
    <content><![CDATA[/* @font-face {
  font-family: Candyhome;
  src: url(https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/Candyhome.ttf);
  font-display: swap;
  font-weight: lighter;
} */
@font-face {
    font-family: ZhuZiAYuanJWD;
    src: url(https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/ZhuZiAWan.woff2);
    font-display: swap;
    font-weight: lighter;
  }
  
  div#menus {
    font-family: "ZhuZiAYuanJWD";
  }
  h1#site-title {
    font-family: ZhuZiAYuanJWD;
    font-size: 3em !important;
  }
  a.blog-slider__title,
  a.categoryBar-list-link,
  h1.post-title {
    font-family: ZhuZiAYuanJWD;
  }
  article {
    max-width: 500px; /* 按需调整宽度，控制文章条整体长度 */
    margin-left: auto; /* 添加这一行，让文章内容靠右显示 */
  }
  article .article-title {
    font-size: 18px; /* 适当缩小标题字体大小 */
    max-width: 100%;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap; /* 标题过长时省略显示 */
  }

  a.blog-slider__title,
  a.categoryBar-list-link,
  h1.post-title {
    font-family: ZhuZiAYuanJWD;
  }
/* 文章容器样式 */
.post-body {
    max-width: 800px; /* 设置最大宽度 */
    margin: 0 auto; /* 水平居中 */
    padding: 20px; /* 添加内边距 */
    display: flex; /* 使用 Flexbox 布局 */
    flex-direction: column; /* 垂直排列子元素 */
    align-items: center; /* 水平居中子元素 */
}

/* 文章标题样式 */
.post-title {
    text-align: center; /* 居中对齐标题 */
}

/* 文章内容样式 */
.post-content {
    text-align: justify; /* 两端对齐文本 */
}
/* 文章内容居中显示 */
.post-body {
    max-width: 800px; /* 根据需要调整最大宽度 */
    margin: 0 auto; /* 水平居中 */
    padding: 20px; /* 添加内边距 */
}

/* 调整文章与目录的间隔 */
.post-body,
.toc {
    margin-bottom: 20px; /* 根据需要调整间隔 */
}
/* Remove any default margins or padding that might be causing the gap */
body {
    margin: 0;
    padding: 0;
}

/* Ensure the article container is centered and takes full width */
#article-container {
    max-width: 800px; /* Adjust this value as needed */
    margin: 0 auto; /* Centers the article */
    padding: 20px;
    width: 100%;
    box-sizing: border-box;
}

/* Remove any extra spacing around the article */


/* Adjust the table of contents if necessary */
#toc {
    margin: 0 auto; /* Centers the TOC */
    max-width: 800px; /* Match the article width */
    padding: 20px;
    box-sizing: border-box;
}

/* Ensure the post content is properly aligned */
div#post {
    display: flex;
    flex-direction: column;
    align-items: center;
    width: 100%;
    margin: 0;
    padding: 0;
}
  article .article-title {
    font-size: 18px; /* 适当缩小标题字体大小 */
    max-width: 100%;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap; /* 标题过长时省略显示 */
  }
  article .article-entry {
    width: 600px; /* 缩短文章条目宽度，可按需调整 */
    float: left; /* 使其固定在左侧 */
    margin-right: 10px; /* 与右侧元素间隔一定距离 */
  }
  article .article-excerpt {
    font-size: 14px; /* 缩小摘要字体大小 */
    line-height: 1.4; /* 调整行间距 */
}
  article .article-meta {
    width: 100%; /* 元信息（日期、分类等）占满剩余宽度 */
    font-size: 12px; /* 适当缩小元信息字体大小 */
  }
  .iconfont {
    font-family: "iconfont" !important;
    font-size: 3em;
    /* 可以定义图标大小 */
    font-style: normal;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
  }
  
  /* 时间轴生肖icon */
  svg.icon {
    /* 这里定义svg.icon，避免和Butterfly自带的note标签冲突 */
    width: 1em;
    height: 1em;
    /* width和height定义图标的默认宽度和高度*/
    vertical-align: -0.15em;
    fill: currentColor;
    overflow: hidden;
  }
  
  .icon-zhongbiao::before {
    color: #f7c768;
  }
  
  /* bilibli番剧插件 */
  #article-container .bangumi-tab.bangumi-active {
    background: var(--anzhiyu-theme);
    color: var(--anzhiyu-ahoverbg);
    border-radius: 10px;
  }
  a.bangumi-tab:hover {
    text-decoration: none !important;
  }
  .bangumi-button:hover {
    background: var(--anzhiyu-theme) !important;
    border-radius: 10px !important;
    color: var(--anzhiyu-ahoverbg) !important;
  }
  a.bangumi-button.bangumi-nextpage:hover {
    text-decoration: none !important;
  }
  .bangumi-button {
    padding: 5px 10px !important;
  }
  
  a.bangumi-tab {
    padding: 5px 10px !important;
  }
  svg.icon.faa-tada {
    font-size: 1.1em;
  }
  .bangumi-info-item {
    border-right: 1px solid #f2b94b;
  }
  .bangumi-info-item span {
    color: #f2b94b;
  }
  .bangumi-info-item em {
    color: #f2b94b;
  }
  
  /* 解决artitalk的图标问题 */
  #uploadSource > svg {
    width: 1.19em;
    height: 1.5em;
  }
  
  /*top-img黑色透明玻璃效果移除，不建议加，除非你执着于完全一图流或者背景图对比色明显 */
  #page-header:not(.not-top-img):before {
    background-color: transparent !important;
  }
  
  /* 首页文章卡片 */
  #recent-posts > .recent-post-item {
    background: rgba(255, 255, 255, 0.9);
  }
  
  /* 首页侧栏卡片 */
  #aside-content .card-widget {
    background: rgba(255, 255, 255, 0.9);
  }
  
  /* 文章页面正文背景 */
  div#post {
    background: rgba(255, 255, 255, 0.9);
  }
  
  /* 分页页面 */
  div#page {
    background: rgba(255, 255, 255, 0.9);
  }
  
  /* 归档页面 */
  div#archive {
    background: rgba(255, 255, 255, 0.9);
  }
  
  /* 标签页面 */
  div#tag {
    background: rgba(255, 255, 255, 0.9);
  }
  
  /* 分类页面 */
  div#category {
    background: rgba(255, 255, 255, 0.9);
  }
  
  /*夜间模式伪类遮罩层透明*/
  [data-theme="dark"] #recent-posts > .recent-post-item {
    background: #121212;
  }
  
  [data-theme="dark"] .card-widget {
    background: #121212 !important;
  }
  
  [data-theme="dark"] div#post {
    background: #121212 !important;
  }
  
  [data-theme="dark"] div#tag {
    background: #121212 !important;
  }
  
  [data-theme="dark"] div#archive {
    background: #121212 !important;
  }
  
  [data-theme="dark"] div#page {
    background: #121212 !important;
  }
  
  [data-theme="dark"] div#category {
    background: #121212 !important;
  }
  
  /* 页脚透明 */
  #footer {
    background: transparent !important;
  }
  
  /* 头图透明 */
  #page-header {
    background: transparent !important;
  }
  
  #rightside > div > button {
    border-radius: 5px;
  }
  
  /* 滚动条 */
  
  ::-webkit-scrollbar {
    width: 10px;
    height: 10px;
  }
  
  ::-webkit-scrollbar-thumb {
    background-color: #3b70fc;
    border-radius: 2em;
  }
  
  ::-webkit-scrollbar-corner {
    background-color: transparent;
  }
  
  ::-moz-selection {
    color: #fff;
    background-color: #3b70fc;
  }
  
  /* 音乐播放器 */
  
  /* .aplayer .aplayer-lrc {
    display: none !important;
  } */
  
  .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
    left: -66px !important;
    transition: all 0.3s;
    /* 默认情况下缩进左侧66px，只留一点箭头部分 */
  }
  
  .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
    left: 0 !important;
    transition: all 0.3s;
    /* 鼠标悬停是左侧缩进归零，完全显示按钮 */
  }
  
  .aplayer.aplayer-fixed {
    z-index: 999999 !important;
  }
  
  /* 评论框  */
  .vwrap {
    box-shadow: 2px 2px 5px #bbb;
    background: rgba(255, 255, 255, 0.3);
    border-radius: 8px;
    padding: 30px;
    margin: 30px 0px 30px 0px;
  }
  
  /* 设置评论框 */
  
  .vcard {
    box-shadow: 2px 2px 5px #bbb;
    background: rgba(255, 255, 255, 0.3);
    border-radius: 8px;
    padding: 30px;
    margin: 30px 0px 0px 0px;
  }
  
  /* md网站下划线 */
  #article-container a:hover {
    text-decoration: none !important;
  }
  
  #article-container #hpp_talk p img {
    display: inline;
  }
  
  /* 404页面 */
  #error-wrap {
    position: absolute;
    top: 40%;
    right: 0;
    left: 0;
    margin: 0 auto;
    padding: 0 1rem;
    max-width: 1000px;
    transform: translate(0, -50%);
  }
  
  #error-wrap .error-content {
    display: flex;
    flex-direction: row;
    justify-content: center;
    align-items: center;
    margin: 0 1rem;
    height: 18rem;
    border-radius: 8px;
    background: var(--card-bg);
    box-shadow: var(--card-box-shadow);
    transition: all 0.3s;
  }
  
  #error-wrap .error-content .error-img {
    box-flex: 1;
    flex: 1;
    height: 100%;
    border-top-left-radius: 8px;
    border-bottom-left-radius: 8px;
    background-color: #3b70fc;
    background-position: center;
    background-size: cover;
  }
  
  #error-wrap .error-content .error-info {
    box-flex: 1;
    flex: 1;
    padding: 0.5rem;
    text-align: center;
    font-size: 14px;
    font-family: Titillium Web, "PingFang SC", "Hiragino Sans GB", "Microsoft JhengHei", "Microsoft YaHei", sans-serif;
  }
  #error-wrap .error-content .error-info .error_title {
    margin-top: -4rem;
    font-size: 9em;
  }
  #error-wrap .error-content .error-info .error_subtitle {
    margin-top: -3.5rem;
    word-break: break-word;
    font-size: 1.6em;
  }
  #error-wrap .error-content .error-info a {
    display: inline-block;
    margin-top: 0.5rem;
    padding: 0.3rem 1.5rem;
    background: var(--btn-bg);
    color: var(--btn-color);
  }
  
  #body-wrap.error .aside-list {
    display: flex;
    flex-direction: row;
    flex-wrap: nowrap;
    bottom: 0px;
    position: absolute;
    padding: 1rem;
    width: 100%;
    overflow: hidden;
  }
  
  #body-wrap.error .aside-list .aside-list-group {
    display: flex;
    flex-direction: row;
    flex-wrap: nowrap;
    max-width: 1200px;
    margin: 0 auto;
  }
  
  #body-wrap.error .aside-list .aside-list-item {
    padding: 0.5rem;
  }
  
  #body-wrap.error .aside-list .aside-list-item img {
    width: 100%;
    object-fit: cover;
    border-radius: 12px;
  }
  
  #body-wrap.error .aside-list .aside-list-item .thumbnail {
    overflow: hidden;
    width: 230px;
    height: 143px;
    background: var(--anzhiyu-card-bg);
    display: flex;
  }
  
  #body-wrap.error .aside-list .aside-list-item .content .title {
    overflow: hidden;
    display: -webkit-box;
    -webkit-box-orient: vertical;
    line-height: 1.5;
    justify-content: center;
    align-items: flex-end;
    align-content: center;
    padding-top: 0.5rem;
    color: white;
  }
  
  #body-wrap.error .aside-list .aside-list-item .content time {
    display: none;
  }
  
  /* 代码框主题 */
  #article-container figure.highlight {
    border-radius: 10px;
  }
  .pace {
    -webkit-pointer-events: none;
    pointer-events: none;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
    z-index: 2000;
    position: fixed;
    margin: auto;
    top: 10px;
    left: 0;
    right: 0;
    height: 8px;
    border-radius: 8px;
    width: 4rem;
    background: #eaecf2;
    border: 1px #e3e8f7;
    overflow: hidden;
  }
  
  .pace-inactive .pace-progress {
    opacity: 0;
    transition: 0.3s ease-in;
  }
  
  .pace .pace-progress {
    -webkit-box-sizing: border-box;
    -moz-box-sizing: border-box;
    -ms-box-sizing: border-box;
    -o-box-sizing: border-box;
    box-sizing: border-box;
    -webkit-transform: translate3d(0, 0, 0);
    -moz-transform: translate3d(0, 0, 0);
    -ms-transform: translate3d(0, 0, 0);
    -o-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    max-width: 200px;
    position: absolute;
    z-index: 2000;
    display: block;
    top: 0;
    right: 100%;
    height: 100%;
    width: 100%;
    background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab);
    animation: gradient 1.5s ease infinite;
    background-size: 200%;
  }
  
  .pace.pace-inactive {
    opacity: 0;
    transition: 0.3s;
    top: -8px;
  }
  @keyframes gradient {
    0% {
      background-position: 0% 50%;
    }
    50% {
      background-position: 100% 50%;
    }
    100% {
      background-position: 0% 50%;
    }
  }
/*!
  Theme: GitHub Dark
  Description: Dark theme as seen on github.com
  Author: github.com
  Maintainer: @Hirse
  Updated: 2021-05-15

  Outdated base version: https://github.com/primer/github-syntax-dark
  Current colors taken from GitHub's CSS
*/
/* 新添加的內容
  ------------------------------------- 
  --hl-color                  代碼框字體顔色 【必須】 (把下面.hljs的 color複製到這裏來)
  --hl-bg                     代碼框背景色 【必須】 (把下面.hljs的 background複製到這裏來)
  --hltools-bg: #321a0f       代碼框頂部工具欄背景色 【可選】(如果你關掉了 copy、lang 和 shrink,可不用配置這個）
  --hltools-color: #fff       代碼框頂部工具欄字體顔色 【可選】(如果你關掉了 copy、lang 和 shrink,可不用配置這個）
  --hlnumber-bg: #221a0f      代碼框行數背景色 【可選】(如果已經關掉 line_number,可以不用配置這個)
  --hlnumber-color: #fff      代碼框行數字體顔色 【可選】 (如果已經關掉 line_number,可以不用配置這個)
  --hlscrollbar-bg: #d3af86   代碼框滾動條顔色 【可選】（默認為主題主顔色）
  --hlexpand-bg: #d3af86      代碼框底部展開背景色 【可選】(如果已經關掉 highlight_height_limit,可以不用配置這個)
*/

:root {
  --hl-color: #c9d1d9;
  --hl-bg: #0d1117;
  --hltools-bg: #110672;
  --hltools-color: #fff;
  --hlnumber-bg: #0d1117;
  --hlnumber-color: #fff;
  --hlscrollbar-bg: #0d1117;
  --hlexpand-bg: #e1e4eb;
}
pre code.hljs {
  display: block;
  overflow-x: auto;
  padding: 1em
}
code.hljs {
  padding: 3px 5px
}
/*!
  Theme: GitHub Dark
  Description: Dark theme as seen on github.com
  Author: github.com
  Maintainer: @Hirse
  Updated: 2021-05-15

  Outdated base version: https://github.com/primer/github-syntax-dark
  Current colors taken from GitHub's CSS
*/
#article-container figure.highlight .hljs {
  color: #c9d1d9;
  background: #0d1117
}
.hljs-doctag,
.hljs-keyword,
.hljs-meta .hljs-keyword,
.hljs-template-tag,
.hljs-template-variable,
.hljs-type,
.hljs-variable.language_ {
  /* prettylights-syntax-keyword */
  color: #ff7b72
}
.hljs-title,
.hljs-title.class_,
.hljs-title.class_.inherited__,
.hljs-title.function_ {
  /* prettylights-syntax-entity */
  color: #d2a8ff
}
.hljs-attr,
.hljs-attribute,
.hljs-literal,
.hljs-meta,
.hljs-number,
.hljs-operator,
.hljs-variable,
.hljs-selector-attr,
.hljs-selector-class,
.hljs-selector-id {
  /* prettylights-syntax-constant */
  color: #79c0ff
}
.hljs-regexp,
.hljs-string,
.hljs-meta .hljs-string {
  /* prettylights-syntax-string */
  color: #a5d6ff
}
.hljs-built_in,
.hljs-symbol {
  /* prettylights-syntax-variable */
  color: #ffa657
}
.hljs-comment,
.hljs-code,
.hljs-formula {
  /* prettylights-syntax-comment */
  color: #8b949e
}
.hljs-name,
.hljs-quote,
.hljs-selector-tag,
.hljs-selector-pseudo {
  /* prettylights-syntax-entity-tag */
  color: #7ee787
}
.hljs-subst {
  /* prettylights-syntax-storage-modifier-import */
  color: #c9d1d9
}
.hljs-section {
  /* prettylights-syntax-markup-heading */
  color: #1f6feb;
  font-weight: bold
}
.hljs-bullet {
  /* prettylights-syntax-markup-list */
  color: #f2cc60
}
.hljs-emphasis {
  /* prettylights-syntax-markup-italic */
  color: #c9d1d9;
  font-style: italic
}
.hljs-strong {
  /* prettylights-syntax-markup-bold */
  color: #c9d1d9;
  font-weight: bold
}
.hljs-addition {
  /* prettylights-syntax-markup-inserted */
  color: #aff5b4;
  background-color: #033a16
}
.hljs-deletion {
  /* prettylights-syntax-markup-deleted */
  color: #ffdcd7;
  background-color: #67060c
}
.hljs-char.escape_,
.hljs-link,
.hljs-params,
.hljs-property,
.hljs-punctuation,
.hljs-tag {
  /* purposely ignored */
  
}
]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/css/fuction.css</url>
    <content><![CDATA[.loading-img {
  background: url(https://s21.ax1x.com/2025/05/12/pEXkRIO.jpg) no-repeat center center;
  background-size: cover;
}
/* 返回顶部 */


button#go-up span:nth-child(2) {
    font-size: 12px!important;
    margin-right: -1px;
}
/* 返回顶部 */
button#go-up #percent {
    display: none;
    /*font-weight: bold;*/
    font-size: 16px !important;
}

/* 鼠标滑动到按钮上时显示返回顶部图标 */
button#go-up:hover i {
    display: block !important;
}

button#go-up:hover #percent {
    display: none !important;
}

#rightside > div > button,
#rightside > div > a {
    border-radius: 50%;
    display: flex;
    align-items: center;
    -webkit-box-align: center;
    -webkit-box-pack: center;
    justify-content: center;
}
]]></content>
  </entry>
  <entry>
    <title>artitalk</title>
    <url>/artitalk/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/css/progress_bar.css</url>
    <content><![CDATA[.pace {
  -webkit-pointer-events: none;
  pointer-events: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  user-select: none;
  z-index: 2000;
  position: fixed;
  margin: auto;
  top: 10px;
  left: 0;
  right: 0;
  height: 8px;
  border-radius: 8px;
  width: 4rem;
  background: #eaecf2;
  border: 1px #e3e8f7;
  overflow: hidden;
}

.pace-inactive .pace-progress {
  opacity: 0;
  transition: 0.3s ease-in;
}

.pace .pace-progress {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  -ms-box-sizing: border-box;
  -o-box-sizing: border-box;
  box-sizing: border-box;
  -webkit-transform: translate3d(0, 0, 0);
  -moz-transform: translate3d(0, 0, 0);
  -ms-transform: translate3d(0, 0, 0);
  -o-transform: translate3d(0, 0, 0);
  transform: translate3d(0, 0, 0);
  max-width: 200px;
  position: absolute;
  z-index: 2000;
  display: block;
  top: 0;
  right: 100%;
  height: 100%;
  width: 100%;
  background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab);
  animation: gradient 1.5s ease infinite;
  background-size: 200%;
}

.pace.pace-inactive {
  opacity: 0;
  transition: 0.3s;
  top: -8px;
}
@keyframes gradient {
  0% {
    background-position: 0% 50%;
  }
  50% {
    background-position: 100% 50%;
  }
  100% {
    background-position: 0% 50%;
  }
}]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/css/swiperstyle.css</url>
    <content><![CDATA[* {
    -webkit-box-sizing:border-box;
    -moz-box-sizing:border-box;
    box-sizing:border-box
  }
  
  div#swiper_container {
    background:rgba(255, 255, 255, 0);
    width: 57%;
    padding: 0;
    overflow: initial;
  }
  .blog-slider {
    width:100%;
    position:relative;
    border-radius:12px 8px 8px 12px;
    margin:auto;
    background:var(--global-bg);
    padding:10px;
    -webkit-transition:all .3s;
    -moz-transition:all .3s;
    -o-transition:all .3s;
    -ms-transition:all .3s;
    transition:all .3s
  }
  .blog-slider__item {
    display:-webkit-box;
    display:-moz-box;
    display:-webkit-flex;
    display:-ms-flexbox;
    display:box;
    display:flex;
    -webkit-box-align:center;
    -moz-box-align:center;
    -o-box-align:center;
    -ms-flex-align:center;
    -webkit-align-items:center;
    align-items:center
  }
  .blog-slider__item.swiper-slide-active .blog-slider__img img {
    opacity:1;
    -ms-filter:none;
    filter:none;
    -webkit-transition-delay:.3s;
    -moz-transition-delay:.3s;
    -o-transition-delay:.3s;
    -ms-transition-delay:.3s;
    transition-delay:.3s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>* {
    opacity:1;
    -ms-filter:none;
    filter:none;
    -webkit-transform:none;
    -moz-transform:none;
    -o-transform:none;
    -ms-transform:none;
    transform:none
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(1) {
    -webkit-transition-delay:.3s;
    -moz-transition-delay:.3s;
    -o-transition-delay:.3s;
    -ms-transition-delay:.3s;
    transition-delay:.3s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(2) {
    -webkit-transition-delay:.4s;
    -moz-transition-delay:.4s;
    -o-transition-delay:.4s;
    -ms-transition-delay:.4s;
    transition-delay:.4s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(3) {
    -webkit-transition-delay:.5s;
    -moz-transition-delay:.5s;
    -o-transition-delay:.5s;
    -ms-transition-delay:.5s;
    transition-delay:.5s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(4) {
    -webkit-transition-delay:.6s;
    -moz-transition-delay:.6s;
    -o-transition-delay:.6s;
    -ms-transition-delay:.6s;
    transition-delay:.6s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(5) {
    -webkit-transition-delay:.7s;
    -moz-transition-delay:.7s;
    -o-transition-delay:.7s;
    -ms-transition-delay:.7s;
    transition-delay:.7s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(6) {
    -webkit-transition-delay:.8s;
    -moz-transition-delay:.8s;
    -o-transition-delay:.8s;
    -ms-transition-delay:.8s;
    transition-delay:.8s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(7) {
    -webkit-transition-delay:.9s;
    -moz-transition-delay:.9s;
    -o-transition-delay:.9s;
    -ms-transition-delay:.9s;
    transition-delay:.9s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(8) {
    -webkit-transition-delay:1s;
    -moz-transition-delay:1s;
    -o-transition-delay:1s;
    -ms-transition-delay:1s;
    transition-delay:1s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(9) {
    -webkit-transition-delay:1.1s;
    -moz-transition-delay:1.1s;
    -o-transition-delay:1.1s;
    -ms-transition-delay:1.1s;
    transition-delay:1.1s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(10) {
    -webkit-transition-delay:1.2s;
    -moz-transition-delay:1.2s;
    -o-transition-delay:1.2s;
    -ms-transition-delay:1.2s;
    transition-delay:1.2s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(11) {
    -webkit-transition-delay:1.3s;
    -moz-transition-delay:1.3s;
    -o-transition-delay:1.3s;
    -ms-transition-delay:1.3s;
    transition-delay:1.3s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(12) {
    -webkit-transition-delay:1.4s;
    -moz-transition-delay:1.4s;
    -o-transition-delay:1.4s;
    -ms-transition-delay:1.4s;
    transition-delay:1.4s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(13) {
    -webkit-transition-delay:1.5s;
    -moz-transition-delay:1.5s;
    -o-transition-delay:1.5s;
    -ms-transition-delay:1.5s;
    transition-delay:1.5s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(14) {
    -webkit-transition-delay:1.6s;
    -moz-transition-delay:1.6s;
    -o-transition-delay:1.6s;
    -ms-transition-delay:1.6s;
    transition-delay:1.6s
  }
  .blog-slider__item.swiper-slide-active .blog-slider__content>:nth-child(15) {
    -webkit-transition-delay:1.7s;
    -moz-transition-delay:1.7s;
    -o-transition-delay:1.7s;
    -ms-transition-delay:1.7s;
    transition-delay:1.7s
  }
  .blog-slider__img {
    width:200px;
    -webkit-flex-shrink:0;
    flex-shrink:0;
    height:200px;
    padding:10px;
    border-radius:5px;
    -webkit-transform:translateX(0);
    -moz-transform:translateX(0);
    -o-transform:translateX(0);
    -ms-transform:translateX(0);
    transform:translateX(0);
    overflow:hidden
  }
  .blog-slider__img:after {
    content:'';
    position:absolute;
    top:0;
    left:0;
    width:100%;
    height:100%;
    border-radius:5px;
    opacity:.8
  }
  .blog-slider__img img {
    width:100%;
    height:100%;
    object-fit:cover;
    display:block;
    opacity:0;
    border-radius:5px;
    -webkit-transition:all .3s;
    -moz-transition:all .3s;
    -o-transition:all .3s;
    -ms-transition:all .3s;
    transition:all .3s
  }
  .blog-slider__content {
    padding-right:50px;
    padding-left:50px
  }
  .blog-slider__content>* {
    opacity:0;
    -webkit-transform:translateY(25px);
    -moz-transform:translateY(25px);
    -o-transform:translateY(25px);
    -ms-transform:translateY(25px);
    transform:translateY(25px);
    -webkit-transition:all .4s;
    -moz-transition:all .4s;
    -o-transition:all .4s;
    -ms-transition:all .4s;
    transition:all .4s
  }
  .blog-slider__code {
    color:var(--font-color);
    margin-bottom:0;
    display:block;
    font-weight:500
  }
  .blog-slider__title {
    font-size:18px;
    font-weight:700;
    color:var(--font-color);
    margin-bottom:15px;
    -webkit-perspective: 1200px; /* Chrome, Safari */
    -moz-perspective: 1200px;    /* Firefox */
    -ms-perspective: 1200px;     /* Internet Explorer */
    -o-perspective: 1200px;      /* Opera */
    perspective: 1200px;  
    display:-webkit-box;
    overflow:hidden;
    -webkit-box-orient:vertical
  }
  .blog-slider__text {
    color:var(--font-color);
    -webkit-perspective: 1200px; /* Chrome, Safari */
    -moz-perspective: 1200px;    /* Firefox */
    -ms-perspective: 1200px;     /* Internet Explorer */
    -o-perspective: 1200px;      /* Opera */
    perspective: 1200px;  
    display:-webkit-box;
    overflow:hidden;
    -webkit-box-orient:vertical;
    margin-bottom:15px;
    line-height:1.5em;
    width:100%;
    display:block;
    word-break:break-all;
    word-wrap:break-word
  }
  .blog-slider__button {
    display:-webkit-inline-box;
    display:-moz-inline-box;
    display:-webkit-inline-flex;
    display:-ms-inline-flexbox;
    display:inline-box;
    display:inline-flex;
    background-color:var(--btn-bg);
    padding:4px 14px;
    border-radius:8px;
    color:var(--btn-color);
    text-decoration:none;
    font-weight:500;
    -webkit-box-pack:center;
    -moz-box-pack:center;
    -o-box-pack:center;
    -ms-flex-pack:center;
    -webkit-justify-content:center;
    justify-content:center;
    text-align:center;
    letter-spacing:1px;
    display:none
  }
  .blog-slider__button:hover {
    background-color:var(--btn-hover-color);
    color:var(--btn-color)
  }
  .blog-slider .swiper-container-horizontal>.swiper-pagination-bullets, .blog-slider .swiper-pagination-custom, .blog-slider .swiper-pagination-fraction {
    bottom:10px;
    left:0;
    width:100%
  }
  .blog-slider__pagination {
    position:absolute;
    z-index:21;
    right:20px;
    width:11px!important;
    text-align:center;
    left:auto!important;
    top:50%;
    bottom:auto!important;
    -webkit-transform:translateY(-50%);
    -moz-transform:translateY(-50%);
    -o-transform:translateY(-50%);
    -ms-transform:translateY(-50%);
    transform:translateY(-50%)
  }
  .blog-slider__pagination.swiper-pagination-bullets .swiper-pagination-bullet {
    margin:8px 0
  }
  .blog-slider__pagination .swiper-pagination-bullet {
    width:11px;
    height:11px;
    display:block;
    border-radius:10px;
    background:#858585;
    opacity:.2;
    -webkit-transition:all .3s;
    -moz-transition:all .3s;
    -o-transition:all .3s;
    -ms-transition:all .3s;
    transition:all .3s
  }
  .blog-slider__pagination .swiper-pagination-bullet-active {
    opacity:1;
    -ms-filter:none;
    filter:none;
    background:var(--btn-bg);
    height:30px
  }
  @media screen and (max-width:600px) {
    .blog-slider__pagination {
        -webkit-transform:translateX(-50%);
        -moz-transform:translateX(-50%);
        -o-transform:translateX(-50%);
        -ms-transform:translateX(-50%);
        transform:translateX(-50%);
        left:50%!important;
        top:320px;
        width:100%!important;
        display:-webkit-box;
        display:-moz-box;
        display:-webkit-flex;
        display:-ms-flexbox;
        display:box;
        display:flex;
        -webkit-box-pack:center;
        -moz-box-pack:center;
        -o-box-pack:center;
        -ms-flex-pack:center;
        -webkit-justify-content:center;
        justify-content:center;
        -webkit-box-align:center;
        -moz-box-align:center;
        -o-box-align:center;
        -ms-flex-align:center;
        -webkit-align-items:center;
        align-items:center
    }
    .blog-slider__pagination.swiper-pagination-bullets .swiper-pagination-bullet {
        margin:0 5px
    }
    .blog-slider__pagination .swiper-pagination-bullet-active {
        height:11px;
        width:30px
    }
    .blog-slider__button {
        display:-webkit-inline-box;
        display:-moz-inline-box;
        display:-webkit-inline-flex;
        display:-ms-inline-flexbox;
        display:inline-box;
        display:inline-flex;
        width:100%
    }
    .blog-slider__text {
        margin-bottom:40px
    }
    .blog-slider {
        min-height:350px;
        height:auto;
        margin-top:110px;
        margin-bottom:10px
    }
    .blog-slider__content {
        margin-top:-80px;
        text-align:center;
        padding:0 30px
    }
    .blog-slider__item {
        -webkit-box-orient:vertical;
        -moz-box-orient:vertical;
        -o-box-orient:vertical;
        -webkit-flex-direction:column;
        -ms-flex-direction:column;
        flex-direction:column
    }
    .blog-slider__img {
        -webkit-transform:translateY(-50%);
        -moz-transform:translateY(-50%);
        -o-transform:translateY(-50%);
        -ms-transform:translateY(-50%);
        transform:translateY(-50%);
        width:90%
    }
    .blog-slider__content {
        padding-left:10px;
        padding-right:10px
    }
    .blog-slider__pagination.swiper-pagination-clickable.swiper-pagination-bullets {
        top:110px
    }
  }
  @media screen and (min-width:600px) {
    .blog-slider {
        height:200px
    }
    .blog-slider__img {
        height:200px
    }
  }
  /* 闅忔満鏂囩珷css */
  #random{
    background-color: var(--anzhiyu-white-acrylic1)!important;
    border-radius: 8px;
    font-family: 'ZhuZiAYuanJWD',PingFang,寰蒋闆呴粦,HYTMR,SimSun;
    background-size:cover;
    color:var(--anzhiyu-white);
    font-size:44px; /*瑙夊緱灏忓彲浠ュ紑澶т竴鐐�*/
    position: relative;
    border-radius: 8px;
    overflow: hidden;
    
    margin-left: 10px;
  }
  #random-banner{
    width:100% !important;
    height:178.5px !important;
    border-radius:8px;
    display: flex;
  }
  #random-hover{
    opacity: 0;
    padding-left: 0rem;
    position:absolute;
    left: 0;
    top: 0;
    height:100%!important;
    color: #0000;
    border-radius: 8px;
    transition: cubic-bezier(.71,.15,.16,1.15) .6s;
  }
  #random-hover:hover{
    padding-left: 1rem;
    opacity: 1;
   color:#ffffff;
   background-color: #425aefdd;
   backdrop-filter: blur(10px) saturate(180%);
   -webkit-backdrop-filter: blur(10px) saturate(180%);
   transition: .3s;
   background-size: 200%;
   cursor: pointer;
  }
  
  @media screen and (max-width: 1050px){
    #random{
        display: none;
    }
    div#swiper_container {
      width: 100%;
    }
  }
  
  .swiper_container_card {
    display: flex !important;
    justify-content: space-around;
    flex-direction: row !important;
  }]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/css/swiper.min.css</url>
    <content><![CDATA[/**
 * Swiper 4.1.6
 * Most modern mobile touch slider and framework with hardware accelerated transitions
 * http://www.idangero.us/swiper/
 *
 * Copyright 2014-2018 Vladimir Kharlampidi
 *
 * Released under the MIT License
 *
 * Released on: February 11, 2018
 */
.swiper-container {
    margin: 0 auto;
    position: relative;
    overflow: hidden;
    list-style: none;
    padding: 0;
    z-index: 1
}
.swiper-container-no-flexbox .swiper-slide {
    float: left
}

.swiper-container-vertical>.swiper-wrapper {
    -webkit-box-orient: vertical;
    -webkit-box-direction: normal;
    -webkit-flex-direction: column;
    -ms-flex-direction: column;
    flex-direction: column
}

.swiper-wrapper {
    position: relative;
    width: 100%;
    height: 100%;
    z-index: 1;
    display: -webkit-box;
    display: -webkit-flex;
    display: -ms-flexbox;
    display: flex;
    -webkit-transition-property: -webkit-transform;
    transition-property: -webkit-transform;
    -o-transition-property: transform;
    transition-property: transform;
    transition-property: transform, -webkit-transform;
    -webkit-box-sizing: content-box;
    box-sizing: content-box
}

.swiper-container-android .swiper-slide,
.swiper-wrapper {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0)
}

.swiper-container-multirow>.swiper-wrapper {
    -webkit-flex-wrap: wrap;
    -ms-flex-wrap: wrap;
    flex-wrap: wrap
}

.swiper-container-free-mode>.swiper-wrapper {
    -webkit-transition-timing-function: ease-out;
    -o-transition-timing-function: ease-out;
    transition-timing-function: ease-out;
    margin: 0 auto
}

.swiper-slide {
    -webkit-flex-shrink: 0;
    -ms-flex-negative: 0;
    flex-shrink: 0;
    width: 100%;
    height: 100%;
    position: relative;
    -webkit-transition-property: -webkit-transform;
    transition-property: -webkit-transform;
    -o-transition-property: transform;
    transition-property: transform;
    transition-property: transform, -webkit-transform
}

.swiper-invisible-blank-slide {
    visibility: hidden
}

.swiper-container-autoheight,
.swiper-container-autoheight .swiper-slide {
    height: auto
}

.swiper-container-autoheight .swiper-wrapper {
    -webkit-box-align: start;
    -webkit-align-items: flex-start;
    -ms-flex-align: start;
    align-items: flex-start;
    -webkit-transition-property: height, -webkit-transform;
    transition-property: height, -webkit-transform;
    -o-transition-property: transform, height;
    transition-property: transform, height;
    transition-property: transform, height, -webkit-transform
}

.swiper-container-3d {
    -webkit-perspective: 1200px;
    perspective: 1200px
}

.swiper-container-3d .swiper-cube-shadow,
.swiper-container-3d .swiper-slide,
.swiper-container-3d .swiper-slide-shadow-bottom,
.swiper-container-3d .swiper-slide-shadow-left,
.swiper-container-3d .swiper-slide-shadow-right,
.swiper-container-3d .swiper-slide-shadow-top,
.swiper-container-3d .swiper-wrapper {
    -webkit-transform-style: preserve-3d;
    transform-style: preserve-3d
}

.swiper-container-3d .swiper-slide-shadow-bottom,
.swiper-container-3d .swiper-slide-shadow-left,
.swiper-container-3d .swiper-slide-shadow-right,
.swiper-container-3d .swiper-slide-shadow-top {
    position: absolute;
    left: 0;
    top: 0;
    width: 100%;
    height: 100%;
    pointer-events: none;
    z-index: 10
}

.swiper-container-3d .swiper-slide-shadow-left {
    background-image: -webkit-gradient(linear, right top, left top, from(rgba(0, 0, 0, .5)), to(rgba(0, 0, 0, 0)));
    background-image: -webkit-linear-gradient(right, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0));
    background-image: -o-linear-gradient(right, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0));
    background-image: linear-gradient(to left, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0))
}

.swiper-container-3d .swiper-slide-shadow-right {
    background-image: -webkit-gradient(linear, left top, right top, from(rgba(0, 0, 0, .5)), to(rgba(0, 0, 0, 0)));
    background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0));
    background-image: -o-linear-gradient(left, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0));
    background-image: linear-gradient(to right, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0))
}

.swiper-container-3d .swiper-slide-shadow-top {
    background-image: -webkit-gradient(linear, left bottom, left top, from(rgba(0, 0, 0, .5)), to(rgba(0, 0, 0, 0)));
    background-image: -webkit-linear-gradient(bottom, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0));
    background-image: -o-linear-gradient(bottom, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0));
    background-image: linear-gradient(to top, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0))
}

.swiper-container-3d .swiper-slide-shadow-bottom {
    background-image: -webkit-gradient(linear, left top, left bottom, from(rgba(0, 0, 0, .5)), to(rgba(0, 0, 0, 0)));
    background-image: -webkit-linear-gradient(top, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0));
    background-image: -o-linear-gradient(top, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0));
    background-image: linear-gradient(to bottom, rgba(0, 0, 0, .5), rgba(0, 0, 0, 0))
}

.swiper-container-wp8-horizontal,
.swiper-container-wp8-horizontal>.swiper-wrapper {
    -ms-touch-action: pan-y;
    touch-action: pan-y
}

.swiper-container-wp8-vertical,
.swiper-container-wp8-vertical>.swiper-wrapper {
    -ms-touch-action: pan-x;
    touch-action: pan-x
}

.swiper-button-next,
.swiper-button-prev {
    position: absolute;
    top: 50%;
    width: 27px;
    height: 44px;
    margin-top: -22px;
    z-index: 10;
    cursor: pointer;
    background-size: 27px 44px;
    background-position: center;
    background-repeat: no-repeat
}

.swiper-button-next.swiper-button-disabled,
.swiper-button-prev.swiper-button-disabled {
    opacity: .35;
    cursor: auto;
    pointer-events: none
}

.swiper-button-prev,
.swiper-container-rtl .swiper-button-next {
    background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20viewBox%3D'0%200%2027%2044'%3E%3Cpath%20d%3D'M0%2C22L22%2C0l2.1%2C2.1L4.2%2C22l19.9%2C19.9L22%2C44L0%2C22L0%2C22L0%2C22z'%20fill%3D'%23007aff'%2F%3E%3C%2Fsvg%3E");
    left: 10px;
    right: auto
}

.swiper-button-next,
.swiper-container-rtl .swiper-button-prev {
    background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20viewBox%3D'0%200%2027%2044'%3E%3Cpath%20d%3D'M27%2C22L27%2C22L5%2C44l-2.1-2.1L22.8%2C22L2.9%2C2.1L5%2C0L27%2C22L27%2C22z'%20fill%3D'%23007aff'%2F%3E%3C%2Fsvg%3E");
    right: 10px;
    left: auto
}

.swiper-button-prev.swiper-button-white,
.swiper-container-rtl .swiper-button-next.swiper-button-white {
    background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20viewBox%3D'0%200%2027%2044'%3E%3Cpath%20d%3D'M0%2C22L22%2C0l2.1%2C2.1L4.2%2C22l19.9%2C19.9L22%2C44L0%2C22L0%2C22L0%2C22z'%20fill%3D'%23ffffff'%2F%3E%3C%2Fsvg%3E")
}

.swiper-button-next.swiper-button-white,
.swiper-container-rtl .swiper-button-prev.swiper-button-white {
    background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20viewBox%3D'0%200%2027%2044'%3E%3Cpath%20d%3D'M27%2C22L27%2C22L5%2C44l-2.1-2.1L22.8%2C22L2.9%2C2.1L5%2C0L27%2C22L27%2C22z'%20fill%3D'%23ffffff'%2F%3E%3C%2Fsvg%3E")
}

.swiper-button-prev.swiper-button-black,
.swiper-container-rtl .swiper-button-next.swiper-button-black {
    background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20viewBox%3D'0%200%2027%2044'%3E%3Cpath%20d%3D'M0%2C22L22%2C0l2.1%2C2.1L4.2%2C22l19.9%2C19.9L22%2C44L0%2C22L0%2C22L0%2C22z'%20fill%3D'%23000000'%2F%3E%3C%2Fsvg%3E")
}

.swiper-button-next.swiper-button-black,
.swiper-container-rtl .swiper-button-prev.swiper-button-black {
    background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20viewBox%3D'0%200%2027%2044'%3E%3Cpath%20d%3D'M27%2C22L27%2C22L5%2C44l-2.1-2.1L22.8%2C22L2.9%2C2.1L5%2C0L27%2C22L27%2C22z'%20fill%3D'%23000000'%2F%3E%3C%2Fsvg%3E")
}

.swiper-button-lock {
    display: none
}

.swiper-pagination {
    position: absolute;
    text-align: center;
    -webkit-transition: .3s opacity;
    -o-transition: .3s opacity;
    transition: .3s opacity;
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    z-index: 10
}

.swiper-pagination.swiper-pagination-hidden {
    opacity: 0
}

.swiper-container-horizontal>.swiper-pagination-bullets,
.swiper-pagination-custom,
.swiper-pagination-fraction {
    bottom: 10px;
    left: 0;
    width: 100%
}

.swiper-pagination-bullets-dynamic {
    overflow: hidden;
    font-size: 0
}

.swiper-pagination-bullets-dynamic .swiper-pagination-bullet {
    -webkit-transform: scale(.33);
    -ms-transform: scale(.33);
    transform: scale(.33);
    position: relative
}

.swiper-pagination-bullets-dynamic .swiper-pagination-bullet-active {
    -webkit-transform: scale(1);
    -ms-transform: scale(1);
    transform: scale(1)
}

.swiper-pagination-bullets-dynamic .swiper-pagination-bullet-active-main {
    -webkit-transform: scale(1);
    -ms-transform: scale(1);
    transform: scale(1)
}

.swiper-pagination-bullets-dynamic .swiper-pagination-bullet-active-prev {
    -webkit-transform: scale(.66);
    -ms-transform: scale(.66);
    transform: scale(.66)
}

.swiper-pagination-bullets-dynamic .swiper-pagination-bullet-active-prev-prev {
    -webkit-transform: scale(.33);
    -ms-transform: scale(.33);
    transform: scale(.33)
}

.swiper-pagination-bullets-dynamic .swiper-pagination-bullet-active-next {
    -webkit-transform: scale(.66);
    -ms-transform: scale(.66);
    transform: scale(.66)
}

.swiper-pagination-bullets-dynamic .swiper-pagination-bullet-active-next-next {
    -webkit-transform: scale(.33);
    -ms-transform: scale(.33);
    transform: scale(.33)
}

.swiper-pagination-bullet {
    width: 8px;
    height: 8px;
    display: inline-block;
    border-radius: 100%;
    background: #000;
    opacity: .2
}

button.swiper-pagination-bullet {
    border: none;
    margin: 0;
    padding: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
    -webkit-appearance: none;
    -moz-appearance: none;
    appearance: none
}

.swiper-pagination-clickable .swiper-pagination-bullet {
    cursor: pointer
}

.swiper-pagination-bullet-active {
    opacity: 1;
    background: #007aff
}

.swiper-container-vertical>.swiper-pagination-bullets {
    right: 10px;
    top: 50%;
    -webkit-transform: translate3d(0, -50%, 0);
    transform: translate3d(0, -50%, 0)
}

.swiper-container-vertical>.swiper-pagination-bullets .swiper-pagination-bullet {
    margin: 6px 0;
    display: block
}

.swiper-container-vertical>.swiper-pagination-bullets.swiper-pagination-bullets-dynamic {
    top: 50%;
    -webkit-transform: translateY(-50%);
    -ms-transform: translateY(-50%);
    transform: translateY(-50%);
    width: 8px
}

.swiper-container-vertical>.swiper-pagination-bullets.swiper-pagination-bullets-dynamic .swiper-pagination-bullet {
    display: inline-block;
    -webkit-transition: .2s top, .2s -webkit-transform;
    transition: .2s top, .2s -webkit-transform;
    -o-transition: .2s transform, .2s top;
    transition: .2s transform, .2s top;
    transition: .2s transform, .2s top, .2s -webkit-transform
}

.swiper-container-horizontal>.swiper-pagination-bullets .swiper-pagination-bullet {
    margin: 0 4px
}

.swiper-container-horizontal>.swiper-pagination-bullets.swiper-pagination-bullets-dynamic {
    left: 50%;
    -webkit-transform: translateX(-50%);
    -ms-transform: translateX(-50%);
    transform: translateX(-50%);
    white-space: nowrap
}

.swiper-container-horizontal>.swiper-pagination-bullets.swiper-pagination-bullets-dynamic .swiper-pagination-bullet {
    -webkit-transition: .2s left, .2s -webkit-transform;
    transition: .2s left, .2s -webkit-transform;
    -o-transition: .2s transform, .2s left;
    transition: .2s transform, .2s left;
    transition: .2s transform, .2s left, .2s -webkit-transform
}

.swiper-container-horizontal.swiper-container-rtl>.swiper-pagination-bullets-dynamic .swiper-pagination-bullet {
    -webkit-transition: .2s right, .2s -webkit-transform;
    transition: .2s right, .2s -webkit-transform;
    -o-transition: .2s transform, .2s right;
    transition: .2s transform, .2s right;
    transition: .2s transform, .2s right, .2s -webkit-transform
}

.swiper-pagination-progressbar {
    background: rgba(0, 0, 0, .25);
    position: absolute
}

.swiper-pagination-progressbar .swiper-pagination-progressbar-fill {
    background: #007aff;
    position: absolute;
    left: 0;
    top: 0;
    width: 100%;
    height: 100%;
    -webkit-transform: scale(0);
    -ms-transform: scale(0);
    transform: scale(0);
    -webkit-transform-origin: left top;
    -ms-transform-origin: left top;
    transform-origin: left top
}

.swiper-container-rtl .swiper-pagination-progressbar .swiper-pagination-progressbar-fill {
    -webkit-transform-origin: right top;
    -ms-transform-origin: right top;
    transform-origin: right top
}

.swiper-container-horizontal>.swiper-pagination-progressbar {
    width: 100%;
    height: 4px;
    left: 0;
    top: 0
}

.swiper-container-vertical>.swiper-pagination-progressbar {
    width: 4px;
    height: 100%;
    left: 0;
    top: 0
}

.swiper-pagination-white .swiper-pagination-bullet-active {
    background: #fff
}

.swiper-pagination-progressbar.swiper-pagination-white {
    background: rgba(255, 255, 255, .25)
}

.swiper-pagination-progressbar.swiper-pagination-white .swiper-pagination-progressbar-fill {
    background: #fff
}

.swiper-pagination-black .swiper-pagination-bullet-active {
    background: #000
}

.swiper-pagination-progressbar.swiper-pagination-black {
    background: rgba(0, 0, 0, .25)
}

.swiper-pagination-progressbar.swiper-pagination-black .swiper-pagination-progressbar-fill {
    background: #000
}

.swiper-pagination-lock {
    display: none
}

.swiper-container-coverflow .swiper-wrapper {
    -webkit-perspective: 1200px; /* Chrome, Safari */
    -moz-perspective: 1200px;    /* Firefox */
    -ms-perspective: 1200px;     /* Internet Explorer */
    -o-perspective: 1200px;      /* Opera */
    perspective: 1200px;          /* 标准属性 */
  }

.swiper-container-horizontal>.swiper-scrollbar {
    position: absolute;
    left: 1%;
    bottom: 3px;
    z-index: 50;
    height: 5px;
    width: 98%
}

.swiper-container-vertical>.swiper-scrollbar {
    position: absolute;
    right: 3px;
    top: 1%;
    z-index: 50;
    width: 5px;
    height: 98%
}

.swiper-scrollbar-drag {
    height: 100%;
    width: 100%;
    position: relative;
    background: rgba(0, 0, 0, .5);
    border-radius: 10px;
    left: 0;
    top: 0
}

.swiper-scrollbar-cursor-drag {
    cursor: move
}

.swiper-scrollbar-lock {
    display: none
}

.swiper-zoom-container {
    width: 100%;
    height: 100%;
    display: -webkit-box;
    display: -webkit-flex;
    display: -ms-flexbox;
    display: flex;
    -webkit-box-pack: center;
    -webkit-justify-content: center;
    -ms-flex-pack: center;
    justify-content: center;
    -webkit-box-align: center;
    -webkit-align-items: center;
    -ms-flex-align: center;
    align-items: center;
    text-align: center
}

.swiper-zoom-container>canvas,
.swiper-zoom-container>img,
.swiper-zoom-container>svg {
    max-width: 100%;
    max-height: 100%;
    -o-object-fit: contain;
    object-fit: contain
}

.swiper-slide-zoomed {
    cursor: move
}

.swiper-lazy-preloader {
    width: 42px;
    height: 42px;
    position: absolute;
    left: 50%;
    top: 50%;
    margin-left: -21px;
    margin-top: -21px;
    z-index: 10;
    -webkit-transform-origin: 50%;
    -ms-transform-origin: 50%;
    transform-origin: 50%;
    -webkit-animation: swiper-preloader-spin 1s steps(12, end) infinite;
    animation: swiper-preloader-spin 1s steps(12, end) infinite
}

.swiper-lazy-preloader:after {
    display: block;
    content: '';
    width: 100%;
    height: 100%;
    background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg%20viewBox%3D'0%200%20120%20120'%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20xmlns%3Axlink%3D'http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink'%3E%3Cdefs%3E%3Cline%20id%3D'l'%20x1%3D'60'%20x2%3D'60'%20y1%3D'7'%20y2%3D'27'%20stroke%3D'%236c6c6c'%20stroke-width%3D'11'%20stroke-linecap%3D'round'%2F%3E%3C%2Fdefs%3E%3Cg%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(30%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(60%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(90%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(120%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(150%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.37'%20transform%3D'rotate(180%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.46'%20transform%3D'rotate(210%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.56'%20transform%3D'rotate(240%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.66'%20transform%3D'rotate(270%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.75'%20transform%3D'rotate(300%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.85'%20transform%3D'rotate(330%2060%2C60)'%2F%3E%3C%2Fg%3E%3C%2Fsvg%3E");
    background-position: 50%;
    background-size: 100%;
    background-repeat: no-repeat
}

.swiper-lazy-preloader-white:after {
    background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg%20viewBox%3D'0%200%20120%20120'%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20xmlns%3Axlink%3D'http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink'%3E%3Cdefs%3E%3Cline%20id%3D'l'%20x1%3D'60'%20x2%3D'60'%20y1%3D'7'%20y2%3D'27'%20stroke%3D'%23fff'%20stroke-width%3D'11'%20stroke-linecap%3D'round'%2F%3E%3C%2Fdefs%3E%3Cg%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(30%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(60%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(90%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(120%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.27'%20transform%3D'rotate(150%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.37'%20transform%3D'rotate(180%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.46'%20transform%3D'rotate(210%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.56'%20transform%3D'rotate(240%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.66'%20transform%3D'rotate(270%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.75'%20transform%3D'rotate(300%2060%2C60)'%2F%3E%3Cuse%20xlink%3Ahref%3D'%23l'%20opacity%3D'.85'%20transform%3D'rotate(330%2060%2C60)'%2F%3E%3C%2Fg%3E%3C%2Fsvg%3E")
}

@-webkit-keyframes swiper-preloader-spin {
    100% {
        -webkit-transform: rotate(360deg);
        transform: rotate(360deg)
    }
}

@keyframes swiper-preloader-spin {
    100% {
        -webkit-transform: rotate(360deg);
        transform: rotate(360deg)
    }
}

.swiper-container .swiper-notification {
    position: absolute;
    left: 0;
    top: 0;
    pointer-events: none;
    opacity: 0;
    z-index: -1000
}

.swiper-container-fade.swiper-container-free-mode .swiper-slide {
    -webkit-transition-timing-function: ease-out;
    -o-transition-timing-function: ease-out;
    transition-timing-function: ease-out
}

.swiper-container-fade .swiper-slide {
    pointer-events: none;
    -webkit-transition-property: opacity;
    -o-transition-property: opacity;
    transition-property: opacity
}

.swiper-container-fade .swiper-slide .swiper-slide {
    pointer-events: none
}

.swiper-container-fade .swiper-slide-active,
.swiper-container-fade .swiper-slide-active .swiper-slide-active {
    pointer-events: auto
}

.swiper-container-cube {
    overflow: visible
}

.swiper-container-cube .swiper-slide {
    pointer-events: none;
    -webkit-backface-visibility: hidden;
    backface-visibility: hidden;
    z-index: 1;
    visibility: hidden;
    -webkit-transform-origin: 0 0;
    -ms-transform-origin: 0 0;
    transform-origin: 0 0;
    width: 100%;
    height: 100%
}

.swiper-container-cube .swiper-slide .swiper-slide {
    pointer-events: none
}

.swiper-container-cube.swiper-container-rtl .swiper-slide {
    -webkit-transform-origin: 100% 0;
    -ms-transform-origin: 100% 0;
    transform-origin: 100% 0
}

.swiper-container-cube .swiper-slide-active,
.swiper-container-cube .swiper-slide-active .swiper-slide-active {
    pointer-events: auto
}

.swiper-container-cube .swiper-slide-active,
.swiper-container-cube .swiper-slide-next,
.swiper-container-cube .swiper-slide-next+.swiper-slide,
.swiper-container-cube .swiper-slide-prev {
    pointer-events: auto;
    visibility: visible
}

.swiper-container-cube .swiper-slide-shadow-bottom,
.swiper-container-cube .swiper-slide-shadow-left,
.swiper-container-cube .swiper-slide-shadow-right,
.swiper-container-cube .swiper-slide-shadow-top {
    z-index: 0;
    -webkit-backface-visibility: hidden;
    backface-visibility: hidden
}

.swiper-container-cube .swiper-cube-shadow {
    position: absolute;
    left: 0;
    bottom: 0;
    width: 100%;
    height: 100%;
    background: #000;
    opacity: .6;
    -webkit-filter: blur(50px);
    filter: blur(50px);
    z-index: 0
}

.swiper-container-flip {
    overflow: visible
}

.swiper-container-flip .swiper-slide {
    pointer-events: none;
    -webkit-backface-visibility: hidden;
    backface-visibility: hidden;
    z-index: 1
}

.swiper-container-flip .swiper-slide .swiper-slide {
    pointer-events: none
}

.swiper-container-flip .swiper-slide-active,
.swiper-container-flip .swiper-slide-active .swiper-slide-active {
    pointer-events: auto
}

.swiper-container-flip .swiper-slide-shadow-bottom,
.swiper-container-flip .swiper-slide-shadow-left,
.swiper-container-flip .swiper-slide-shadow-right,
.swiper-container-flip .swiper-slide-shadow-top {
    z-index: 0;
    -webkit-backface-visibility: hidden;
    backface-visibility: hidden
}

.swiper-container-coverflow .swiper-wrapper {
    -webkit-perspective: 1200px; /* Chrome, Safari */
    -moz-perspective: 1200px;    /* Firefox */
    -ms-perspective: 1200px;     /* Internet Explorer */
    -o-perspective: 1200px;      /* Opera */
    perspective: 1200px;          /* 标准属性 */
  }]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/js/ali_font.js</url>
    <content><![CDATA[window._iconfont_svg_string_4902343='',(h=>{var c=(l=(l=document.getElementsByTagName("script"))[l.length-1]).getAttribute("data-injectcss"),l=l.getAttribute("data-disable-injectsvg");if(!l){var i,a,t,o,v,m=function(c,l){l.parentNode.insertBefore(c,l)};if(c&&!h.__iconfont__svg__cssinject__){h.__iconfont__svg__cssinject__=!0;try{document.write(".svgfont {display: inline-block;width: 1em;height: 1em;fill: currentColor;vertical-align: -0.1em;font-size:16px;}")}catch(c){console&&console.log(c)}}i=function(){var c,l=document.createElement("div");l.innerHTML=h._iconfont_svg_string_4902343,(l=l.getElementsByTagName("svg")[0])&&(l.setAttribute("aria-hidden","true"),l.style.position="absolute",l.style.width=0,l.style.height=0,l.style.overflow="hidden",l=l,(c=document.body).firstChild?m(l,c.firstChild):c.appendChild(l))},document.addEventListener?~["complete","loaded","interactive"].indexOf(document.readyState)?setTimeout(i,0):(a=function(){document.removeEventListener("DOMContentLoaded",a,!1),i()},document.addEventListener("DOMContentLoaded",a,!1)):document.attachEvent&&(t=i,o=h.document,v=!1,z(),o.onreadystatechange=function(){"complete"==o.readyState&&(o.onreadystatechange=null,s())})}function s(){v||(v=!0,t())}function z(){try{o.documentElement.doScroll("left")}catch(c){return void setTimeout(z,50)}s()}})(window);]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/js/fuction.js</url>
    <content><![CDATA[// 分享本页
function share() {
    let url = window.location.origin + window.location.pathname
    new ClipboardJS(".share", { text: function() { return '标题：' + document.title + '\n链接：' + url } });
    btf.snackbarShow("本页链接已复制到剪切板，快去分享吧~")
}
window.onscroll = percent;// 执行函数

// 页面百分比
function percent() {
    // 检查页面中是否存在 card-toc 元素
    const cardToc = document.getElementById('card-toc');
    if (!cardToc) {
        return; // 如果不存在，直接返回，不执行后续代码
    }

    // 获取百分比文本内容
    const percentage = cardToc.querySelector('.toc-percentage').textContent.trim();

    // 获取 go-up 元素
    let up = document.querySelector("#go-up");

    // 根据百分比设置样式和内容
    if (percentage ]]></content>
  </entry>
  <entry>
    <title>友情链接</title>
    <url>/link/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>music</title>
    <url>/music/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
