<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>梯度下降 | molittle</title><meta name="author" content="molittle"><meta name="copyright" content="molittle"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="梯度下降线性回归预测房价 数据加载 数据介绍 数据拆分 数据建模 数据预测 数据评估  1、无约束最优化问题1.1、无约束最优化  无约束最优化问题（unconstrained optimizationproblem）指的是从一个问题的所有可能的备选方案中，选择出依某种指标来说是最优的解决方案。从数学上说，最优化是研究在一个给定的集合S上泛函$J(\theta)$的极小化或极大化问题：广义上，最优">
<meta property="og:type" content="article">
<meta property="og:title" content="梯度下降">
<meta property="og:url" content="http://molittle-git.github.io/posts/3c50d4b7.html">
<meta property="og:site_name" content="molittle">
<meta property="og:description" content="梯度下降线性回归预测房价 数据加载 数据介绍 数据拆分 数据建模 数据预测 数据评估  1、无约束最优化问题1.1、无约束最优化  无约束最优化问题（unconstrained optimizationproblem）指的是从一个问题的所有可能的备选方案中，选择出依某种指标来说是最优的解决方案。从数学上说，最优化是研究在一个给定的集合S上泛函$J(\theta)$的极小化或极大化问题：广义上，最优">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://molittle-git.github.io/img/123.png">
<meta property="article:published_time" content="2025-03-03T02:49:32.000Z">
<meta property="article:modified_time" content="2025-04-25T10:28:18.820Z">
<meta property="article:author" content="molittle">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://molittle-git.github.io/img/123.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "梯度下降",
  "url": "http://molittle-git.github.io/posts/3c50d4b7.html",
  "image": "http://molittle-git.github.io/img/123.png",
  "datePublished": "2025-03-03T02:49:32.000Z",
  "dateModified": "2025-04-25T10:28:18.820Z",
  "author": [
    {
      "@type": "Person",
      "name": "molittle",
      "url": "http://molittle-git.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/butterfly-icon.png"><link rel="canonical" href="http://molittle-git.github.io/posts/3c50d4b7.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#3b70fc"/><link rel="apple-touch-icon" sizes="180x180" href="/img/siteicon/128.png"/><link rel="icon" type="image/png" sizes="32x32" href="/img/siteicon/32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/img/siteicon/16.png"/><link rel="mask-icon" href="/img/siteicon/128.png" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '梯度下降',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><script src="/js/title.js"></script><script data-pjax src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="/css/runtime/runtime.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="molittle" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(/img/789.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-home1"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-shangpinfenlei24"></use></svg><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/artitalk/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-liaotian"></use></svg><span> 闲言碎语</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/music/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-customerservice-fill1"></use></svg><span> 音乐馆</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/comments/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-liuyanban"></use></svg><span> 留言板</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/archives/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-shijianzhou_gaoliang"></use></svg><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/bangumis/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-icon_bilibili-circle"></use></svg><span> 追番</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/fcircle/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-zhifeiji1"></use></svg><span> 朋友圈</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/link/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-icon-lianjie"></use></svg><span> 友人帐</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/about/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-medium-circle-fill"></use></svg><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(img/123.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">molittle</span></a><a class="nav-page-title" href="/"><span class="site-name">梯度下降</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-home1"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-shangpinfenlei24"></use></svg><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/artitalk/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-liaotian"></use></svg><span> 闲言碎语</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/music/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-customerservice-fill1"></use></svg><span> 音乐馆</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/comments/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-liuyanban"></use></svg><span> 留言板</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/archives/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-shijianzhou_gaoliang"></use></svg><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/bangumis/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-icon_bilibili-circle"></use></svg><span> 追番</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/fcircle/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-zhifeiji1"></use></svg><span> 朋友圈</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/link/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-icon-lianjie"></use></svg><span> 友人帐</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/about/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-medium-circle-fill"></use></svg><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">梯度下降</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-03T02:49:32.000Z" title="发表于 2025-03-03 10:49:32">2025-03-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-25T10:28:18.820Z" title="更新于 2025-04-25 18:28:18">2025-04-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/program/">program</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/program/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="线性回归预测房价"><a href="#线性回归预测房价" class="headerlink" title="线性回归预测房价"></a>线性回归预测房价</h3><ul>
<li>数据加载</li>
<li>数据介绍</li>
<li>数据拆分</li>
<li>数据建模</li>
<li>数据预测</li>
<li>数据评估</li>
</ul>
<h3 id="1、无约束最优化问题"><a href="#1、无约束最优化问题" class="headerlink" title="1、无约束最优化问题"></a>1、无约束最优化问题</h3><h4 id="1-1、无约束最优化"><a href="#1-1、无约束最优化" class="headerlink" title="1.1、无约束最优化"></a>1.1、无约束最优化</h4><p>  <strong>无约束最优化问题</strong>（unconstrained optimizationproblem）指的是从一个问题的所有<strong>可能</strong>的备选方案中，选择出依某种指标来说是<strong>最优</strong>的解决方案。从数学上说，最优化是研究在一个给定的集合S上泛函$J(\theta)$的极小化或极大化问题：<strong>广义上</strong>，最优化包括数学规划、图和网络、组合最优化、库存论、决策论、排队论、最优控制等。<strong>狭义上</strong>，最优化仅指数学规划。</p>
<h4 id="1-2、梯度下降"><a href="#1-2、梯度下降" class="headerlink" title="1.2、梯度下降"></a>1.2、梯度下降</h4><p>  <strong>梯度下降法</strong>(Gradient Descent)是一个算法，但不是像多元线性回归那样是一个具体做回归任务的算法，而是一个非常<strong>通用</strong>的优化算法来帮助一些机器学习算法（都是无约束最优化问题）求解出<strong>最优解</strong>， 所谓的通用就是很多机器学习算法都是用梯度下降，甚至<strong>深度学习</strong>也是用它来求解最优解。所有优化算法的目的都是期望以<strong>最快</strong>的速度把模型参数θ求解出来，梯度下降法就是一种<strong>经典</strong>常用的优化算法。</p>
<p>  之前利用正规方程求解的 θ 是最优解的原因是 MSE 这个损失函数是凸函数。但是，机器学习的损失函数并非都是凸函数，设置导数为 0 会得到很多个极值，不能确定唯一解。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/1-非凸函数.jpg" alt=""></p>
<p>  使用正规方程 $\theta = (X^TX)^{-1}X^Ty$ 求解的另一个限制是特征维度（$X_1、X_2……、X_n$）不能太多，矩阵逆运算的时间复杂度通常为 $O(n^3)$ 。换句话说，就是如果特征数量翻倍，你的计算时间大致为原来的 $2^3$ 倍，也就是之前时间的8倍。举个例子，2 个特征 1 秒，4 个特征就是 8 秒，8 个特征就是 64 秒，16 个特征就是 512 秒，当特征更多的时候呢？运行时间会非常漫长~</p>
<p>  所以正规方程求出最优解<strong>并不是</strong>机器学习甚至深度学习常用的手段。</p>
<p>  之前我们令导数为 0，反过来求解最低点 θ 是多少，而梯度下降法是<strong>一点点</strong>去逼近最优解!</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/2-梯度下降思想.jpeg" alt=""></p>
<p>  其实这就跟生活中的情形很像，比如你问一个朋友的工资是多少，他说你猜？那就很难了，他说你猜完我告诉你是猜高了还是猜低了，这样你就可以奔着对的方向一直猜下去，最后总会猜对！梯度下降法就是这样的，多次尝试。并且，在试的过程中还得想办法知道是不是在猜对的路上，说白了就是得到正确的反馈再调整然后继续猜才有意义~</p>
<p>  这个就好比道士下山，我们把 Loss （或者称为Cost，即损失）曲线看成是<strong>山谷</strong>，如果走过了，就再往回返，所以是一个迭代的过程。</p>
<h4 id="1-3、梯度下降公式"><a href="#1-3、梯度下降公式" class="headerlink" title="1.3、梯度下降公式"></a>1.3、梯度下降公式</h4><p>  这里梯度下降法的公式就是一个式子指导计算机迭代过程中如何去调整$\theta$，可以通过泰勒公式一阶展开来进行推导和证明：</p>
<ul>
<li><p>$\theta^{n + 1} = \theta^{n} - \alpha * gradient$</p>
<p>其中 $\alpha$ 表示学习率，gradient 表示梯度</p>
</li>
<li><p>$\theta^{n + 1} = \theta^{n} - \alpha * \frac{\partial J(\theta)}{\partial \theta}$</p>
<p>有些公式，使用其他字母表示：</p>
</li>
<li><p>$\theta^{n + 1} = \theta^{n} - \eta * \frac{\partial J(\theta)}{\partial \theta}$</p>
</li>
<li><p>$w_j^{n + 1} = w_j^{n} - \eta * \frac{\partial J(\theta)}{\partial \theta_j}$</p>
</li>
</ul>
<p>  这里的 $w_j$ 就是 $\theta$ 中的某一个 j = 0…m，这里的 $\eta$ 就是梯度下降图里的 learning step，很多时候也叫学习率 learning rate，很多时候也用 $\alpha$ 表示，这个学习率我们可以看作是下山迈的<strong>步子</strong>的大小，步子迈的大下山就快。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/2-梯度下降思想.jpeg" alt=""></p>
<p>  学习率一般都是<strong>正数</strong>，如果在山左侧（曲线<strong>左半边</strong>）梯度是负的，那么这个负号就会把 $w_j$ 往大了调， 如果在山右侧（曲线右半边）梯度就是正的，那么负号就会把 $w_j$ 往小了调。每次 $w_j$ 调整的幅度就是 $\eta * gradient$，就是横轴上移动的距离。</p>
<p>  因此，无论在左边，还是在右边，梯度下降都可以快速找到最优解，实现快速<strong>下山</strong>~</p>
<p>  如果特征或维度越多，那么这个公式用的次数就越多，也就是每次迭代要应用的这个式子多次（多少特征，就应用多少次），所以其实上面的图不是特别准，因为 $\theta$ 对应的是很多维度，应该每一个维度都可以画一个这样的图，或者是一个多维空间的图。</p>
<ul>
<li>$w_0^{n + 1} = w_0^{n} - \eta * \frac{\partial J(\theta)}{\partial \theta_0}$</li>
<li>$w_1^{n + 1} = w_1^{n} - \eta * \frac{\partial J(\theta)}{\partial \theta_1}$</li>
<li>……</li>
<li>$w_m^{n + 1} = w_m^{n} - \eta * \frac{\partial J(\theta)}{\partial \theta_m}$</li>
<li><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/7-梯度下降公式.jpeg" alt=""></li>
</ul>
<p>  所以观察上图我们可以发现不是某一个 $\theta_0$ 或 $\theta_1$ 找到最小值就是最优解，而是它们一起找到 $J(\theta)$ 最小值才是最优解。</p>
<h4 id="1-4、学习率"><a href="#1-4、学习率" class="headerlink" title="1.4、学习率"></a>1.4、学习率</h4><p>  根据我们上面讲的梯度下降公式，我们知道 $\eta$ 是学习率，设置大的学习率 $w_j$ 每次调整的幅度就大，设置小的学习率 $w_j$ 每次调整的幅度就小，然而如果步子迈的太大也会有问题，俗话说步子大了容易扯着蛋！学习率大，可能一下子迈过了，到另一边去了（从曲线左半边跳到右半边），继续梯度下降又迈回来， 使得来来回回震荡。步子太小呢，就像蜗牛一步步往前挪，也会使得整体迭代次数增加。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/8-学习率.jpeg" alt=""></p>
<p>  学习率的设置是门一门学问，一般我们会把它设置成一个比较小的正整数，0.1、0.01、0.001、0.0001，都是常见的设定数值（然后根据情况调整）。一般情况下学习率在整体迭代过程中是不变，但是也可以设置成随着迭代次数增多学习率逐渐变小，因为越靠近山谷我们就可以步子迈小点，可以更精准的走入最低点，同时防止走过。还有一些深度学习的优化算法会自己控制调整学习率这个值，后面学习过程中这些策略在讲解代码中我们会一一讲到。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/9-学习率.jpeg" alt=""></p>
<h4 id="1-5、全局最优化"><a href="#1-5、全局最优化" class="headerlink" title="1.5、全局最优化"></a>1.5、全局最优化</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/10-全局最优化.png" alt=""></p>
<p>上图显示了梯度下降的两个主要挑战：</p>
<ul>
<li>若随机初始化，算法从左侧起步，那么会收敛到一个局部最小值，而不是全局最小值；</li>
<li>若随机初始化，算法从右侧起步，那么需要经过很长时间才能越过Plateau（函数停滞带，梯度很小），如果停下得太早，则永远达不到全局最小值；</li>
</ul>
<p>  而线性回归的模型MSE损失函数恰好是个凸函数，凸函数保证了只有一个全局最小值，其次是个连续函数，斜率不会发生陡峭的变化，因此即便是乱走，梯度下降都可以趋近全局最小值。</p>
<p>  上图损失函数是非凸函数，梯度下降法是有可能落到局部最小值的，所以其实步长不能设置的太小太稳健，那样就很容易落入局部最优解，虽说局部最小值也没大问题， 因为模型只要是<strong>堪用</strong>的就好嘛，但是我们肯定还是尽量要奔着全局最优解去！</p>
<h4 id="1-6、梯度下降步骤"><a href="#1-6、梯度下降步骤" class="headerlink" title="1.6、梯度下降步骤"></a>1.6、梯度下降步骤</h4><p>梯度下降流程就是“猜”正确答案的过程:</p>
<ul>
<li><p>1、“瞎蒙”，Random 随机数生成 $\theta$，随机生成一组数值 $w_0、w_1……w_n$ ，期望 $\mu$ 为 0 方差 $\sigma$ 为 1 的正太分布数据。</p>
</li>
<li><p>2、求梯度 g ，梯度代表曲线某点上的切线的斜率，沿着切线往下就相当于沿着坡度最陡峭的方向下降</p>
</li>
<li><p>3、if g &lt; 0, $\theta$ 变大，if g &gt; 0, $\theta$ 变小</p>
</li>
<li><p>4、判断是否收敛 convergence，如果收敛跳出迭代，如果没有达到收敛，回第 2 步再次执行2~4步</p>
<p>收敛的判断标准是：随着迭代进行损失函数Loss，变化非常微小甚至不再改变，即认为达到收敛</p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/11-梯度下降步骤.jpeg" alt=""></p>
<h4 id="1-7、代码模拟梯度下降"><a href="#1-7、代码模拟梯度下降" class="headerlink" title="1.7、代码模拟梯度下降"></a>1.7、代码模拟梯度下降</h4><ul>
<li><p>梯度下降优化算法，比正规方程，应用更加广泛</p>
</li>
<li><p>什么是梯度？</p>
<ul>
<li>梯度就是导数对应的值！</li>
</ul>
</li>
<li><p>下降？</p>
<ul>
<li>涉及到优化问题，最小二乘法</li>
</ul>
</li>
<li><p>梯度下降呢？</p>
<ul>
<li>梯度方向下降，速度最快的~</li>
</ul>
</li>
</ul>
<p>  接下来，我们使用代码来描述上面梯度下降的过程：</p>
<p>方程如下：</p>
<p>$f(x) = (x - 3.5)^2 - 4.5x + 10$</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/3-函数曲线.jpg" alt=""></p>
<p>使用梯度下降的思想，来一步步逼近，函数的最小值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">f = <span class="keyword">lambda</span> x : (x - <span class="number">3.5</span>)**<span class="number">2</span> -<span class="number">4.5</span>*x + <span class="number">10</span></span><br><span class="line"><span class="comment"># 导函数</span></span><br><span class="line">d = <span class="keyword">lambda</span> x :<span class="number">2</span>*(x - <span class="number">3.5</span>) - <span class="number">4.5</span> <span class="comment"># 梯度 == 导数</span></span><br><span class="line"><span class="comment"># 梯度下降的步幅，比例，（学习率，幅度）</span></span><br><span class="line">step = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 求解当x等于多少的时候，函数值最小。求解目标值：随机生成的</span></span><br><span class="line"><span class="comment"># 相等于：&#x27;瞎蒙&#x27; ----&gt; 方法 ----&gt; 优化</span></span><br><span class="line">x = np.random.randint(<span class="number">0</span>,<span class="number">12</span>,size = <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 梯度下降，每下降一步，每走一步，目标值，都会更新。</span></span><br><span class="line"><span class="comment"># 更新的这个新值和上一步的值，差异，如果差异很小（万分之一）</span></span><br><span class="line"><span class="comment"># 梯度下降退出</span></span><br><span class="line">last_x = x + <span class="number">0.02</span> <span class="comment"># 记录上一步的值，首先让last_x和x有一定的差异！！！</span></span><br><span class="line"><span class="comment"># 精确率，真实计算，都是有误差，自己定义</span></span><br><span class="line">precision = <span class="number">1e-4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;+++++++++++++++++++++&#x27;</span>, x)</span><br><span class="line">x_ = [x]</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># 退出条件，精确度，满足了</span></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">abs</span>(x - last_x) &lt; precision:</span><br><span class="line">        <span class="keyword">break</span>     </span><br><span class="line">    <span class="comment"># 更新</span></span><br><span class="line">    last_x = x</span><br><span class="line">    x -= step*d(x) <span class="comment"># 更新，减法：最小值</span></span><br><span class="line">    x_.append(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;--------------------&#x27;</span>,x)</span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = <span class="string">&#x27;Kaiti SC&#x27;</span></span><br><span class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</span><br><span class="line">x = np.linspace(<span class="number">5.75</span> - <span class="number">5</span>, <span class="number">5.75</span> + <span class="number">5</span>, <span class="number">100</span>)</span><br><span class="line">y = f(x)</span><br><span class="line">plt.plot(x,y,color = <span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;梯度下降&#x27;</span>,size = <span class="number">24</span>,pad = <span class="number">15</span>)</span><br><span class="line">x_ = np.array(x_)</span><br><span class="line">y_ = f(x_)</span><br><span class="line">plt.scatter(x_, y_,color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;./图片/5-梯度下降.jpg&#x27;</span>,dpi = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<p>函数的最优解是：<strong>5.75</strong>。你可以发现，随机赋值的变量 x ，无论<strong>大于</strong>5.75，还是<strong>小于</strong>5.75，经过梯度下降，最终都慢慢靠近5.75这个最优解！</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/4-梯度下降.jpg" alt=""></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/5-梯度下降.jpg" alt=""></p>
<p><strong>注意：</strong></p>
<ol>
<li>梯度下降存在一定误差，不是完美解~</li>
<li>在误差允许的范围内，梯度下降所求得的机器学习模型，是堪用的！</li>
<li>梯度下降的步幅step，不能太大，俗话说步子不能迈的太大！</li>
<li>精确度，可以根据实际情况调整</li>
<li>while True循环里面，持续进行梯度下降：</li>
</ol>
<p>   $\theta = \theta - \eta \frac{\partial}{\partial \theta}J(\theta)$ 其中的 $\eta $ 叫做学习率</p>
<p>  $x = x - \eta\frac{\partial}{\partial x}f(x)$</p>
<p>  $x = x - step*\frac{\partial}{\partial x} f(x)$ 其中的 $step $ 叫做学习率</p>
<p>  $x = x - step * f’(x)$</p>
<ol>
<li>while 循环退出条件是：x更新之后和上一次相差绝对值小于特定精确度！</li>
</ol>
<h3 id="2、梯度下降方法"><a href="#2、梯度下降方法" class="headerlink" title="2、梯度下降方法"></a>2、梯度下降方法</h3><h4 id="2-1、三种梯度下降不同"><a href="#2-1、三种梯度下降不同" class="headerlink" title="2.1、三种梯度下降不同"></a>2.1、三种梯度下降不同</h4><p>梯度下降分三类：批量梯度下降BGD（<strong>Batch Gradient Descent</strong>）、小批量梯度下降MBGD（<strong>Mini-Batch Gradient Descent</strong>）、随机梯度下降SGD（<strong>Stochastic Gradient Descent</strong>）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/12-梯度下降方式.jpeg" alt=""></p>
<p>三种梯度下降有什么不同呢？我们从梯度下降步骤开始讲起，梯度下降步骤分一下四步：</p>
<ul>
<li><p>1、随机赋值，Random 随机数生成 $\theta$，随机一组数值 $w_0、w_1……w_n$</p>
</li>
<li><p>2、求梯度 g ，梯度代表曲线某点上的切线的斜率，沿着切线往下就相当于沿着坡度最陡峭的方向下降</p>
</li>
<li><p>3、if g &lt; 0, $\theta$ 变大，if g &gt; 0, $\theta$ 变小</p>
</li>
<li><p>4、判断是否收敛 convergence，如果收敛跳出迭代，如果没有达到收敛，回第 2 步再次执行2~4步</p>
<p>收敛的判断标准是：随着迭代进行损失函数Loss，变化非常微小甚至不再改变，即认为达到收敛</p>
</li>
</ul>
<p>三种梯度下降不同，体现在第二步中：</p>
<ul>
<li><p>BGD是指在<strong>每次迭代</strong>使用<strong>所有样本</strong>来进行梯度的更新</p>
</li>
<li><p>MBGD是指在<strong>每次迭代</strong>使用<strong>一部分样本</strong>（所有样本500个，使用其中32个样本）来进行梯度的更新</p>
</li>
<li><p>SGD是指<strong>每次迭代</strong>随机选择<strong>一个样本</strong>来进行梯度更新</p>
</li>
</ul>
<h4 id="2-2、线性回归梯度更新公式"><a href="#2-2、线性回归梯度更新公式" class="headerlink" title="2.2、线性回归梯度更新公式"></a>2.2、线性回归梯度更新公式</h4><p>回顾上一讲公式！</p>
<p>最小二乘法公式如下：</p>
<p>$J(\theta) = \frac{1}{2}\sum\limits<em>{i = 1}^n(h</em>{\theta}(x^{(i)}) - y^{(i)})^2$</p>
<p>矩阵写法：</p>
<p>$J(\theta) = \frac{1}{2}(X\theta - y)^T(X\theta - y)$</p>
<p>接着我们来讲解如何求解上面梯度下降的第 2 步，即我们要推导出损失函数的导函数来。</p>
<ul>
<li><p>$\theta_j^{n + 1} = \theta_j^{n} - \eta * \frac{\partial J(\theta)}{\partial \theta_j}$ 其中 j 表示第 j 个系数</p>
</li>
<li><p>$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{\partial}{\partial \theta<em>j}\frac{1}{2}(h</em>{\theta}(x) - y)^2$</p>
</li>
</ul>
<p>$= \frac{1}{2}*2(h_{\theta}(x) - y)\frac{\partial}{\partial \theta<em>j}(h</em>{\theta}(x) - y)$ (1)</p>
<p>$= (h_{\theta}(x) - y)\frac{\partial}{\partial \theta<em>j}(\sum\limits</em>{i = 0}^n\theta_ix_i - y)$ (2)</p>
<p>$= (h_{\theta}(x) - y)x_j$ (3)</p>
<p>  $x^2$的导数就是 2x，根据链式求导法则，我们可以推出上面第（1）步。然后是多元线性回归，所以 $h_{\theta}(x)$ 就 是 $\theta^Tx$ 即是$w_0x_0 + w_1x_1 + …… + w_nx<em>n$ 即$\sum\limits</em>{i = 0}^n\theta_ix_i$。到这里我们是对 $\theta_j$ 来求偏导，那么和 $w_j$ 没有关系的可以忽略不计，所以只剩下 $x_j$。</p>
<p>  我们可以得到结论就是 $\theta_j$ 对应的梯度与预测值 $\hat{y}$ 和真实值 y 有关，这里 $\hat{y}$ 和 y 是列向量（即多个数据），同时还与 $\theta_j$ 对应的特征维度 $x_j$ 有关，这里 $x_j$ 是原始数据集矩阵的第 j 列。如果我们分别去对每个维度 $\theta_0、\theta_1……\theta_n$ 求偏导，即可得到所有维度对应的梯度值。</p>
<ul>
<li>$g<em>0 = (h</em>{\theta}(x) - y)x_0$</li>
<li>$g<em>1 = (h</em>{\theta}(x) - y)x_1$</li>
<li>……</li>
<li>$g<em>j = (h</em>{\theta}(x) - y)x_j$</li>
</ul>
<p><strong>总结：</strong></p>
<p>$\theta_j^{n + 1} = \theta<em>j^{n} - \eta * (h</em>{\theta}(x) - y )x_j$</p>
<h4 id="2-3、批量梯度下降BGD"><a href="#2-3、批量梯度下降BGD" class="headerlink" title="2.3、批量梯度下降BGD"></a>2.3、批量梯度下降BGD</h4><p>  <strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每次迭代</strong>使用<strong>所有样本</strong>来进行梯度的更新。每次迭代参数更新公式如下：</p>
<p>$\theta_j^{n + 1} = \theta<em>j^{n} - \eta *\frac{1}{n}\sum\limits</em>{i = 1}^{n} (h_{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)}$</p>
<p>去掉 $\frac{1}{n}$ 也可以，因为它是一个常量，可以和 $\eta$ 合并</p>
<script type="math/tex; mode=display">\\theta\_j^{n + 1} = \\theta_j^{n} - \\eta\*\\sum\\limits_{i = 1}^{n} (h\_{\\theta}(x^{(i)}) - y^{(i)} )x\_j^{(i)}</script><p>矩阵写法：</p>
<p>$\theta^{n + 1} = \theta^{n} - \eta * X^T(X\theta -y)$</p>
<p>其中 𝑖 = 1, 2, …, n 表示样本数， 𝑗 = 0, 1……表示特征数，<strong>这里我们使用了偏置项，即解决$x_0^{(i)} = 1$</strong>。</p>
<p><strong>注意这里更新时存在一个求和函数，即为对所有样本进行计算处理！</strong></p>
<p><strong>优点：</strong>   （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。   （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。 <strong>缺点：</strong>   （1）当样本数目 n 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。</p>
<p>从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/13-BGD.jpeg" alt=""></p>
<h4 id="2-4、随机梯度下降SGD"><a href="#2-4、随机梯度下降SGD" class="headerlink" title="2.4、随机梯度下降SGD"></a>2.4、随机梯度下降SGD</h4><p><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是<strong>每次迭代</strong>使用<strong>一个样本</strong>来对参数进行更新。使得训练速度加快。每次迭代参数更新公式如下：</p>
<p>$\theta_j^{n + 1} = \theta<em>j^{n} - \eta *(h</em>{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)}$</p>
<p><strong>批量梯度下降</strong>算法每次都会使用<strong>全部</strong>训练样本，因此这些计算是冗余的，因为每次都使用完全相同的样本集。而<strong>随机梯度下降</strong>算法每次只随机选择<strong>一个</strong>样本来更新模型参数，因此每次的学习是非常快速的。</p>
<p>  <strong>优点：</strong>   （1）由于不是在全部训练数据上的更新计算，而是在每轮迭代中，随机选择一条数据进行更新计算，这样每一轮参数的更新速度大大加快。   <strong>缺点：</strong>   （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。   （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。</p>
<p>  <strong>解释一下为什么SGD收敛速度比BGD要快：</strong></p>
<ul>
<li>这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）。</li>
<li>而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被迭代30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。</li>
<li>也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。</li>
</ul>
<p>从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程就会盲目一些。其迭代的收敛曲线示意图可以表示如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/14-SGD.jpeg" alt=""></p>
<h4 id="2-5、小批量梯度下降MBGD"><a href="#2-5、小批量梯度下降MBGD" class="headerlink" title="2.5、小批量梯度下降MBGD"></a>2.5、小批量梯度下降MBGD</h4><p><strong>小批量梯度下降</strong>，是对批量梯度下降以及随机梯度下降的一个<strong>折中</strong>办法。其思想是：<strong>每次迭代</strong>使用总样本中的一部分（batch_size）样本来对参数进行更新。这里我们假设 batch_size = 20，样本数 n = 1000 。实现了更新速度与更新次数之间的平衡。每次迭代参数更新公式如下：</p>
<p>$\theta_j^{n + 1} = \theta<em>j^{n} - \eta *\frac{1}{batch_size}\sum\limits</em>{i = 1}^{batch_size} (h_{\theta}(x^{(i)}) - y^{(i)} )x_j^{(i)}$</p>
<p>相对于随机梯度下降算法，小批量梯度下降算法降低了收敛波动性， 即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。</p>
<p>一般情况下，小批量梯度下降是梯度下降的推荐变体，特别是在深度学习中。每次随机选择2的幂数个样本来进行学习，例如：8、16、32、64、128、256。因为计算机的结构就是二进制的。但是也要根据具体问题而选择，实践中可以进行多次试验， 选择一个更新速度与更次次数都较适合的样本数。</p>
<p>MBGD梯度下降迭代的收敛曲线更加温柔一些：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/12-梯度下降方式.jpeg" alt=""></p>
<h4 id="2-6、梯度下降优化"><a href="#2-6、梯度下降优化" class="headerlink" title="2.6、梯度下降优化"></a>2.6、梯度下降优化</h4><p>虽然梯度下降算法效果很好，并且广泛使用，但是不管用上面三种哪一种，都存在一些挑战与问题，我们可以从以下几点进行优化:</p>
<ol>
<li><p>选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/15-合适学习率.jpeg" alt=""></p>
</li>
<li><p>学习速率调整，试图在每次更新过程中， 改变学习速率。从经验上看，<strong>学习率在一开始要保持大些来保证收敛速度，在收敛到最优点附近时要小些以避免来回震荡。</strong>比较简单的学习率调整可以通过 <strong>学习率衰减（Learning Rate Decay）</strong>的方式来实现。假设初始化学习率为 $\eta_0$，在第 t 次迭代时的学习率 $\eta_t$。常用的衰减方式为可以设置为 <strong>按迭代次数</strong> 进行衰减，迭代次数越大，学习率越小！<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/16-调整学习率.jpeg" alt=""></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/17-调整学习率.jpeg" alt=""></p>
</li>
<li><p>模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的，或者每个特征有着不同的统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/18-特征学习率.jpg" alt=""></p>
</li>
<li><p>对于非凸目标函数，容易陷入那些次优的局部极值点中，如在神经网路中。那么如何避免呢。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./图片/10-全局最优化.png" alt=""></p>
<p>简单的问题，一般使用随机梯度下降即可解决。在深度学习里，对梯度下降进行了很多改进，比如：自适应梯度下降。在深度学习章节，我们会具体介绍。</p>
</li>
<li><p>轮次和批次</p>
<p>轮次：epoch，轮次顾名思义是把我们已有的训练集数据学习多少轮，迭代多少次。</p>
<p>批次：batch，批次这里指的的我们已有的训练集数据比较多的时候，一轮要学习太多数据， 那就把一轮次要学习的数据分成多个批次，一批一批数据的学习。</p>
<p>就好比，你要背诵一片《赤壁赋》，很长。你在背诵的时候，一段段的背诵，就是批次batch。花费了一天终于背诵下来了，以后的9天，每天都进行一轮背诵复习，这就是轮次epoch。这样，《赤壁赋》的背诵效果，就非常牢固了。</p>
<p>在进行，机器学习训练时，我们也要合理选择轮次和批次~</p>
</li>
</ol>
<h3 id="3、代码实战梯度下降"><a href="#3、代码实战梯度下降" class="headerlink" title="3、代码实战梯度下降"></a>3、代码实战梯度下降</h3><h4 id="3-1、批量梯度下降BGD"><a href="#3-1、批量梯度下降BGD" class="headerlink" title="3.1、批量梯度下降BGD"></a>3.1、批量梯度下降BGD</h4><p><strong>这里我们使用了偏置项，即解决$x_0^{(i)} = 1$</strong>。一元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">w,b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">2</span>)</span><br><span class="line">y = w * X  + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X,np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次</span></span><br><span class="line">epoches = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">1000</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    <span class="comment"># 根据公式计算梯度</span></span><br><span class="line">    g = X.T.dot(X.dot(θ) - y)</span><br><span class="line">    <span class="comment"># 应用梯度下降的公式去调整 θ 值</span></span><br><span class="line">    learning_rate = learning_rate_schedule(i)</span><br><span class="line">    θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<p><strong>这里我们使用了偏置项，即解决$x_0^{(i)} = 1$</strong>。多元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w)  + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X,np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次</span></span><br><span class="line">epoches = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    <span class="comment"># 根据公式计算梯度</span></span><br><span class="line">    g = X.T.dot(X.dot(θ) - y)</span><br><span class="line">    <span class="comment"># 应用梯度下降的公式去调整 θ 值</span></span><br><span class="line">    learning_rate = learning_rate_schedule(i)</span><br><span class="line">    θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<h4 id="3-2、随机梯度下降SGD"><a href="#3-2、随机梯度下降SGD" class="headerlink" title="3.2、随机梯度下降SGD"></a>3.2、随机梯度下降SGD</h4><p><strong>这里我们使用了偏置项，即解决$x_0^{(i)} = 1$</strong>。一元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">w,b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">2</span>)</span><br><span class="line">y = w * X + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次、样本数量</span></span><br><span class="line">epochs = <span class="number">10000</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n) <span class="comment"># 0 ~99</span></span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index] <span class="comment"># 打乱顺序</span></span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        X_i = X[[i]]</span><br><span class="line">        y_i = y[[i]]</span><br><span class="line">        g = X_i.T.dot(X_i.dot(θ)-y_i)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch*n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<p><strong>这里我们使用了偏置项，即解决$x_0^{(i)} = 1$</strong>。多元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次、样本数量</span></span><br><span class="line">epochs = <span class="number">10000</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">6</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n) <span class="comment"># 0 ~99</span></span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index] <span class="comment"># 打乱顺序</span></span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        X_i = X[[i]]</span><br><span class="line">        y_i = y[[i]]</span><br><span class="line">        g = X_i.T.dot(X_i.dot(θ)-y_i)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch*n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<h4 id="3-3、小批量梯度下降MBGD"><a href="#3-3、小批量梯度下降MBGD" class="headerlink" title="3.3、小批量梯度下降MBGD"></a>3.3、小批量梯度下降MBGD</h4><p><strong>这里我们使用了偏置项，即解决$x_0^{(i)} = 1$</strong>。一元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">w,b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">2</span>)</span><br><span class="line">y = w * X + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、创建超参数轮次、样本数量、小批量数量</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">num_batches = <span class="built_in">int</span>(n / batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n)</span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index]</span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">        <span class="comment"># 一次取一批数据16个样本</span></span><br><span class="line">        X_batch = X[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        y_batch = y[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        g = X_batch.T.dot(X_batch.dot(θ)-y_batch)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch * n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<p><strong>这里我们使用了偏置项，即解决$x_0^{(i)} = 1$</strong>。多元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项 X_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、创建超参数轮次、样本数量、小批量数量</span></span><br><span class="line">epochs = <span class="number">10000</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">num_batches = <span class="built_in">int</span>(n / batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n)</span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index]</span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">        <span class="comment"># 一次取一批数据16个样本</span></span><br><span class="line">        X_batch = X[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        y_batch = y[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        g = X_batch.T.dot(X_batch.dot(θ)-y_batch)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch * n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://molittle-git.github.io">molittle</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://molittle-git.github.io/posts/3c50d4b7.html">http://molittle-git.github.io/posts/3c50d4b7.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://molittle-git.github.io" target="_blank">molittle</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post-share"><div class="social-share" data-image="/img/123.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/2d61ce16.html" title="梯度下降优化"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/123.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">梯度下降优化</div></div><div class="info-2"><div class="info-item-1">梯度下降优化1、归一化...</div></div></div></a><a class="pagination-related" href="/posts/7d0d429b.html" title="多元线性回归"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/123.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">多元线性回归</div></div><div class="info-2"><div class="info-item-1">[toc] 多元线性回归1、基本概念  线性回归是机器学习中有监督机器学习下的一种算法。...</div></div></div></a></nav><p>相关文章功能暂时不可用</p></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">molittle</div><div class="author-info-description">MyBlog</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/molittle-git"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon faa-parent animated-hover" href="https://github.com/molittle-git" target="_blank" title="Github"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-github"></use></svg></a><a class="social-icon faa-parent animated-hover" href="https://wx.mail.qq.com/list/readtemplate?name=proxy_spwd.html&amp;type=indepent-password-login&amp;account=3466954759&amp;redirect_url=%2Fhome%2Findex%3Fsid%3DzQdUMoyIbFMupTJnAM5FMAAA&amp;loginfrom=qqconnect&amp;sid=zQdUMoyIbFMupTJnAM5FMAAA&amp;" target="_blank" title="Email"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-youxiang"></use></svg></a><a class="social-icon faa-parent animated-hover" href="/atom.xml" target="_blank" title="RSS"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-RSS"></use></svg></a><a class="social-icon faa-parent animated-hover" href="https://space.bilibili.com/1572041910" target="_blank" title="BiliBili"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-bilibili-nh"></use></svg></a><a class="social-icon faa-parent animated-hover" href="tencent://Message/?Uin=2268025923&amp;amp;websiteName=local.edu.com:8888=&amp;amp;Menu=yes" target="_blank" title="QQ"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-QQ"></use></svg></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7"><span class="toc-number">1.1.</span> <span class="toc-text">线性回归预测房价</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E6%97%A0%E7%BA%A6%E6%9D%9F%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="toc-number">1.2.</span> <span class="toc-text">1、无约束最优化问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1%E3%80%81%E6%97%A0%E7%BA%A6%E6%9D%9F%E6%9C%80%E4%BC%98%E5%8C%96"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.1、无约束最优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.2.2.</span> <span class="toc-text">1.2、梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F"><span class="toc-number">1.2.3.</span> <span class="toc-text">1.3、梯度下降公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4%E3%80%81%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">1.2.4.</span> <span class="toc-text">1.4、学习率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5%E3%80%81%E5%85%A8%E5%B1%80%E6%9C%80%E4%BC%98%E5%8C%96"><span class="toc-number">1.2.5.</span> <span class="toc-text">1.5、全局最优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.2.6.</span> <span class="toc-text">1.6、梯度下降步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-7%E3%80%81%E4%BB%A3%E7%A0%81%E6%A8%A1%E6%8B%9F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.2.7.</span> <span class="toc-text">1.7、代码模拟梯度下降</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">2、梯度下降方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1%E3%80%81%E4%B8%89%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8D%E5%90%8C"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1、三种梯度下降不同</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2、线性回归梯度更新公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3%E3%80%81%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DBGD"><span class="toc-number">1.3.3.</span> <span class="toc-text">2.3、批量梯度下降BGD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DSGD"><span class="toc-number">1.3.4.</span> <span class="toc-text">2.4、随机梯度下降SGD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5%E3%80%81%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DMBGD"><span class="toc-number">1.3.5.</span> <span class="toc-text">2.5、小批量梯度下降MBGD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.6.</span> <span class="toc-text">2.6、梯度下降优化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.4.</span> <span class="toc-text">3、代码实战梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1%E3%80%81%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DBGD"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1、批量梯度下降BGD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DSGD"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2、随机梯度下降SGD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3%E3%80%81%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DMBGD"><span class="toc-number">1.4.3.</span> <span class="toc-text">3.3、小批量梯度下降MBGD</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/program/"><span class="card-category-list-name">program</span><span class="card-category-list-count">16</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/program/%E4%B8%80%E4%BA%9B%E9%A2%98%E8%A7%A3/"><span class="card-category-list-name">一些题解</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/program/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="card-category-list-name">机器学习</span><span class="card-category-list-count">5</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/program/%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98%E8%B7%AF%E7%BA%BF/"><span class="card-category-list-name">算法刷题路线</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/program/%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF/"><span class="card-category-list-name">算法模板</span><span class="card-category-list-count">9</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/uncategorized/"><span class="card-category-list-name">uncategorized</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/Numpy/" style="font-size: 1.1em; color: #999">Numpy</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 1.1em; color: #999">排序</a> <a href="/tags/spfa/" style="font-size: 1.1em; color: #999">spfa</a> <a href="/tags/%E4%BA%8C%E5%88%86/" style="font-size: 1.3em; color: #99a1ac">二分</a> <a href="/tags/%E6%9C%80%E7%9F%AD%E8%B7%AF/" style="font-size: 1.1em; color: #999">最短路</a> <a href="/tags/AI/" style="font-size: 1.5em; color: #99a9bf">AI</a> <a href="/tags/%E5%B7%AE%E5%88%86/" style="font-size: 1.1em; color: #999">差分</a> <a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 1.1em; color: #999">数学</a> <a href="/tags/%E5%89%8D%E7%BC%80%E5%92%8C/" style="font-size: 1.1em; color: #999">前缀和</a> <a href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/" style="font-size: 1.1em; color: #999">树状数组</a> <a href="/tags/Floyd/" style="font-size: 1.1em; color: #999">Floyd</a></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By molittle</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><script src="/js/runtime.js"></script><link rel="stylesheet" href="/css/runtime/runtime.css"/></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><div class="aplayer no-destroy" data-id="4879584972" data-server="netease" data-type="playlist"   data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><script async src="//at.alicdn.com/t/c/font_4902343_uyeqwzz0p0r.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script>(() => {
  const destroyAplayer = () => {
    if (window.aplayers) {
      for (let i = 0; i < window.aplayers.length; i++) {
        if (!window.aplayers[i].options.fixed) {
          window.aplayers[i].destroy()
        }
      }
    }
  }

  const runMetingJS = () => {
    typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()
  }

  btf.addGlobalFn('pjaxSend', destroyAplayer, 'destroyAplayer')
  btf.addGlobalFn('pjaxComplete', loadMeting, 'runMetingJS')
})()</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg(style=getBgPath(theme.background))",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      false 
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><div class="app-refresh" id="app-refresh" style="position: fixed;top: -2.2rem;left: 0;right: 0;z-index: 99999;padding: 0 1rem;font-size: 15px;height: 2.2rem;transition: all 0.3s ease;"><div class="app-refresh-wrap" style=" display: flex;color: #fff;height: 100%;align-items: center;justify-content: center;"><label>✨ 有新文章啦！ 👉</label><a href="javascript:void(0)" onclick="location.reload()"><span style="color: #fff;text-decoration: underline;cursor: pointer;">🍗点击食用🍔</span></a></div></div><script>if ('serviceWorker' in navigator) {
if (navigator.serviceWorker.controller) {
navigator.serviceWorker.addEventListener('controllerchange', function() {
showNotification()
})
}
window.addEventListener('load', function() {
navigator.serviceWorker.register('/sw.js')
})
}
function showNotification() {
if (GLOBAL_CONFIG.Snackbar) {
var snackbarBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
GLOBAL_CONFIG.Snackbar.bgLight :
GLOBAL_CONFIG.Snackbar.bgDark
var snackbarPos = GLOBAL_CONFIG.Snackbar.position
Snackbar.show({
text: '✨ 有新文章啦！ 👉',
backgroundColor: snackbarBg,
duration: 500000,
pos: snackbarPos,
actionText: '🍗点击食用🍔',
actionTextColor: '#fff',
onActionClick: function(e) {
location.reload()
},
})
} else {
var showBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
'#3b70fc' :
'#1f1f1f'
var cssText = `top: 0; background: ${showBg};`
document.getElementById('app-refresh').style.cssText = cssText
}
}</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script>
    // 全局变量声明区域
    var fdata = {
      apiurl: 'https://hexo-friendcircle-api-ai9d4hwad-anzhiyu-c.vercel.app/api',
      initnumber: 20, //【可选】页面初始化展示文章数量
      stepnumber: 10, //【可选】每次加载增加的篇数
      error_img: 'https://npm.elemecdn.com/akilar-candyassets/image/404.gif' //【可选，头像图片加载失败时的默认头像】
    }
    //存入本地存储
    localStorage.setItem("fdatalist",JSON.stringify(fdata))
    </script>
    <script defer src="https://npm.elemecdn.com/hexo-filter-fcircle/assets/js/fetch.min.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><div id="ghbdages" style="overflow:hidden;max-height:90px;height:auto;text-align:center;margin-top:10px"><div class="swiper-wrapper"><div class="swiper-slide"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" title="博客框架为Hexo_v5.4.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" title="主题版本Butterfly_v4.2.2"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px" title="本站使用JsDelivr为静态资源提供CDN加速"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" alt=""/></a></div><div class="swiper-slide"><a class="github-badge" target="_blank" href="https://beian.miit.gov.cn/#/Integrated/index" style="margin-inline:5px" title="本站已在鲁进行备案"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/e1d492.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></div></div></div><style>a.github-badge:hover:before {display:none}</style>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script defer src="https://unpkg.zhimg.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify/lib/swiperbdage_init.min.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><script async src="/js/ali_font.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body></html>